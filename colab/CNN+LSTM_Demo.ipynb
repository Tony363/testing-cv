{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Bmc2z7DPfD4CytgFoId6UrPI-o4FbHeK",
      "authorship_tag": "ABX9TyO8df+MAwRbFT+mykSGn0yc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hc07180011/testing-cv/blob/main/flicker-detection/flicker-detection/CNN%2BLSTM_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Relevent Object classes\n",
        "\n",
        "- Tensor flow feature extractor\n",
        "- Custom Data Loader (main preprocess event)\n",
        "- pytorch LSTM model\n",
        "- Custom F1 precision recall aggregation object\n",
        "- Statistic evaluation wrapper"
      ],
      "metadata": {
        "id": "_izjFhQ-96zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import cv2\n",
        "import json\n",
        "import gc\n",
        "import torch\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import NearMiss\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, roc_auc_score, f1_score, classification_report\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.applications import resnet\n",
        "\n",
        "from typing import Callable,Tuple\n",
        "from io import StringIO\n",
        "\n",
        "\n",
        "class BaseCNN:\n",
        "    \"\"\"\n",
        "    adaptive pooling sample:\n",
        "    https://ideone.com/cJoN3x\n",
        "    \"\"\"\n",
        "    # tf.random.set_seed(12345)\n",
        "    tf.keras.utils.set_random_seed(12345)\n",
        "    tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.__target_shape = (200, 200)\n",
        "        self.__embedding = None\n",
        "        self.strategy = tf.distribute.MirroredStrategy()\n",
        "        tf.get_logger().setLevel('INFO')\n",
        "\n",
        "    def get_embedding(self, images: np.ndarray, batched=True) -> np.ndarray:\n",
        "        if not batched:\n",
        "            images = np.expand_dims(images, axis=0)\n",
        "        with self.strategy.scope():\n",
        "            resized_images = tf.image.resize(\n",
        "                images, self.__target_shape, tf.image.ResizeMethod.NEAREST_NEIGHBOR)  # check resize differences\n",
        "            return self.__embedding.predict(resnet.preprocess_input(resized_images))\n",
        "\n",
        "    def get_embed_cpu(self, images: np.ndarray, batched=True) -> np.ndarray:\n",
        "        if not batched:\n",
        "            images = np.expand_dims(images, axis=0)\n",
        "        resized_images = np.array([cv2.resize(image, dsize=self.__target_shape,\n",
        "                                              interpolation=cv2.INTER_CUBIC) for image in images])\n",
        "        with self.strategy.scope():\n",
        "            image_tensor = tf.convert_to_tensor(resized_images, np.float32)\n",
        "            return self.__embedding(resnet.preprocess_input(image_tensor)).numpy()\n",
        "\n",
        "    def extractor(self, extractor: Model, weights: str = \"imagenet\", pooling: str = \"Max\") -> Model:\n",
        "        with self.strategy.scope():\n",
        "            self.__embedding = extractor(\n",
        "                weights=weights,\n",
        "                input_shape=self.__target_shape + (3,),\n",
        "                include_top=False,\n",
        "                pooling=pooling\n",
        "            )\n",
        "            return self.__embedding\n",
        "\n",
        "\n",
        "\n",
        "class Streamer(object):\n",
        "    \"\"\"\n",
        "    https://jamesmccaffrey.wordpress.com/2021/03/08/working-with-huge-training-data-files-for-pytorch/\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 embedding_list_train: list,\n",
        "                 label_path: str,\n",
        "                 data_dir: str,\n",
        "                 mem_split: int,\n",
        "                 chunk_size: int,\n",
        "                 batch_size: int,\n",
        "                 sampler: Callable = None,\n",
        "                 multiclass: bool = False,\n",
        "                 overlap_chunking: bool = False,\n",
        "                 ) -> None:\n",
        "        self.multiclass = multiclass\n",
        "        self.overlap_chunking = overlap_chunking\n",
        "\n",
        "        self.embedding_list_train = embedding_list_train\n",
        "        self.chunk_embedding_list = np.array_split(\n",
        "            embedding_list_train, mem_split)\n",
        "        self.data_dir = data_dir\n",
        "        self.raw_labels = json.load(open(label_path, \"r\"))\n",
        "\n",
        "        self.mem_split = mem_split\n",
        "        self.chunk_size = chunk_size\n",
        "        self.batch_size = batch_size\n",
        "        self.sampler = sampler\n",
        "        self.sampling_params = None\n",
        "\n",
        "        self.cur_chunk = 0\n",
        "        self.X_buffer, self.y_buffer = (), ()\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        # FIX ME\n",
        "        return len(self.embedding_list_train)*len(self.chunk_embedding_list)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        if (not self.X_buffer or not self.y_buffer) and self.cur_chunk == len(self.chunk_embedding_list):\n",
        "            gc.collect()\n",
        "            raise StopIteration\n",
        "\n",
        "        if (not self.X_buffer or not self.y_buffer):\n",
        "            self._load_embeddings(\n",
        "                self.chunk_embedding_list[self.cur_chunk])\n",
        "    \n",
        "            self.cur_chunk += 1\n",
        "            X, y = self._re_sample()\n",
        "            self.X_buffer, self.y_buffer = self._batch_sample(\n",
        "                X, y, self.batch_size)\n",
        "            gc.collect()\n",
        "\n",
        "        X, y = self.X_buffer.pop(), self.y_buffer.pop()\n",
        "        idx = np.arange(X.shape[0]) - 1\n",
        "        random.shuffle(idx)\n",
        "        return torch.from_numpy(X[idx]).float(), torch.from_numpy(y[idx]).long()\n",
        "\n",
        "    def _re_sample(self,) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        X, y = np.array(self.X_buffer), np.array(self.y_buffer)\n",
        "        if self.sampler is None and self.ipca is None or not np.any(np.array(self.y_buffer)) == 1:\n",
        "            return X, y\n",
        "\n",
        "        if self.sampler:\n",
        "            return self._sampling(X, y, self.sampler)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def _load_embeddings(\n",
        "        self,\n",
        "        embedding_list_train: list,\n",
        "        mov_dif: bool = False,\n",
        "    ) -> None:\n",
        "        for key in embedding_list_train:\n",
        "            real_filename = key.replace(\"reduced_\", \"\").replace(\".npy\", \"\")\n",
        "            loaded = np.load(\n",
        "                \"{}\".format(os.path.join(\n",
        "                    self.data_dir, key))\n",
        "            )\n",
        "\n",
        "            flicker_idxs = np.array(\n",
        "                self.raw_labels[real_filename], dtype=np.uint16) - 1\n",
        "            if self.overlap_chunking:\n",
        "                self.X_buffer += (*self._overlap_chunks(loaded,\n",
        "                                  flicker_idxs, self.chunk_size),)\n",
        "                self.y_buffer += (1,)*flicker_idxs.size\n",
        "                loaded = np.delete(loaded, flicker_idxs, axis=0)\n",
        "                flicker_idxs = np.array([])\n",
        "\n",
        "            buf_label = np.zeros(loaded.shape[0])\n",
        "            buf_label[flicker_idxs.tolist()] = 1\n",
        "            self.X_buffer += (*self._get_chunk_array(loaded,\n",
        "                                                     self.chunk_size),)\n",
        "            self.y_buffer += tuple(\n",
        "                sum(x) if self.multiclass else 1 if sum(x) else 0\n",
        "                for x in self._get_chunk_array(buf_label, self.chunk_size)\n",
        "            )\n",
        "            gc.collect()\n",
        "\n",
        "    def _shuffle(self) -> None:\n",
        "        random.shuffle(self.embedding_list_train)\n",
        "        self.chunk_embedding_list = np.array_split(\n",
        "            self.embedding_list_train, self.mem_split)\n",
        "        self.cur_chunk = 0\n",
        "        self.X_buffer, self.y_buffer = (), ()\n",
        "        gc.collect()\n",
        "\n",
        "    @staticmethod\n",
        "    def _mov_dif_chunks(\n",
        "        input_arr: np.ndarray,\n",
        "    ) -> np.ndarray:\n",
        "        difference = np.diff(input_arr, axis=-1)\n",
        "        return (255*(difference - np.min(difference))/np.ptp(difference)).astype(np.int8)\n",
        "\n",
        "    @staticmethod\n",
        "    def _overlap_chunks(\n",
        "        input_arr: np.ndarray,\n",
        "        labels: np.ndarray,\n",
        "        chunk_size: int\n",
        "    ) -> np.ndarray:\n",
        "        vid_pad = np.zeros(\n",
        "            (input_arr.shape[0]+chunk_size, *input_arr.shape[1:]))\n",
        "        vid_pad[chunk_size//2:-chunk_size//2] = input_arr\n",
        "        return np.array([\n",
        "            vid_pad[idx:idx+chunk_size]\n",
        "            for idx in labels\n",
        "        ])\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_chunk_array(input_arr: np.array, chunk_size: int) -> list:\n",
        "        chunks = np.array_split(\n",
        "            input_arr,\n",
        "            list(range(\n",
        "                chunk_size,\n",
        "                input_arr.shape[0] + 1,\n",
        "                chunk_size\n",
        "            ))\n",
        "        )\n",
        "        i_pad = np.zeros(chunks[0].shape)\n",
        "        i_pad[:len(chunks[-1])] = chunks[-1]\n",
        "        chunks[-1] = i_pad\n",
        "        return chunks\n",
        "\n",
        "    @staticmethod\n",
        "    def _sampling(\n",
        "        X_train: np.array,\n",
        "        y_train: np.array,\n",
        "        sampler: Callable,\n",
        "    ) -> Tuple[np.array, np.array]:\n",
        "        \"\"\"\n",
        "        batched alternative:\n",
        "        https://imbalanced-learn.org/stable/references/generated/imblearn.keras.BalancedBatchGenerator.html\n",
        "        \"\"\"\n",
        "        if isinstance(sampler, list):\n",
        "            sampler = Pipeline(sampler)\n",
        "        original_X_shape = X_train.shape\n",
        "        X_train, y_train = sampler.fit_resample(\n",
        "            np.reshape(X_train, (-1, np.prod(original_X_shape[1:]))),\n",
        "            y_train\n",
        "        )\n",
        "        X_train = np.reshape(X_train, (-1,) + original_X_shape[1:])\n",
        "        return X_train, y_train\n",
        "\n",
        "    @staticmethod\n",
        "    def _batch_sample(\n",
        "        X: np.ndarray,\n",
        "        y: np.ndarray,\n",
        "        batch_size: int,\n",
        "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        X = [\n",
        "            X[i:i+batch_size]\n",
        "            for i in range(0, len(X), batch_size)\n",
        "        ]\n",
        "        y = [\n",
        "            y[i:i+batch_size]\n",
        "            for i in range(0, len(y), batch_size)\n",
        "        ]\n",
        "        return X, y\n",
        "\n",
        "\n",
        "class LSTM(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        output_dim: int,\n",
        "        hidden_dim: int,\n",
        "        layer_dim: int,\n",
        "        bidirectional=False,\n",
        "    ) -> None:\n",
        "        super(LSTM, self).__init__()\n",
        "        # Hidden dimensions\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # Number of hidden layers\n",
        "        self.layer_dim = layer_dim\n",
        "        # Output dim classes\n",
        "        self.output_dim = output_dim\n",
        "        self.n_directions = 2 if bidirectional else 1\n",
        "\n",
        "        # LSTM Layer\n",
        "        self.lstm = torch.nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=layer_dim,\n",
        "                            batch_first=True, bidirectional=bidirectional)\n",
        "        # Linear Dense\n",
        "        self.fc1 = torch.nn.Linear(hidden_dim*self.n_directions, hidden_dim//2)\n",
        "        # Linear Dense\n",
        "        self.fc2 = torch.nn.Linear(hidden_dim//2, self.output_dim)\n",
        "        # initialize weights & bias with stdv -> 0.05\n",
        "        self.initialization()\n",
        "\n",
        "    def init_hidden(self, x: torch.Tensor) -> torch.FloatTensor:\n",
        "        h0 = torch.zeros(\n",
        "            self.layer_dim*self.n_directions,\n",
        "            x.size(0),\n",
        "            self.hidden_dim,\n",
        "            device=\"cuda\"\n",
        "        ).requires_grad_()\n",
        "\n",
        "        # Initialize cell state\n",
        "        c0 = torch.zeros(\n",
        "            self.layer_dim*self.n_directions,\n",
        "            x.size(0),\n",
        "            self.hidden_dim,\n",
        "            device=\"cuda\"\n",
        "        ).requires_grad_()\n",
        "        return h0, c0\n",
        "\n",
        "    def forward(self, x) -> torch.Tensor:\n",
        "        # One time step\n",
        "        out, _ = self.lstm(x, self.init_hidden(x))\n",
        "        # Dense lstm\n",
        "        out = self.fc1(out)\n",
        "        # Dense for softmax\n",
        "        out = self.fc2(out)\n",
        "        return out[:, -1]\n",
        "\n",
        "    def initialization(self) -> None:\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, torch.nn.Linear):\n",
        "                torch.nn.init.normal_(param.data, std=0.05)\n",
        "            elif isinstance(m, torch.nn.LSTM):\n",
        "                for name, param in m.named_parameters():\n",
        "                    if 'weight_ih' in name:\n",
        "                        for i in range(4):\n",
        "                            mul = param.shape[0]//4\n",
        "                            torch.nn.init.xavier_uniform_(param[i*mul:(i+1)*mul])\n",
        "                    elif 'weight_hh' in name:\n",
        "                        for i in range(4):\n",
        "                            mul = param.shape[0]//4\n",
        "                            torch.nn.init.xavier_uniform_(param[i*mul:(i+1)*mul])\n",
        "                    elif 'bias' in name:\n",
        "                        torch.nn.init.zeros_(param.data)\n",
        "\n",
        "\n",
        "\n",
        "class F1Score(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Class for f1 calculation in Pytorch.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, average: str = 'weighted'):\n",
        "        \"\"\"\n",
        "        Init.\n",
        "\n",
        "        Args:\n",
        "            average: averaging method\n",
        "        \"\"\"\n",
        "        self.average = average\n",
        "        if average not in [None, 'micro', 'macro', 'weighted']:\n",
        "            raise ValueError('Wrong value of average parameter')\n",
        "\n",
        "    @staticmethod\n",
        "    def calc_f1_micro(predictions: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Calculate f1 micro.\n",
        "\n",
        "        Args:\n",
        "            predictions: tensor with predictions\n",
        "            labels: tensor with original labels\n",
        "\n",
        "        Returns:\n",
        "            f1 score\n",
        "        \"\"\"\n",
        "        true_positive = torch.eq(labels, predictions).sum().float()\n",
        "        f1_score = torch.div(true_positive, len(labels))\n",
        "        return f1_score\n",
        "\n",
        "    @staticmethod\n",
        "    def calc_f1_count_for_label(predictions: torch.Tensor,\n",
        "                                labels: torch.Tensor, label_id: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Calculate f1 and true count for the label\n",
        "\n",
        "        Args:\n",
        "            predictions: tensor with predictions\n",
        "            labels: tensor with original labels\n",
        "            label_id: id of current label\n",
        "\n",
        "        Returns:\n",
        "            f1 score and true count for label\n",
        "        \"\"\"\n",
        "        # label count\n",
        "        true_count = torch.eq(labels, label_id).sum()\n",
        "\n",
        "        # true positives: labels equal to prediction and to label_id\n",
        "        true_positive = torch.logical_and(torch.eq(labels, predictions),\n",
        "                                          torch.eq(labels, label_id)).sum().float()\n",
        "        # precision for label\n",
        "        precision = torch.div(true_positive, torch.eq(\n",
        "            predictions, label_id).sum().float())\n",
        "        # replace nan values with 0\n",
        "        precision = torch.where(torch.isnan(precision),\n",
        "                                torch.zeros_like(precision).type_as(\n",
        "                                    true_positive),\n",
        "                                precision)\n",
        "\n",
        "        # recall for label\n",
        "        recall = torch.div(true_positive, true_count)\n",
        "        # f1\n",
        "        f1 = 2 * precision * recall / (precision + recall)\n",
        "        # replace nan values with 0\n",
        "        f1 = torch.where(torch.isnan(f1), torch.zeros_like(\n",
        "            f1).type_as(true_positive), f1)\n",
        "        return f1, true_count\n",
        "\n",
        "    def __call__(self, predictions: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Calculate f1 score based on averaging method defined in init.\n",
        "\n",
        "        Args:\n",
        "            predictions: tensor with predictions\n",
        "            labels: tensor with original labels\n",
        "\n",
        "        Returns:\n",
        "            f1 score\n",
        "        \"\"\"\n",
        "\n",
        "        # simpler calculation for micro\n",
        "        if self.average == 'micro':\n",
        "            return self.calc_f1_micro(predictions, labels)\n",
        "\n",
        "        f1_score = 0\n",
        "        for label_id in range(len(labels.unique())):\n",
        "            f1, true_count = self.calc_f1_count_for_label(\n",
        "                predictions, labels, label_id)\n",
        "\n",
        "            if self.average == 'weighted':\n",
        "                f1_score += f1 * true_count\n",
        "            elif self.average == 'macro':\n",
        "                f1_score += f1\n",
        "\n",
        "        if self.average == 'weighted':\n",
        "            f1_score = torch.div(f1_score, len(labels))\n",
        "        elif self.average == 'macro':\n",
        "            f1_score = torch.div(f1_score, len(labels.unique()))\n",
        "\n",
        "        return f1_score\n",
        "\n",
        "\n",
        "class F1_Loss(torch.nn.Module):\n",
        "    '''Calculate F1 score. Can work with gpu tensors\n",
        "\n",
        "    The original implmentation is written by Michal Haltuf on Kaggle.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        `ndim` == 1. epsilon <= val <= 1\n",
        "\n",
        "    Reference\n",
        "    ---------\n",
        "    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n",
        "    #sklearn.metrics.f1_score\n",
        "    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
        "    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n",
        "    - http://www.ryanzhang.info/python/writing-your-own-loss-function-module-for-pytorch/\n",
        "    '''\n",
        "\n",
        "    def __init__(self, epsilon=1e-7):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, y_pred, y_true,):\n",
        "        assert y_pred.ndim == 2\n",
        "        assert y_true.ndim == 1\n",
        "        y_true = F.one_hot(y_true, 2).to(torch.float32)\n",
        "        y_pred = F.softmax(y_pred, dim=1)\n",
        "\n",
        "        tp = (y_true * y_pred).sum(dim=0).to(torch.float32)\n",
        "        tn = ((1 - y_true) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
        "        fp = ((1 - y_true) * y_pred).sum(dim=0).to(torch.float32)\n",
        "        fn = (y_true * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
        "\n",
        "        precision = tp / (tp + fp + self.epsilon)\n",
        "        recall = tp / (tp + fn + self.epsilon)\n",
        "\n",
        "        f1 = 2 * (precision*recall) / (precision + recall + self.epsilon)\n",
        "        f1 = f1.clamp(min=self.epsilon, max=1-self.epsilon)\n",
        "        return 1 - f1.mean()\n",
        "\n",
        "\n",
        "class Evaluation(object):\n",
        "    \"\"\"\n",
        "    https://onlineconfusionmatrix.com/\n",
        "    https://discuss.pytorch.org/t/bce-loss-vs-cross-entropy/97437/3\n",
        "    \"\"\"\n",
        "    logging.getLogger('PIL.PngImagePlugin').setLevel(logging.WARNING)\n",
        "\n",
        "    def __init__(self,\n",
        "                 plots_folder: str = \"plots/\",\n",
        "                 classes: int = 2,\n",
        "                 f1_metric: F1Score = F1Score(average='macro'),\n",
        "                 ) -> None:\n",
        "        self.plots_folder = plots_folder\n",
        "        self.classes = classes\n",
        "        self.f1_metric = f1_metric\n",
        "\n",
        "    def roc_auc(\n",
        "        self,\n",
        "        y_true: np.ndarray,\n",
        "        y_pred: np.ndarray,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        plot ROC Curve\n",
        "        https://stackoverflow.com/questions/45332410/roc-for-multiclass-classification\n",
        "        \"\"\"\n",
        "        roc_auc, fpr, tpr = {}, {}, {}\n",
        "        for i in range(self.classes):\n",
        "            fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_pred[:, i])\n",
        "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "        # Plot of a ROC curve for a specific class\n",
        "        for i in range(self.classes):\n",
        "            plt.figure()\n",
        "            plt.plot([0, 1], [0, 1], linestyle=\"dashed\")\n",
        "            plt.plot(fpr[i], tpr[i], marker=\"o\")\n",
        "            plt.plot([0, 0, 1], [0, 1, 1], linestyle=\"dashed\", c=\"red\")\n",
        "            plt.legend([\n",
        "                \"No Skill\",\n",
        "                \"ROC curve (area = {:.2f})\".format(roc_auc[i]),\n",
        "                \"Perfect\"\n",
        "            ])\n",
        "            plt.xlabel(\"False Positive Rate\")\n",
        "            plt.ylabel(\"True Positive Rate\")\n",
        "            plt.title(f\"Class-{i} ROC Curve\")\n",
        "            plt.savefig(os.path.join(self.plots_folder, f\"roc_curve_{i}.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    def pr_curve(\n",
        "        self,\n",
        "        y_true: np.ndarray,\n",
        "        y_pred: np.ndarray,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        plot PR Curve\n",
        "        \"\"\"\n",
        "        precision, recall = {}, {}\n",
        "        for i in range(self.classes):\n",
        "            precision[i], recall[i], _ = precision_recall_curve(\n",
        "                y_true[:, i], y_pred[:, i])\n",
        "        # Plot of a ROC curve for a specific class\n",
        "        for i in range(self.classes):\n",
        "            plt.figure()\n",
        "            plt.plot([0, 1], [0, 0], linestyle=\"dashed\")\n",
        "            plt.plot(recall[i], precision[i], marker=\"o\")\n",
        "            plt.legend([\n",
        "                \"No Skill\",\n",
        "                \"Model\"\n",
        "            ])\n",
        "            plt.xlabel(\"Recall\")\n",
        "            plt.ylabel(\"Precision\")\n",
        "            plt.title(f\"Class-{i} Precision-recall Curve\")\n",
        "            plt.savefig(os.path.join(self.plots_folder, f\"pc_curve_{i}.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    def cm(\n",
        "        self,\n",
        "        y_true: torch.tensor,\n",
        "        y_pred: torch.tensor,\n",
        "    ) -> None:\n",
        "        f1_score = self.f1_metric(y_pred, y_true)\n",
        "        logging.info(\"f1: {:.4f}\".format(f1_score))\n",
        "\n",
        "        # plot Confusion Matrix\n",
        "        # https://towardsdatascience.com/understanding-the-confusion-matrix-from-scikit-learn-c51d88929c79\n",
        "        cm = confusion_matrix(\n",
        "            y_true.cpu().numpy(),\n",
        "            y_pred.cpu().numpy(),\n",
        "        )\n",
        "        fig = plt.figure(num=-1)\n",
        "        ax = fig.add_subplot()\n",
        "        sns.heatmap(cm, annot=True, fmt='g', ax=ax)\n",
        "        ax.set_xlabel('Predicted')\n",
        "        ax.set_ylabel('Actual')\n",
        "        ax.set_title(\"Multiclass F1 Harmonization: {:.4f}\".format(f1_score))\n",
        "        fig.savefig(os.path.join(self.plots_folder, \"confusion_matrix.png\"))\n",
        "\n",
        "    @ staticmethod\n",
        "    def plot_callback(\n",
        "        train_metric: np.ndarray,\n",
        "        val_metric: np.ndarray,\n",
        "        name: str, num=0\n",
        "    ) -> None:\n",
        "        plt.figure(num=num, figsize=(16, 4), dpi=200)\n",
        "        plt.plot(val_metric)\n",
        "        plt.plot(train_metric)\n",
        "        plt.legend([\"val_{}\".format(name), \"{}\".format(name), ])\n",
        "        plt.xlabel(\"# Epochs\")\n",
        "        plt.ylabel(\"{}\".format(name))\n",
        "        plt.title(\"{} LSTM, Chunked, Oversampling\".format(name))\n",
        "        plt.savefig(\"{}.png\".format(\n",
        "            os.path.join(\"plots/\", name)))\n",
        "        plt.close()\n",
        "\n",
        "    @ staticmethod\n",
        "    def report_to_df(report) -> pd.DataFrame:  # FIX ME\n",
        "        report = re.sub(r\" +\", \" \", report).replace(\"avg / total\",\n",
        "                                                    \"avg/total\").replace(\"\\n \", \"\\n\")\n",
        "        report_df = pd.read_csv(StringIO(\"Classes\" + report),\n",
        "                                sep=' ', index_col=0, on_bad_lines='skip')\n",
        "        report_df.to_csv(\"plots/report.csv\")\n",
        "        return report_df\n",
        "\n",
        "    def report(\n",
        "        self,\n",
        "        y_true: np.ndarray,\n",
        "        y_classes: np.ndarray,\n",
        "    ) -> pd.DataFrame:\n",
        "        return self.report_to_df(\n",
        "            classification_report(y_true, y_classes, digits=4)\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def miss_classified(\n",
        "        X_test: torch.Tensor,\n",
        "        y_classes: torch.Tensor,\n",
        "        y_true: torch.Tensor,\n",
        "        data_src: str = 'data/vgg16_emb',\n",
        "        missed_out: str = 'data/missed_labels.json',\n",
        "        test_set: str = None,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        use numpy mesh grid for all combinations \n",
        "        \"\"\"\n",
        "        midx = (y_classes != y_true).nonzero().flatten()\n",
        "        chunk_size = X_test.shape[1]\n",
        "        X_test = X_test[midx].flatten(start_dim=0, end_dim=1)\n",
        "        # logging.debug(f\"{len(midx)} - {X_test.shape}\")\n",
        "\n",
        "        if len(midx) > 0 and os.path.isdir(data_src) and len(os.listdir(data_src)) != 0:\n",
        "            missed_labels = {}\n",
        "            for emb in test_set:\n",
        "                embedding = torch.from_numpy(\n",
        "                    np.load(f\"{os.path.join(data_src,emb)}\"))\n",
        "                if embedding.shape[0] < X_test.shape[0]:\n",
        "                    embedding = torch.cat((\n",
        "                        embedding,\n",
        "                        torch.ones((X_test.shape[0]-embedding.shape[0], X_test.shape[-1])\n",
        "                                   )), dim=0)\n",
        "                else:\n",
        "                    X_test = torch.cat((\n",
        "                        X_test,\n",
        "                        torch.ones((embedding.shape[0]-X_test.shape[0], embedding.shape[-1])\n",
        "                                   )), dim=0)\n",
        "\n",
        "                idx = torch.logical_not(\n",
        "                    torch.sum((X_test - embedding), dim=1)).sum().item()\n",
        "                # logging.debug(f\"{emb} - {idx//chunk_size} - {type(idx)}\")\n",
        "                missed_labels[emb] = idx//chunk_size\n",
        "            json.dump(missed_labels, open(f\"{missed_out}\", \"w\"))\n",
        "        return X_test[midx]"
      ],
      "metadata": {
        "id": "MwkAMxDXmPvG"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main functions\n",
        "- extract vgg16 embeddings\n",
        "- train test split on embedding names\n",
        "- torch testing"
      ],
      "metadata": {
        "id": "uwhGJitY-jIA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "z_qQpacLmBFN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import tqdm\n",
        "import logging\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "\n",
        "from tensorflow.keras.applications import DenseNet121, mobilenet, vgg16, InceptionResNetV2, InceptionV3\n",
        "\n",
        "data_base_dir = \"data\"\n",
        "os.makedirs(data_base_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "def np_embed(\n",
        "    video_data_dir: str,\n",
        "    output_dir: str\n",
        ") -> None:\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    feature_extractor = BaseCNN()\n",
        "    feature_extractor.extractor(vgg16.VGG16)  # mobilenet.MobileNet\n",
        "    for path in tqdm.tqdm(os.listdir(video_data_dir)):\n",
        "        if os.path.exists(os.path.joinm(output_dir, \"{}.npy\".format(path))):\n",
        "            continue\n",
        "\n",
        "        vidcap = cv2.VideoCapture(os.path.join(video_data_dir, path))\n",
        "        success, image = vidcap.read()\n",
        "\n",
        "        embeddings = ()\n",
        "        while success:\n",
        "            embeddings += (\n",
        "                feature_extractor.get_embed_cpu(\n",
        "                    cv2.resize(image, (200, 200)), batched=False\n",
        "                ).flatten(),)\n",
        "            success, image = vidcap.read()\n",
        "\n",
        "        embeddings = np.array(embeddings)\n",
        "        logging.info(f\"{path} / {embeddings.shape}\")\n",
        "\n",
        "        real_name = path.split(\".mp4\")[0]  # mapping[path.split(\".mp4\")[0]]\n",
        "\n",
        "        np.save(os.path.join(output_dir, real_name), embeddings)\n",
        "\n",
        "def preprocessing(\n",
        "    label_path: str,\n",
        "    data_dir: str,\n",
        "    cache_path: str,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "\n",
        "    if os.path.exists(\"/{}.npz\".format(cache_path)):\n",
        "        __cache__ = np.load(\"/{}.npz\".format(cache_path), allow_pickle=True)\n",
        "        return tuple(__cache__[k] for k in __cache__)\n",
        "\n",
        "    raw_labels = json.load(open(label_path, \"r\"))\n",
        "\n",
        "    embedding_path_list = sorted([\n",
        "        x for x in os.listdir(data_dir)\n",
        "        if x.replace(\".npy\", \"\") in raw_labels\n",
        "    ])\n",
        "    # video_0B061FQCB00136_barbet_07-07-2022_00-12-11-280.npy <- file not found, not enough space using my google drive\n",
        "    false_positives_vid = [\n",
        "        '17271FQCB00002_video_6.npy',\n",
        "        'video_0B061FQCB00136_barbet_07-07-2022_00-05-51-678.npy',\n",
        "        'video_0B061FQCB00136_barbet_07-07-2022_00-12-11-280.npy',\n",
        "        'video_0B061FQCB00136_barbet_07-21-2022_15-37-32-891.npy',\n",
        "        'video_0B061FQCB00136_barbet_07-21-2022_14-17-42-501.npy',\n",
        "        'video_03121JEC200057_sunfish_07-06-2022_23-18-35-286.npy'\n",
        "    ]\n",
        "    embedding_list_test = random.sample(list(set(embedding_path_list) - set(false_positives_vid)),int(len(embedding_path_list)*0.1))\n",
        "    embedding_list_test += false_positives_vid\n",
        "    embedding_list_val = embedding_list_test\n",
        "\n",
        "    embedding_list_train = list(\n",
        "        set(embedding_path_list) - set(embedding_list_test))\n",
        "\n",
        "    length = max([len(embedding_list_test), len(\n",
        "        embedding_list_val), len(embedding_list_train)])\n",
        "    pd.DataFrame({\n",
        "        \"train\": tuple(embedding_list_train) + (\"\",) * (length - len(embedding_list_train)),\n",
        "        \"val\": tuple(embedding_list_val) + (\"\",) * (length - len(embedding_list_val)),\n",
        "        \"test\": tuple(embedding_list_test) + (\"\",) * (length - len(embedding_list_test))\n",
        "    }).to_csv(\"{}.csv\".format(cache_path))\n",
        "\n",
        "    np.savez(cache_path, embedding_list_train,\n",
        "             embedding_list_val, embedding_list_test)\n",
        "\n",
        "\n",
        "def load_metrics(load_path):\n",
        "    if load_path == None:\n",
        "        return\n",
        "\n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    logging.info(f'Model loaded from <== {load_path}')\n",
        "\n",
        "    return torch.Tensor(state_dict['loss_callback']).numpy(),\\\n",
        "        torch.Tensor(state_dict['f1_callback']).numpy(),\\\n",
        "        torch.Tensor(state_dict['val_loss_callback']).numpy(),\\\n",
        "        torch.Tensor(state_dict['val_f1_callback']).numpy()\n",
        "\n",
        "def torch_testing(\n",
        "    ds_test: Streamer,\n",
        "    model: torch.nn.Module,\n",
        "    objective: Callable = torch.nn.Softmax(),\n",
        "    classes: int = 2,\n",
        "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    save_path: str = 'model0',\n",
        ") -> None:\n",
        "    logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
        "    metrics = Evaluation(plots_folder=\"plots/\", classes=classes)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    y_pred, y_true = (), ()\n",
        "    with torch.no_grad():\n",
        "        for (x0, y0) in ds_test:\n",
        "            x0, y0 = x0.to(device), y0.to(device)\n",
        "            output = model(x0)\n",
        "            y_pred += (objective(output),)\n",
        "            y_true += (y0,)\n",
        "\n",
        "    y_pred, y_true = torch.cat(y_pred, dim=0), torch.cat(y_true, dim=0)\n",
        "    y_classes = torch.topk(y_pred, k=1, dim=1).indices.flatten()\n",
        "    metrics.cm(y_true.detach(), y_classes.detach())\n",
        "\n",
        "    y_pred, y_true = (y_pred).cpu().numpy(), y_true.cpu().numpy()\n",
        "    y_bin = np.zeros((y_true.shape[0], classes))\n",
        "    idx = np.array([[i] for i in y_true])\n",
        "    np.put_along_axis(y_bin, idx, 1, axis=1)\n",
        "\n",
        "    metrics.roc_auc(y_bin, y_pred)\n",
        "    metrics.pr_curve(y_bin, y_pred)\n",
        "\n",
        "    loss, f1, val_loss, val_f1 = load_metrics(f\"{save_path}/metrics.pth\")\n",
        "    metrics.plot_callback(loss, val_loss, \"loss\", num=43)\n",
        "    metrics.plot_callback(f1, val_f1, \"f1\", num=42)\n",
        "    metrics.report(y_true, y_classes.cpu().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Script\n"
      ],
      "metadata": {
        "id": "C3kM0SOC-yDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_path = \"/content/drive/MyDrive/google_cv/flicker_detection/plots/new_label.json\"\n",
        "data_path = \"/content/drive/MyDrive/google_cv/flicker_detection/plots/vgg16_emb\"\n",
        "videos_path = \"/content/drive/MyDrive/google_cv/Flicker_Videos/0824\"\n",
        "cache_path = \"/content/drive/MyDrive/google_cv/flicker_detection/plots/.cache\"\n",
        "model_path = \"/content/drive/MyDrive/google_cv/flicker_detection/plots/model0/\"\n",
        "\n",
        "# np_embed(\n",
        "#     videos_path,\n",
        "#     data_path,\n",
        "# )\n",
        "\n",
        "preprocessing(\n",
        "    label_path,\n",
        "    data_path,\n",
        "    cache_path,\n",
        ")\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "3A6Qrrcumpr8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "241606eb-27e2-4148-b0a2-1d930553c5fc"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['04011FDD4000FC_video_0_7b1ffc26-10f0-4678-a72b-d6f88692e900.npy',\n",
              "        '07141FDD40022L_video_0_b563013b-832f-4d3b-85a9-95b221c88454.npy',\n",
              "        '95281FFBA0006N_video_0_f589f9bd-9f38-4c83-8917-9081ae176447_2.npy',\n",
              "        '06051FDD4000K6_video_0_b563013b-832f-4d3b-85a9-95b221c88454.npy',\n",
              "        '04021FDD40000Q_video_0_0122ff53-396c-415d-8ef7-c07234f21572.npy',\n",
              "        '04021FDD40000Q_video_0_17e3ff08-9da2-4b2e-af05-3723238a870f.npy',\n",
              "        '95281FFBA0006N_video_0_56140057-ae22-4822-8e72-f20a9862a98c.npy',\n",
              "        '00_flicker_issue_00_00_32 - 00_00_33_606ae6bf-cd77-4792-9cf6-82277c3b1416.npy',\n",
              "        '0A271FQCB00132_video_0_a01b478b-fb37-4283-8339-0ab71b2f321c.npy',\n",
              "        '92SBA06321_video_0_b9b480c1-7c0d-41c2-a16b-7a00ecae70cb.npy',\n",
              "        '00_flicker_issues_00_00_28 - 00_00_29_f589f9bd-9f38-4c83-8917-9081ae176447.npy',\n",
              "        '06041FQCB00116_video_0_ec94c3fc-240e-4b7b-b2b4-3e6b70e859d4.npy',\n",
              "        '07141FDD40022L_video_2_a79d380c-9352-42cd-9f7e-ca08dda19e87.npy',\n",
              "        '05051FQCB00314_video_3_c6e56a69-428a-4136-8f43-e43d51652b05.npy',\n",
              "        '04011FDD4000FC_video_3_fefe88a6-40ac-4a4a-84c8-c9ec25ef27b9.npy',\n",
              "        '0A271FQCB00132_video_0_7ecccac8-95eb-42bb-bb45-66378867e1db.npy',\n",
              "        '06141FDD40000W_video_0_698eb055-62c3-4c5a-9906-5067d3560baa.npy',\n",
              "        '0A271FQCB00132_video_0_25f48d25-66f9-4215-9bb1-d602f62f8714.npy',\n",
              "        '05051FQCB00297_video_0_57010349-46b0-4a7e-8ad3-8d1363ea01ef.npy',\n",
              "        '05091FQCB00109_video_2_a6b13d7f-4e05-4824-8d32-4eadc3274a2e.npy',\n",
              "        '07141FDD40022L_video_0_ f6b2650a-7143-417c-bd8c-56f96f78948e.npy',\n",
              "        '05051FQCB00472_video_0_50050b19-e8e5-4941-9b82-ba0c239b9d3f.npy',\n",
              "        '9C041FFBA0013V_video_0_aa4c2e7b-0813-4694-b673-9b7b8b8e0f89.npy',\n",
              "        '95281FFBA0006N_video_0_606ae6bf-cd77-4792-9cf6-82277c3b1416_2.npy',\n",
              "        '05091FQCB00109_video_0_9b323ab1-387d-42df-8092-fdea952d80f8.npy',\n",
              "        '92SBA06321_video_0_606ae6bf-cd77-4792-9cf6-82277c3b1416.npy',\n",
              "        '07141FDD40022L_video_0_b6de8637-fcba-420e-8cc1-2c7ebcd5f281.npy',\n",
              "        '95281FFBA0006N_video_0_825e13ee-faf8-4c77-85d0-4e59568bb132.npy',\n",
              "        '95281FFBA0006N_video_0_d6ba5c53-a494-41b2-8ecc-2a092cd58ceb.npy',\n",
              "        '04011FDD4000FC_video_3_9ecb562b-d963-4a98-a906-dab17bdce871.npy',\n",
              "        '04021FDD40000Q_video_0_bc2f128c-5b11-4c11-bd0f-37813c7d6bae_2.npy',\n",
              "        '07141FDD40022L_video_0_a83fb10d-aabd-4211-b57a-21634ed1cf8b.npy',\n",
              "        '09161FQCB00072_video_1_bb8adb4a-77e9-4878-a7c2-5472823d427b.npy',\n",
              "        '95281FFBA0006N_video_1_9bd4327c-0e70-4ba6-a840-c688bf4e7544.npy',\n",
              "        '06141FDD40000W_video_0_6919692f-b405-469b-bee4-df00a389e9c9.npy',\n",
              "        '04011FDD4000FC_video_0_b6de8637-fcba-420e-8cc1-2c7ebcd5f281.npy',\n",
              "        '95281FFBA0006N_video_0_aa4c2e7b-0813-4694-b673-9b7b8b8e0f89.npy',\n",
              "        '06141FDD40000W_video_0_a83fb10d-aabd-4211-b57a-21634ed1cf8b.npy',\n",
              "        '06141FDD40000W_video_0_ec1564ec-c140-46ec-8680-50b85319f595_2.npy',\n",
              "        '05091FQCB00109_video_0_50050b19-e8e5-4941-9b82-ba0c239b9d3f.npy',\n",
              "        '00_flicker_issue_00_00_35.955 - 00_00_36.068_8848a5d9-d407-4ff9-b01a-cd0a12ffa6b6.npy',\n",
              "        '06141FDD40000W_video_0698eb055-62c3-4c5a-9906-5067d3560baa.npy',\n",
              "        '09161FQCB00072_video_0_7ecccac8-95eb-42bb-bb45-66378867e1db.npy',\n",
              "        '04161FQCB00182_video_0_f42657d4-7e9a-4616-8823-57a6f5053993.npy',\n",
              "        '06021FDD40011S_video_0_bc2f128c-5b11-4c11-bd0f-37813c7d6bae.npy',\n",
              "        '04021FDD40000Q_video_0_174e1b4a-b685-4d44-b7a1-df9cd9db09a0.npy',\n",
              "        '04161FQCB00182_video_0_e91241ca-cc83-4c9f-90e1-bdc83a9994bc.npy',\n",
              "        '95281FFBA0006N_video_0_a53ca302-6735-4bb1-914f-b05a14124a20.npy',\n",
              "        '09161FQCB00072_video_0_fd7928ed-298c-48ea-b6f8-7d1386973756.npy',\n",
              "        '05051FQCB00297_video_0_a6b13d7f-4e05-4824-8d32-4eadc3274a2e.npy',\n",
              "        '04161FQCB00182_video_0_a6b13d7f-4e05-4824-8d32-4eadc3274a2e.npy',\n",
              "        '98091FFBA001LC_video_0_a018e5bb-c09d-468e-8113-ba6a8131f189.npy',\n",
              "        '05091FQCB00423_video_0_9b323ab1-387d-42df-8092-fdea952d80f8.npy',\n",
              "        '06041FQCB00116_video_0_9d34ee26-dbb2-4860-8fe9-8bb1b8b15036.npy',\n",
              "        '05091FQCB00423_video_0_dfbe6160-f575-48b5-a44e-6f798590705b.npy',\n",
              "        '00_flicker_issue_00_00_40.509 - 00_00_40.645_60f7fbc5-8cfe-46af-8e08-569c4a25c985.npy',\n",
              "        '95281FFBA0006K_video_0_b9b480c1-7c0d-41c2-a16b-7a00ecae70cb.npy',\n",
              "        '06041FQCB00116_video_0_8d88507b-58f9-4762-b7d7-d6c845ed5aa9.npy',\n",
              "        '04021FDD40000Q_video_0_a83fb10d-aabd-4211-b57a-21634ed1cf8b.npy',\n",
              "        '00_flicker_issue_00_00_34.395 - 00_00_34.798_68028f36-e073-45e7-8760-6b0e4e2cd865.npy',\n",
              "        '06141FDD40000W_video_0_ec1564ec-c140-46ec-8680-50b85319f595.npy',\n",
              "        '95281FFBA0006N_video_0_fe710e38-dd62-46ac-ad18-f6c9bb70fd49.npy',\n",
              "        '04161FQCB00182_video_0_9b323ab1-387d-42df-8092-fdea952d80f8.npy',\n",
              "        '00_flicker_issue_00_00_27.918 - 00_00_28.032_048021f1-f569-4060-84d0-9b26107c8508.npy',\n",
              "        '06141FDD40000W_video_0_b563013b-832f-4d3b-85a9-95b221c88454_2.npy',\n",
              "        '00_flicker_issues_00_01_35 - 00_01_36_c47f15d8-3404-4b98-bc87-426ee3d6f9ee.npy',\n",
              "        '04011FDD4000FC_video_3_8804c515-ef10-4a97-9b58-f4c9f8f5dd2c.npy',\n",
              "        '09161FQCB00072_video_1_7ecccac8-95eb-42bb-bb45-66378867e1db.npy',\n",
              "        '05051FQCB00297_video_3_c6e56a69-428a-4136-8f43-e43d51652b05.npy',\n",
              "        '92SBA06321_video_0_b9b480c1-7c0d-41c2-a16b-7a00ecae70cb_2.npy',\n",
              "        '04161FQCB00182_video_0_50050b19-e8e5-4941-9b82-ba0c239b9d3f.npy',\n",
              "        '05051FQCB00297_video_0_ec94c3fc-240e-4b7b-b2b4-3e6b70e859d4.npy',\n",
              "        '09161FQCB00072_video_2_bb8adb4a-77e9-4878-a7c2-5472823d427b.npy',\n",
              "        '04161FQCB00182_video_005fcc549-3c89-43c3-a45e-4bf985e7977d.npy',\n",
              "        '06021FDD40011S_video_0_6f83f827-f1f7-4938-9880-f9a26169e68c.npy',\n",
              "        '05091FQCB00109_video_0_8d88507b-58f9-4762-b7d7-d6c845ed5aa9.npy',\n",
              "        '09161FQCB00072_video_0_a01b478b-fb37-4283-8339-0ab71b2f321c.npy',\n",
              "        '95281FFBA0006K_video_0_a53ca302-6735-4bb1-914f-b05a14124a20.npy',\n",
              "        '06141FDD40000W_video_0_5e2fc80f-c6ea-4c45-9147-89fa6f3b1620.npy',\n",
              "        '05051FQCB00314_video_0_a6b13d7f-4e05-4824-8d32-4eadc3274a2e.npy',\n",
              "        '05051FQCB00314_video_1_f667f068-cbc9-48ab-a24c-64970f7de3b7.npy',\n",
              "        '0A271FQCB00132_video_0_8c4692b7-7237-4f88-a770-085e37ee1de0.npy',\n",
              "        '06041FQCB00116_video_1_750f6a3e-663a-48cd-adbd-9953e53a80bb.npy',\n",
              "        '00_flicker_issue_00_00_18.304 - 00_00_18.606_b1d8b1fc-a81d-4ab6-bbc9-d9e8f6e072dd.npy',\n",
              "        '0B041FQCB00161_video_0_a01b478b-fb37-4283-8339-0ab71b2f321c.npy',\n",
              "        '92SBA06321_video_0_f589f9bd-9f38-4c83-8917-9081ae176447.npy',\n",
              "        '05051FQCB00314_video_0_bbc80a18-397c-4e1e-8f81-152c85651226.npy',\n",
              "        '04021FDD40000Q_video_1_6dc2c4c6-0951-4bfc-9b2e-50bf5382778c.npy',\n",
              "        '06141FDD40000W_video_0_a904c21a-3f76-4743-9ac2-67e254322b88_2.npy',\n",
              "        '06141FDD40000W_video_0_36d75e16-5cb4-4869-a02d-57d583c15047.npy',\n",
              "        '04161FQCB00182_video_0_750f6a3e-663a-48cd-adbd-9953e53a80bb.npy',\n",
              "        '05051FQCB00472_video_0_16bfab1b-df47-4501-899c-45e4708f1907.npy'],\n",
              "       dtype='<U85'),\n",
              " array(['09161FQCB00072_video_0_8c4692b7-7237-4f88-a770-085e37ee1de0.npy',\n",
              "        '00_flicker_issues_00_01_39 - 00_01_40_9bd4327c-0e70-4ba6-a840-c688bf4e7544.npy',\n",
              "        '04161FQCB00182_video_0_67f0f5eb-575e-4957-b822-8ea779ea213d.npy',\n",
              "        '04011FDD4000FC_video_1_339a89b0-db70-4f9e-bc90-562dcdf4b883.npy',\n",
              "        '07141FDD40025D_video_0_b6de8637-fcba-420e-8cc1-2c7ebcd5f281.npy',\n",
              "        '95281FFBA0006N_video_0_f589f9bd-9f38-4c83-8917-9081ae176447.npy',\n",
              "        '06041FQCB00116_video_0_1bfd3a7d-11a8-4b60-b5bd-aa55f809d175.npy',\n",
              "        '04021FDD40000Q_video_0_989c08aa-7418-47ac-a4d6-c7e2f21e50c4.npy',\n",
              "        '98091FFBA001LC_video_ fe710e38-dd62-46ac-ad18-f6c9bb70fd49.npy',\n",
              "        '05091FQCB00109_video_0_5a9858ff-800f-48a5-a23c-9b53b92e0827.npy',\n",
              "        '17271FQCB00002_video_6.npy',\n",
              "        'video_0B061FQCB00136_barbet_07-07-2022_00-05-51-678.npy',\n",
              "        'video_0B061FQCB00136_barbet_07-07-2022_00-12-11-280.npy',\n",
              "        'video_0B061FQCB00136_barbet_07-21-2022_15-37-32-891.npy',\n",
              "        'video_0B061FQCB00136_barbet_07-21-2022_14-17-42-501.npy',\n",
              "        'video_03121JEC200057_sunfish_07-06-2022_23-18-35-286.npy'],\n",
              "       dtype='<U78'),\n",
              " array(['09161FQCB00072_video_0_8c4692b7-7237-4f88-a770-085e37ee1de0.npy',\n",
              "        '00_flicker_issues_00_01_39 - 00_01_40_9bd4327c-0e70-4ba6-a840-c688bf4e7544.npy',\n",
              "        '04161FQCB00182_video_0_67f0f5eb-575e-4957-b822-8ea779ea213d.npy',\n",
              "        '04011FDD4000FC_video_1_339a89b0-db70-4f9e-bc90-562dcdf4b883.npy',\n",
              "        '07141FDD40025D_video_0_b6de8637-fcba-420e-8cc1-2c7ebcd5f281.npy',\n",
              "        '95281FFBA0006N_video_0_f589f9bd-9f38-4c83-8917-9081ae176447.npy',\n",
              "        '06041FQCB00116_video_0_1bfd3a7d-11a8-4b60-b5bd-aa55f809d175.npy',\n",
              "        '04021FDD40000Q_video_0_989c08aa-7418-47ac-a4d6-c7e2f21e50c4.npy',\n",
              "        '98091FFBA001LC_video_ fe710e38-dd62-46ac-ad18-f6c9bb70fd49.npy',\n",
              "        '05091FQCB00109_video_0_5a9858ff-800f-48a5-a23c-9b53b92e0827.npy',\n",
              "        '17271FQCB00002_video_6.npy',\n",
              "        'video_0B061FQCB00136_barbet_07-07-2022_00-05-51-678.npy',\n",
              "        'video_0B061FQCB00136_barbet_07-07-2022_00-12-11-280.npy',\n",
              "        'video_0B061FQCB00136_barbet_07-21-2022_15-37-32-891.npy',\n",
              "        'video_0B061FQCB00136_barbet_07-21-2022_14-17-42-501.npy',\n",
              "        'video_03121JEC200057_sunfish_07-06-2022_23-18-35-286.npy'],\n",
              "       dtype='<U78'))"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "__cache__ = np.load(\"{}.npz\".format(cache_path), allow_pickle=True)\n",
        "\n",
        "embedding_list_train, embedding_list_val, embedding_list_test = tuple(\n",
        "    __cache__[lst] for lst in __cache__)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "chunk_size = 30\n",
        "batch_size = 1024\n",
        "input_dim = 18432\n",
        "output_dim = 2\n",
        "hidden_dim = 64 # pretrained model hidden -> 128\n",
        "layer_dim = 1\n",
        "bidirectional = True\n",
        "multiclass = False\n",
        "overlap_chunking = True\n",
        "\n",
        "ds_test = Streamer(\n",
        "        embedding_list_test,\n",
        "        label_path,\n",
        "        data_path,\n",
        "        mem_split=1,\n",
        "        chunk_size=chunk_size,\n",
        "        batch_size=batch_size,\n",
        "        sampler=None,\n",
        "        overlap_chunking=overlap_chunking,\n",
        "    )\n",
        "\n",
        "model = LSTM(\n",
        "        input_dim=input_dim,\n",
        "        output_dim=output_dim,\n",
        "        hidden_dim=hidden_dim,\n",
        "        layer_dim=layer_dim,\n",
        "        bidirectional=bidirectional,\n",
        "    )\n",
        "dic = torch.load(os.path.join(model_path, 'model.pth'))['model_state_dict']\n",
        "for key in dict(dic).keys():\n",
        "    dic[key.replace(\"module.\",\"\")] = dic.pop(key)\n",
        "\n",
        "print(dic.keys())\n",
        "print(model.eval())\n",
        "\n",
        "model.load_state_dict(dic)\n",
        "torch_testing(ds_test, model,\n",
        "                    device=device, classes=2, save_path=model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 872
        },
        "id": "UHM4JQTLLZ9h",
        "outputId": "62eb434d-be50-4c45-be6e-87ca1b7c787c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'lstm.weight_ih_l0_reverse', 'lstm.weight_hh_l0_reverse', 'lstm.bias_ih_l0_reverse', 'lstm.bias_hh_l0_reverse', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])\n",
            "LSTM(\n",
            "  (lstm): LSTM(18432, 64, batch_first=True, bidirectional=True)\n",
            "  (fc1): Linear(in_features=128, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-22f09870d927>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m torch_testing(ds_test, model,\n\u001b[0;32m---> 44\u001b[0;31m                     device=device, classes=2, save_path=model_path)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-7ae1391c8a66>\u001b[0m in \u001b[0;36mtorch_testing\u001b[0;34m(ds_test, model, objective, classes, device, save_path)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-a11b76f2e5bd>\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_buffer\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             self._load_embeddings(\n\u001b[0;32m--> 114\u001b[0;31m                 self.chunk_embedding_list[self.cur_chunk])\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_chunk\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-a11b76f2e5bd>\u001b[0m in \u001b[0;36m_load_embeddings\u001b[0;34m(self, embedding_list_train, mov_dif)\u001b[0m\n\u001b[1;32m    144\u001b[0m             loaded = np.load(\n\u001b[1;32m    145\u001b[0m                 \"{}\".format(os.path.join(\n\u001b[0;32m--> 146\u001b[0;31m                     self.data_dir, key))\n\u001b[0m\u001b[1;32m    147\u001b[0m             )\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/google_cv/flicker_detection/plots/vgg16_emb/video_0B061FQCB00136_barbet_07-07-2022_00-12-11-280.npy'"
          ]
        }
      ]
    }
  ]
}