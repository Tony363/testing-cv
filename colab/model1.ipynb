{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hc07180011/testing-cv/blob/main/colab/model1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu3vmJEbI8gq"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnuSnIi1Rt7r",
        "outputId": "fe6d98d1-7648-4e67-8f8c-bfe1768a5ccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'testing-cv'...\n",
            "remote: Enumerating objects: 1783, done.\u001b[K\n",
            "remote: Counting objects: 100% (228/228), done.\u001b[K\n",
            "remote: Compressing objects: 100% (143/143), done.\u001b[K\n",
            "remote: Total 1783 (delta 156), reused 107 (delta 85), pack-reused 1555\u001b[K\n",
            "Receiving objects: 100% (1783/1783), 113.92 MiB | 17.58 MiB/s, done.\n",
            "Resolving deltas: 100% (1031/1031), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/hc07180011/testing-cv.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RujnFEboSD1K",
        "outputId": "36b1e296-884a-441f-d7d1-7641cf2156f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/testing-cv/flicker_detection/flicker_detection\n",
            "Requirement already satisfied: absl-py==1.0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.0.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.6.3)\n",
            "Requirement already satisfied: cachetools==4.2.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (4.2.4)\n",
            "Requirement already satisfied: certifi==2021.10.8 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (2021.10.8)\n",
            "Collecting charset-normalizer==2.0.10\n",
            "  Downloading charset_normalizer-2.0.10-py3-none-any.whl (39 kB)\n",
            "Collecting colorama==0.4.4\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting coloredlogs==15.0.1\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting commonmark==0.9.1\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler==0.11.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (0.11.0)\n",
            "Requirement already satisfied: flatbuffers==2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (2.0)\n",
            "Collecting fonttools==4.28.5\n",
            "  Downloading fonttools-4.28.5-py3-none-any.whl (890 kB)\n",
            "\u001b[K     |████████████████████████████████| 890 kB 58.8 MB/s \n",
            "\u001b[?25hCollecting gast==0.4.0\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting google-auth==2.3.3\n",
            "  Downloading google_auth-2.3.3-py2.py3-none-any.whl (155 kB)\n",
            "\u001b[K     |████████████████████████████████| 155 kB 52.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth-oauthlib==0.4.6 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (0.4.6)\n",
            "Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (0.2.0)\n",
            "Collecting grpcio==1.43.0\n",
            "  Downloading grpcio-1.43.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 49.1 MB/s \n",
            "\u001b[?25hCollecting h5py==3.6.0\n",
            "  Downloading h5py-3.6.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 49.5 MB/s \n",
            "\u001b[?25hCollecting humanfriendly==10.0\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 4.4 MB/s \n",
            "\u001b[?25hCollecting idna==3.3\n",
            "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 7.4 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata==4.10.1\n",
            "  Downloading importlib_metadata-4.10.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: joblib==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 21)) (1.1.0)\n",
            "Collecting keras==2.7.0\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 14.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Keras-Preprocessing==1.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 23)) (1.1.2)\n",
            "Collecting kiwisolver==1.3.2\n",
            "  Downloading kiwisolver-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 11.4 MB/s \n",
            "\u001b[?25hCollecting libclang==12.0.0\n",
            "  Downloading libclang-12.0.0-2-py2.py3-none-manylinux1_x86_64.whl (13.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.3 MB 26.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Markdown==3.3.6 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 26)) (3.3.6)\n",
            "Collecting matplotlib==3.5.1\n",
            "  Downloading matplotlib-3.5.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 63.5 MB/s \n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement numpy==1.22.1 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0rc1, 1.13.0rc2, 1.13.0, 1.13.1, 1.13.3, 1.14.0rc1, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0rc1, 1.15.0rc2, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0rc1, 1.16.0rc2, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0rc1, 1.17.0rc2, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0rc1, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0rc1, 1.19.0rc2, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0rc1, 1.20.0rc2, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0rc1, 1.21.0rc2, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5, 1.21.6)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for numpy==1.22.1\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py==1.0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 1)) (1.0.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 2)) (1.6.3)\n",
            "Requirement already satisfied: cached-property==1.5.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 3)) (1.5.2)\n",
            "Collecting cachetools==5.0.0\n",
            "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: certifi==2021.10.8 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 5)) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer==2.0.12 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 6)) (2.0.12)\n",
            "Collecting commonmark==0.9.1\n",
            "  Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "Requirement already satisfied: cycler==0.11.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 8)) (0.11.0)\n",
            "Requirement already satisfied: flatbuffers==2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 9)) (2.0)\n",
            "Collecting fonttools==4.31.2\n",
            "  Downloading fonttools-4.31.2-py3-none-any.whl (899 kB)\n",
            "\u001b[K     |████████████████████████████████| 899 kB 11.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast==0.5.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 11)) (0.5.3)\n",
            "Collecting google-auth==2.6.2\n",
            "  Downloading google_auth-2.6.2-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 42.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth-oauthlib==0.4.6 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 13)) (0.4.6)\n",
            "Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 14)) (0.2.0)\n",
            "Requirement already satisfied: grpcio==1.44.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 15)) (1.44.0)\n",
            "Collecting h5py==3.6.0\n",
            "  Using cached h5py-3.6.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
            "Collecting idna==3.3\n",
            "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
            "Collecting imbalanced-learn==0.9.0\n",
            "  Downloading imbalanced_learn-0.9.0-py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 49.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: imblearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 19)) (0.0)\n",
            "Requirement already satisfied: importlib-metadata==4.11.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 20)) (4.11.3)\n",
            "Requirement already satisfied: joblib==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 21)) (1.1.0)\n",
            "Requirement already satisfied: keras==2.8.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 22)) (2.8.0)\n",
            "Requirement already satisfied: Keras-Preprocessing==1.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 23)) (1.1.2)\n",
            "Requirement already satisfied: kiwisolver==1.4.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 24)) (1.4.2)\n",
            "Collecting libclang==13.0.0\n",
            "  Downloading libclang-13.0.0-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5 MB 49.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Markdown==3.3.6 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 26)) (3.3.6)\n",
            "Collecting matplotlib==3.5.1\n",
            "  Using cached matplotlib-3.5.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "Collecting numpy==1.21.5\n",
            "  Downloading numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 15.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauthlib==3.2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 29)) (3.2.0)\n",
            "Collecting opencv-python==4.5.5.64\n",
            "  Downloading opencv_python-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 60.5 MB 55 kB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 31)) (3.3.0)\n",
            "Requirement already satisfied: packaging==21.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements_dev.txt (line 32)) (21.3)\n",
            "Collecting Pillow==9.1.0\n",
            "  Downloading Pillow-9.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 63.6 MB/s \n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pkg_resources==0.0.0 (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for pkg_resources==0.0.0\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 19.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.16.1\n",
            "Collecting coloredlogs\n",
            "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "Collecting rich\n",
            "  Downloading rich-12.3.0-py3-none-any.whl (232 kB)\n",
            "\u001b[K     |████████████████████████████████| 232 kB 33.2 MB/s \n",
            "\u001b[?25hCollecting humanfriendly>=9.1\n",
            "  Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from rich) (4.2.0)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich) (2.6.1)\n",
            "Installing collected packages: humanfriendly, commonmark, rich, coloredlogs\n",
            "Successfully installed coloredlogs-15.0.1 commonmark-0.9.1 humanfriendly-10.0 rich-12.3.0\n"
          ]
        }
      ],
      "source": [
        "%cd testing-cv/flicker_detection/flicker_detection/\n",
        "%pip install -r requirements.txt\n",
        "%pip install -r requirements_dev.txt\n",
        "%pip install tensorflow-addons\n",
        "%pip install coloredlogs rich\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfzg8rDHwqFg",
        "outputId": "5ff0e77c-d67d-4519-8012-d1b03b39da9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://developer.download.nvidia.com/compute/redist\n",
            "Collecting nvidia-dali-cuda110\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda110/nvidia_dali_cuda110-1.13.0-4481327-py3-none-manylinux2014_x86_64.whl (314.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 314.9 MB 4.4 kB/s \n",
            "\u001b[?25hInstalling collected packages: nvidia-dali-cuda110\n",
            "Successfully installed nvidia-dali-cuda110-1.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --extra-index-url https://developer.download.nvidia.com/compute/redist --upgrade nvidia-dali-cuda110\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z1c3JEjJATB"
      },
      "source": [
        "## Mount drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4haYpT2RpCuN",
        "outputId": "47cefba1-df92-4ba7-f916-da69ea12d40e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BlB-t1FOLjqY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "485b5592-2130-4b4a-be3b-2cfe018c10aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIEgQALXuUW5"
      },
      "source": [
        "## Seed everything first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7VlEIZMduTdW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.utils.set_random_seed(1)\n",
        "tf.config.experimental.enable_op_determinism()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1VIfZ1LHCfV"
      },
      "source": [
        "## logging.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yyAonoQP7Zj-"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import logging\n",
        "\n",
        "from rich.logging import RichHandler\n",
        "\n",
        "\n",
        "def init_logger() -> None:\n",
        "    logger = logging.getLogger(\"rich\")\n",
        "\n",
        "    FORMAT = \"%(name)s[%(process)d] \" + \\\n",
        "        \"%(processName)s(%(threadName)s) \" + \\\n",
        "        \"%(module)s:%(lineno)d  %(message)s\"\n",
        "\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "    formatter = logging.Formatter(\n",
        "        FORMAT,\n",
        "        datefmt=\"%Y%m%d %H:%M:%S\"\n",
        "    )\n",
        "    logging.basicConfig(\n",
        "        level=\"NOTSET\", format=FORMAT, handlers=[RichHandler()]\n",
        "    )\n",
        "\n",
        "    ch = logging.StreamHandler()\n",
        "    ch.setLevel(logging.DEBUG)\n",
        "    ch.setFormatter(formatter)\n",
        "\n",
        "    logger.addHandler(ch)\n",
        "\n",
        "    logging.info(\"Initializing ok.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NYdNlmfHkWu"
      },
      "source": [
        "## keras.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Sf8-Q3mf64tH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, precision_recall_curve, roc_curve, auc,roc_auc_score\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "logging.getLogger(\"matplotlib\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)\n",
        "\n",
        "plots_folder = \"/content/drive/MyDrive/google_cv/flicker_detection/plots/\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "keras metrics api:\n",
        "https://keras.io/api/metrics/\n",
        "custom sensitivity specificity:\n",
        "https://stackoverflow.com/questions/55640149/error-in-keras-when-i-want-to-calculate-the-sensitivity-and-specificity\n",
        "custom auc:\n",
        "https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n",
        "\"\"\"\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall_keras = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall_keras\n",
        "\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision_keras = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision_keras\n",
        "\n",
        "\n",
        "def specificity(y_true, y_pred):\n",
        "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
        "    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n",
        "    return tn / (tn + fp + K.epsilon())\n",
        "\n",
        "\n",
        "def negative_predictive_value(y_true, y_pred):\n",
        "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
        "    fn = K.sum(K.round(K.clip(y_true * (1 - y_pred), 0, 1)))\n",
        "    return tn / (tn + fn + K.epsilon())\n",
        "\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
        "\n",
        "# TODO\n",
        "def auroc(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    https://www.codegrepper.com/code-examples/python/auc+callback+keras\n",
        "    \"\"\"\n",
        "    if tf.math.count_nonzero(y_true) == 0:\n",
        "        return tf.cast(0.0, tf.float32)\n",
        "    return tf.numpy_function(roc_auc_score, (y_true, y_pred), tf.float32) \n",
        "\n",
        "def fbeta(y_true, y_pred, beta=2):\n",
        "    y_pred = K.clip(y_pred, 0, 1)\n",
        "\n",
        "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=1)\n",
        "    fp = K.sum(K.round(K.clip(y_pred - y_true, 0, 1)), axis=1)\n",
        "    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)), axis=1)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    num = (1 + beta ** 2) * (p * r)\n",
        "    den = (beta ** 2 * p + r + K.epsilon())\n",
        "    return K.mean(num / den)\n",
        "\n",
        "\n",
        "def matthews_correlation_coefficient(y_true, y_pred):\n",
        "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
        "    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n",
        "    fn = K.sum(K.round(K.clip(y_true * (1 - y_pred), 0, 1)))\n",
        "\n",
        "    num = tp * tn - fp * fn\n",
        "    den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
        "    return num / K.sqrt(den + K.epsilon())\n",
        "\n",
        "\n",
        "def equal_error_rate(y_true, y_pred):\n",
        "    n_imp = tf.math.count_nonzero(tf.equal(y_true, 0), dtype=tf.float32) + tf.constant(K.epsilon())\n",
        "    n_gen = tf.math.count_nonzero(tf.equal(y_true, 1), dtype=tf.float32) + tf.constant(K.epsilon())\n",
        "\n",
        "    scores_imp = tf.boolean_mask(y_pred, tf.equal(y_true, 0))\n",
        "    scores_gen = tf.boolean_mask(y_pred, tf.equal(y_true, 1))\n",
        "\n",
        "    loop_vars = (tf.constant(0.0), tf.constant(1.0), tf.constant(0.0))\n",
        "    cond = lambda t, fpr, fnr: tf.greater_equal(fpr, fnr)\n",
        "    body = lambda t, fpr, fnr: (\n",
        "        t + 0.001,\n",
        "        tf.divide(tf.math.count_nonzero(tf.greater_equal(scores_imp, t), dtype=tf.float32), n_imp),\n",
        "        tf.divide(tf.math.count_nonzero(tf.less(scores_gen, t), dtype=tf.float32), n_gen)\n",
        "    )\n",
        "    t, fpr, fnr = tf.while_loop(cond, body, loop_vars, back_prop=False)\n",
        "    eer = (fpr + fnr) / 2\n",
        "\n",
        "    return eer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Model:\n",
        "    \"\"\"\n",
        "    callbacks:\n",
        "    https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: tf.keras.models.Sequential,\n",
        "        loss: str,\n",
        "        optimizer: tf.keras.optimizers,\n",
        "        metrics = tuple((\n",
        "            # \"accuracy\",\n",
        "            # precision,\n",
        "            # recall,\n",
        "            f1,\n",
        "            # auroc,\n",
        "            # fbeta,\n",
        "            # specificity,\n",
        "            # negative_predictive_value,\n",
        "            # matthews_correlation_coefficient,\n",
        "            # equal_error_rate\n",
        "        )),\n",
        "        summary=True\n",
        "    ) -> None:\n",
        "        self.model = model\n",
        "        self.model.compile(\n",
        "            loss=loss,\n",
        "            optimizer=optimizer,\n",
        "            metrics=metrics\n",
        "        )\n",
        "        if summary:\n",
        "            print(self.model.summary())\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        X_train: np.array,\n",
        "        y_train: np.array,\n",
        "        epochs: int,\n",
        "        validation_split: float,\n",
        "        batch_size: int,\n",
        "        model_path: str = \"/content/drive/MyDrive/google_cv/flicker_detection/models/model1.h5\",\n",
        "        monitor: str = \"val_f1\",\n",
        "        mode: str = \"max\"\n",
        "    ) -> None:\n",
        "        self.history = self.model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=epochs,\n",
        "            validation_split=validation_split,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=[\n",
        "                tf.keras.callbacks.ModelCheckpoint(\n",
        "                    model_path,\n",
        "                    save_best_only=True,\n",
        "                    monitor=monitor,\n",
        "                    mode=mode\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def plot_history(self, key: str, title=None) -> None:\n",
        "        plt.figure(figsize=(16, 4), dpi=200)\n",
        "        plt.plot(self.history.history[\"{}\".format(key)])\n",
        "        plt.plot(self.history.history[\"val_{}\".format(key)])\n",
        "        plt.legend([\"{}\".format(key), \"val_{}\".format(key)])\n",
        "        plt.xlabel(\"# Epochs\")\n",
        "        plt.ylabel(\"{}\".format(key))\n",
        "        if title:\n",
        "            plt.title(\"{}\".format(title))\n",
        "        plt.savefig(plots_folder+\"{}.png\".format(key))\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "class InferenceModel:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path: str,\n",
        "        custom_objects: dict = {\n",
        "            # \"precision\":precision,\n",
        "            # \"recall\":recall,\n",
        "            \"f1\":f1,\n",
        "            # \"auroc\":auroc,\n",
        "            # \"fbeta\":fbeta,\n",
        "            # \"specificity\":specificity,\n",
        "            # \"negative_predictive_value\":negative_predictive_value,\n",
        "            # \"matthews_correlation_coefficient\":matthews_correlation_coefficient,\n",
        "            # \"equal_error_rate\":equal_error_rate\n",
        "        }\n",
        "    ) -> None:\n",
        "        self.model = tf.keras.models.load_model(\n",
        "            model_path,\n",
        "            custom_objects=custom_objects\n",
        "        )\n",
        "\n",
        "    def predict(self, X_test: np.array) -> np.array:\n",
        "        y_pred = self.model.predict(X_test)\n",
        "        return y_pred.flatten()\n",
        "\n",
        "    def evaluate(self, y_true: np.array, y_pred: np.array) -> None:\n",
        "        threshold_range = np.arange(0.1, 1.0, 0.001)\n",
        "\n",
        "        f1_scores = list()\n",
        "        for lambda_ in threshold_range:\n",
        "            f1_scores.append(f1_score(y_true, (y_pred > lambda_).astype(int)))\n",
        "        \n",
        "        # print(\"Max f1: {:.4f}, at thres = {:.4f}\".format(\n",
        "        #     np.max(f1_scores), threshold_range[np.argmax(f1_scores)]\n",
        "        # ))\n",
        "        logging.info(\"Max f1: {:.4f}, at thres = {:.4f}\".format(\n",
        "            np.max(f1_scores), threshold_range[np.argmax(f1_scores)]\n",
        "        ))\n",
        "\n",
        "        fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
        "        plt.plot([0, 1], [0, 1], linestyle=\"dashed\")\n",
        "        plt.plot(fpr, tpr, marker=\"o\")\n",
        "        plt.plot([0, 0, 1], [0, 1, 1], linestyle=\"dashed\", c=\"red\")\n",
        "        plt.legend([\n",
        "            \"No Skill\",\n",
        "            \"ROC curve (area = {:.2f})\".format(auc(fpr, tpr)),\n",
        "            \"Perfect\"\n",
        "        ])\n",
        "        plt.xlabel(\"False Positive Rate\")\n",
        "        plt.ylabel(\"True Positive Rate\")\n",
        "        plt.title(\"ROC Curve\")\n",
        "        plt.savefig(os.path.join(plots_folder,\"roc_curve.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
        "        plt.plot([0, 1], [0, 0], linestyle=\"dashed\")\n",
        "        plt.plot(recall, precision, marker=\"o\")\n",
        "        plt.legend([\n",
        "            \"No Skill\",\n",
        "            \"Model\"\n",
        "        ])\n",
        "        plt.xlabel(\"Recall\")\n",
        "        plt.ylabel(\"Precision\")\n",
        "        plt.title(\"Precision-recall Curve\")\n",
        "        plt.savefig(os.path.join(plots_folder,\"pc_curve.png\"))\n",
        "\n",
        "        print(confusion_matrix(\n",
        "            y_true,\n",
        "            (y_pred > threshold_range[np.argmax(f1_scores)]).astype(int)\n",
        "        ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS5Hgl-tG8Zm"
      },
      "source": [
        "## facenet.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ceaWGfQSG6N8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.applications import resnet, mobilenet\n",
        "from tensorflow_addons.layers import AdaptiveMaxPooling3D\n",
        "\n",
        "model_info = \"/content/drive/MyDrive/google_cv/flicker_detection/model_overview/\"\n",
        "\n",
        "class Facenet:\n",
        "    \"\"\"\n",
        "    adaptive pooling sample:\n",
        "    https://ideone.com/cJoN3x\n",
        "    \"\"\"\n",
        "    def __init__(self) -> None:\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.__target_shape = (200, 200)\n",
        "\n",
        "        np.random.seed(0)\n",
        "\n",
        "        base_cnn = mobilenet.MobileNet(\n",
        "            weights=\"imagenet\",\n",
        "            input_shape=self.__target_shape + (3,),\n",
        "            include_top=False\n",
        "        )\n",
        "\n",
        "        adaptive_1 = AdaptiveMaxPooling3D(\n",
        "            output_size=(6, 6, 1024))(base_cnn.output)\n",
        "\n",
        "        output = layers.Dense(256)(adaptive_1)\n",
        "\n",
        "        adaptive_m = AdaptiveMaxPooling3D(\n",
        "            output_size=(6, 6, 256))(output)\n",
        "\n",
        "        self.__embedding = Model(base_cnn.input, adaptive_m,name='Embedding')\n",
        "        # with open(os.path.join(model_info,'basecnn_summary.txt'), 'w') as fh:\n",
        "        #     self.__embedding.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
        "\n",
        "        for layer in base_cnn.layers[:-23]:\n",
        "            layer.trainable = False\n",
        "\n",
        "        anchor_input = layers.Input(\n",
        "            name=\"anchor\", shape=self.__target_shape + (3,)\n",
        "        )\n",
        "\n",
        "        adapt_anchor = AdaptiveMaxPooling3D(\n",
        "            output_size=self.__target_shape + (3,))(anchor_input)\n",
        "        adapted_anchor = layers.Input(\n",
        "            name=\"adapted_anchor\", shape=adapt_anchor.shape, tensor=adapt_anchor)\n",
        "\n",
        "        positive_input = layers.Input(\n",
        "            name=\"positive\", shape=self.__target_shape + (3,)\n",
        "        )\n",
        "\n",
        "        adapt_positive = AdaptiveMaxPooling3D(\n",
        "            output_size=self.__target_shape + (3,))(positive_input)\n",
        "        adapted_positive = layers.Input(\n",
        "            name=\"adapted_positive\", shape=adapt_positive.shape, tensor=adapt_positive)\n",
        "\n",
        "        negative_input = layers.Input(\n",
        "            name=\"negative\", shape=self.__target_shape + (3,)\n",
        "        )\n",
        "\n",
        "        adapt_negative = AdaptiveMaxPooling3D(\n",
        "            output_size=self.__target_shape + (3,))(negative_input)\n",
        "        adapted_negative = layers.Input(\n",
        "            name=\"adapted_negative\", shape=adapt_negative.shape, tensor=adapt_negative)\n",
        "\n",
        "        distances = DistanceLayer()(\n",
        "            self.__embedding(resnet.preprocess_input(anchor_input)),\n",
        "            self.__embedding(resnet.preprocess_input(positive_input)),\n",
        "            self.__embedding(resnet.preprocess_input(negative_input)),\n",
        "        )\n",
        "\n",
        "        siamese_network = Model(\n",
        "            inputs=[\n",
        "                adapted_anchor,\n",
        "                adapted_positive,\n",
        "                adapted_negative,\n",
        "                anchor_input,\n",
        "                positive_input,\n",
        "                negative_input,\n",
        "            ],\n",
        "            outputs=distances\n",
        "        )\n",
        "\n",
        "        # with open(os.path.join(model_info,'resnet_preprocess_summary.txt'), 'w') as fh:\n",
        "        #     siamese_network.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
        "\n",
        "        adaptive_0 = AdaptiveMaxPooling3D(\n",
        "            output_size=(1024, 6, 6))(siamese_network.output)\n",
        "\n",
        "        adaptive_siamese_network = Model(siamese_network.input, adaptive_0)\n",
        "\n",
        "        self.__siamese_model = SiameseModel(adaptive_siamese_network)\n",
        "        self.__siamese_model.built = True\n",
        "\n",
        "        # with open(os.path.join(model_info,'adaptive_siamese_summary.txt'), 'w') as fh:\n",
        "        #     self.__siamese_model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
        "        \n",
        "        model_base_dir = os.path.join(model_info)\n",
        "\n",
        "        model_settings = json.load(\n",
        "            open(os.path.join(model_base_dir, \"model.json\"), \"r\")\n",
        "        )\n",
        "        model_path = os.path.join(model_base_dir, model_settings[\"name\"])\n",
        "        if os.path.exists(model_path):\n",
        "            self.__siamese_model.load_weights(model_path)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def get_embedding(self, images: np.ndarray, batched=True) -> np.ndarray:\n",
        "        assert (not batched) or len(\n",
        "            images.shape) == 4, \"images should be an array of image with shape (width, height, 3)\"\n",
        "        if not batched:\n",
        "            images = np.array([images, ])\n",
        "        resized_images = np.array([cv2.resize(image, dsize=self.__target_shape,\n",
        "                                              interpolation=cv2.INTER_CUBIC) for image in images])\n",
        "        image_tensor = tf.convert_to_tensor(resized_images, np.float32)\n",
        "        return self.__embedding(resnet.preprocess_input(image_tensor)).numpy()\n",
        "\n",
        "\n",
        "class DistanceLayer(layers.Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, anchor, positive, negative):\n",
        "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
        "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
        "        return (ap_distance, an_distance)\n",
        "\n",
        "\n",
        "class SiameseModel(Model):\n",
        "\n",
        "    def __init__(self, siamese_network, margin=0.5):\n",
        "        super(SiameseModel, self).__init__()\n",
        "        self.siamese_network = siamese_network\n",
        "        self.margin = margin\n",
        "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.siamese_network(inputs)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self._compute_loss(data)\n",
        "\n",
        "        gradients = tape.gradient(\n",
        "            loss, self.siamese_network.trainable_weights)\n",
        "\n",
        "        self.optimizer.apply_gradients(\n",
        "            zip(gradients, self.siamese_network.trainable_weights)\n",
        "        )\n",
        "\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        loss = self._compute_loss(data)\n",
        "\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}\n",
        "\n",
        "    def _compute_loss(self, data):\n",
        "        ap_distance, an_distance = self.siamese_network(data)\n",
        "        loss = ap_distance - an_distance\n",
        "        loss = tf.maximum(loss + self.margin, 0.0)\n",
        "        return loss\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers"
      ],
      "metadata": {
        "id": "A-81gg7YWaKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
        "        length = tf.shape(inputs)[1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return inputs + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
        "        return mask\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
        "        )\n",
        "        self.dense_proj = tf.keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=tf.nn.gelu), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "\n",
        "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "\n",
        "def transformers(X_train: np.array)->Model:\n",
        "    sequence_length = 20\n",
        "    embed_dim = 9216\n",
        "    classes = 1\n",
        "    dense_dim = 4\n",
        "    num_heads = 1\n",
        "    inputs = tf.keras.Input(shape=X_train.shape[1:])\n",
        "    x = PositionalEmbedding(\n",
        "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
        "    )(inputs)\n",
        "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(classes, activation=\"sigmoid\")(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "j96bBu3BToCx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqQfgylOIFzD"
      },
      "source": [
        "## train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WRYNAZg5IAA-"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "import torch\n",
        "import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Flatten, Bidirectional\n",
        "from keras.layers.convolutional import Conv1D\n",
        "\n",
        "\n",
        "data_base_dir = \"/content/drive/MyDrive/google_cv/flicker_detection/\"\n",
        "os.makedirs(data_base_dir, exist_ok=True)\n",
        "cache_base_dir = \"/content/drive/MyDrive/google_cv/flicker_detection/.cache/\"\n",
        "os.makedirs(cache_base_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "def _embed(\n",
        "    video_data_dir: str,\n",
        "    output_dir: str\n",
        ") -> None:\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    facenet = Facenet()\n",
        "    for path in tqdm.tqdm(os.listdir(video_data_dir)):\n",
        "        if os.path.exists(os.path.join(output_dir, \"{}.npy\".format(path))):\n",
        "            continue\n",
        "\n",
        "        vidcap = cv2.VideoCapture(os.path.join(video_data_dir, path))\n",
        "        success, image = vidcap.read()\n",
        "\n",
        "        embeddings = ()\n",
        "        while success:\n",
        "            embeddings = embeddings + tuple(facenet.get_embedding(cv2.resize(\n",
        "                image, (200, 200)), batched=False)[0].flatten())\n",
        "            success, image = vidcap.read()\n",
        "\n",
        "        embeddings = np.array(embeddings)\n",
        "\n",
        "        np.save(os.path.join(output_dir, path), embeddings)\n",
        "\n",
        "\n",
        "def _get_chunk_array(input_arr: np.array, chunk_size: int) -> np.array:\n",
        "    usable_vec = input_arr[:(\n",
        "        np.floor(len(input_arr)/chunk_size)*chunk_size).astype(int)]\n",
        "\n",
        "    i_pad = np.concatenate((usable_vec, np.array(\n",
        "        [input_arr[-1]]*(chunk_size-len(usable_vec) % chunk_size))))\n",
        "    asymmetric_chunks = np.split(\n",
        "        i_pad,\n",
        "        list(range(\n",
        "            chunk_size,\n",
        "            input_arr.shape[0] + 1,\n",
        "            chunk_size\n",
        "        ))\n",
        "    )\n",
        "    return tuple(asymmetric_chunks)\n",
        "\n",
        "\n",
        "def _preprocess(\n",
        "    label_path: str,\n",
        "    mapping_path: str,\n",
        "    data_dir: str,\n",
        "    cache_path: str\n",
        ") -> Tuple[np.array]:\n",
        "    \"\"\"\n",
        "    can consider reducing precision of np.float32 to np.float16 to reduce memory consumption\n",
        "\n",
        "    abstract:\n",
        "    https://towardsdatascience.com/overcoming-data-preprocessing-bottlenecks-with-tensorflow-data-service-nvidia-dali-and-other-d6321917f851\n",
        "    cuda solution:\n",
        "    https://stackoverflow.com/questions/60996756/how-do-i-assign-a-numpy-int64-to-a-torch-cuda-floattensor\n",
        "    static memory allocation solution:\n",
        "    https://pytorch.org/docs/stable/generated/torch.zeros.html\n",
        "    \"\"\"\n",
        "    if os.path.exists(\"{}.npz\".format(cache_path)):\n",
        "        __cache__ = np.load(\"{}.npz\".format(cache_path), allow_pickle=True)\n",
        "        return tuple((__cache__[k] for k in __cache__))\n",
        "\n",
        "    pass_videos = list([\n",
        "        \"0096.mp4\", \"0097.mp4\", \"0098.mp4\",\n",
        "        \"0125.mp4\", \"0126.mp4\", \"0127.mp4\",\n",
        "        \"0145.mp4\", \"0146.mp4\", \"0147.mp4\",\n",
        "        \"0178.mp4\", \"0179.mp4\", \"0180.mp4\"\n",
        "    ])\n",
        "    raw_labels = json.load(open(label_path, \"r\"))\n",
        "    encoding_filename_mapping = json.load(open(mapping_path, \"r\"))\n",
        "\n",
        "    embedding_path_list = sorted([\n",
        "        x for x in os.listdir(data_dir)\n",
        "        if x.split(\".npy\")[0] not in pass_videos\n",
        "        and encoding_filename_mapping[x.replace(\".npy\", \"\")] in raw_labels\n",
        "    ])\n",
        "\n",
        "    embedding_list_train, embedding_list_test, _, _ = train_test_split(\n",
        "        embedding_path_list,\n",
        "        list(range(len(embedding_path_list))),\n",
        "        test_size=0.1,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    chunk_size = 30\n",
        "\n",
        "    video_embeddings_list_train = ()\n",
        "    video_labels_list_train = ()\n",
        "    # logging.debug(\n",
        "    #     \"taking training chunks, length = {}\".format(len(embedding_list_train))\n",
        "    # )\n",
        "    for path in tqdm.tqdm(embedding_list_train):\n",
        "        real_filename = encoding_filename_mapping[path.replace(\".npy\", \"\")]\n",
        "\n",
        "        buf_embedding = np.load(os.path.join(data_dir, path))\n",
        "        if buf_embedding.shape[0] == 0:\n",
        "            continue\n",
        "\n",
        "        video_embeddings_list_train = video_embeddings_list_train + \\\n",
        "            (*_get_chunk_array(buf_embedding, chunk_size),)\n",
        "\n",
        "        flicker_idxs = np.array(raw_labels[real_filename]) - 1\n",
        "        buf_label = np.zeros(buf_embedding.shape[0]).astype(\n",
        "            np.uint8) if buf_embedding.shape[0] > 0 else np.zeros(1859, dtype=int).tolist()\n",
        "        buf_label[flicker_idxs] = 1\n",
        "        video_labels_list_train = video_labels_list_train + tuple(\n",
        "            1 if sum(x) else 0\n",
        "            for x in _get_chunk_array(buf_label, chunk_size)\n",
        "        )\n",
        "\n",
        "    video_embeddings_list_test = ()\n",
        "    video_labels_list_test = ()\n",
        "    # logging.debug(\n",
        "    #     \"taking testing chunks, length = {}\".format(len(embedding_list_test))\n",
        "    # )\n",
        "    for path in tqdm.tqdm(embedding_list_test):\n",
        "        real_filename = encoding_filename_mapping[path.replace(\".npy\", \"\")]\n",
        "\n",
        "        buf_embedding = np.load(os.path.join(data_dir, path))\n",
        "        if buf_embedding.shape[0] == 0:\n",
        "            continue\n",
        "\n",
        "        video_embeddings_list_test = video_embeddings_list_test + \\\n",
        "            (*_get_chunk_array(buf_embedding, chunk_size),)\n",
        "\n",
        "        flicker_idxs = np.array(raw_labels[real_filename]) - 1\n",
        "        buf_label = np.zeros(buf_embedding.shape[0]).astype(np.uint8)\n",
        "        buf_label[flicker_idxs] = 1\n",
        "        video_labels_list_test = video_labels_list_test + tuple(\n",
        "            1 if sum(x) else 0\n",
        "            for x in _get_chunk_array(buf_label, chunk_size)\n",
        "        )\n",
        "    X_train = np.array(video_embeddings_list_train)\n",
        "    X_test = np.array(video_embeddings_list_test)\n",
        "    y_train = np.array(video_labels_list_train)\n",
        "    y_test = np.array(video_labels_list_test)\n",
        "\n",
        "    # logging.debug(\"ok. got training: {}/{}, testing: {}/{}\".format(\n",
        "    #     X_train.shape, y_train.shape,\n",
        "    #     X_test.shape, y_test.shape\n",
        "    # ))\n",
        "\n",
        "    np.savez(cache_path, X_train, X_test, y_train, y_test)\n",
        "\n",
        "    return (X_train, X_test, y_train, y_test)\n",
        "\n",
        "\n",
        "def _oversampling(\n",
        "    X_train: np.array,\n",
        "    y_train: np.array,\n",
        ") -> Tuple[np.array]:\n",
        "    \"\"\"\n",
        "    batched alternative:\n",
        "    https://imbalanced-learn.org/stable/references/generated/imblearn.keras.BalancedBatchGenerator.html\n",
        "    \"\"\"\n",
        "    sm = SMOTE(random_state=42)\n",
        "    original_X_shape = X_train.shape\n",
        "    X_train, y_train = sm.fit_resample(\n",
        "        np.reshape(X_train, (-1, np.prod(original_X_shape[1:]))),\n",
        "        y_train\n",
        "    )\n",
        "    X_train = np.reshape(X_train, (-1,) + original_X_shape[1:])\n",
        "    return (X_train, y_train)\n",
        "\n",
        "\n",
        "def _train(X_train: np.array, y_train: np.array) -> object:\n",
        "    buf = Sequential()\n",
        "    buf.add(Bidirectional(LSTM(units=256, activation='relu'),\n",
        "                          input_shape=(X_train.shape[1:])))\n",
        "    buf.add(Dense(units=128, activation=\"relu\"))\n",
        "    buf.add(Flatten())\n",
        "    buf.add(Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "    model = Model(\n",
        "        model=transformers(X_train),#buf\n",
        "        loss=\"binary_crossentropy\",\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    )\n",
        "    y_train = y_train.astype('float32').reshape((-1,1))\n",
        "    model.train(X_train, y_train, 1000, 0.1, 1024)\n",
        "    for k in ('loss','accuracy','f1'):#(\"loss\",\"accuracy\",\"precision\",\"recall\",\"f1\",\"fbeta\",\"specificity\",\"negative_predictive_value\",\"matthews_correlation_coefficient\",\"equal_error_rate\"):\n",
        "        model.plot_history(k, title=\"{} - LSTM, Chunk, Oversampling\".format(k))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def _test(model_path: str, X_test: np.array, y_test: np.array) -> None:\n",
        "    model = InferenceModel(model_path)\n",
        "    y_pred = model.predict(X_test)\n",
        "    model.evaluate(y_test, y_pred)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IumzEnVcIGLa"
      },
      "source": [
        "## Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcD66AnZ0WHI",
        "outputId": "5bb7c6ae-b884-4d36-c42b-0e0dfc017550"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "199"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nT-rZW7jGtxy",
        "outputId": "5464683e-a77a-48ae-d04b-ebd268c7dd77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 219.53it/s]\n"
          ]
        }
      ],
      "source": [
        "def pipeline()-> Tuple[np.array]:\n",
        "    # init_logger()\n",
        "\n",
        "    # logging.info(\"[Embedding] Start ...\")\n",
        "    _embed(\n",
        "        os.path.join(data_base_dir, \"Confidential_Videos\"),\n",
        "        os.path.join(data_base_dir, \"embedding\")\n",
        "    )\n",
        "    # logging.info(\"[Embedding] done.\")\n",
        "\n",
        "    \n",
        "    # logging.info(\"[Preprocessing] Start ...\")\n",
        "    X_train,X_test,y_train,y_test = _preprocess(\n",
        "        os.path.join(data_base_dir, \"model_overview/label.json\"),\n",
        "        os.path.join(data_base_dir, \"model_overview/mapping.json\"),\n",
        "        os.path.join(data_base_dir, \"embedding\"),\n",
        "        os.path.join(cache_base_dir, \"train_test\")\n",
        "    )\n",
        "    # logging.info(\"[Preprocessing] done.\")\n",
        "\n",
        "    # def load_batches(cache_base_dir):\n",
        "    #     buf = ()\n",
        "    #     for path in os.listdir(os.path.join(cache_base_dir,\".cache/\")):\n",
        "    #         buf = buf + (np.load(os.path.join(os.path.join(cache_base_dir,\".cache/\"),path)).tolist(),)\n",
        "    #     return np.array(buf)\n",
        "        \n",
        "    # X_train = load_batches(cache_base_dir)  \n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "X_train, X_test, y_train, y_test = pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEv1tGAi_wWQ",
        "outputId": "c87d3353-03dc-4149-94b0-ecda28d6c2c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42847"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0HjDjQdAfnT"
      },
      "source": [
        "## Oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xu35mchY_L_E"
      },
      "outputs": [],
      "source": [
        "def oversampling(X_train,y_train):\n",
        "    # logging.info(\"[Oversampling] Start ...\")\n",
        "    X_train, y_train = _oversampling(\n",
        "        X_train,\n",
        "        y_train\n",
        "    )\n",
        "    # logging.info(\"[Oversampling] done.\")\n",
        "    return X_train,y_train\n",
        "\n",
        "X_train,y_train = oversampling(X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gDa-osyIJ-G"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "j4IHn9VHulk9",
        "outputId": "19939860-358b-463b-c444-d4f861650513"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 30, 9216)]        0         \n",
            "                                                                 \n",
            " frame_position_embedding (P  (None, 30, 9216)         184320    \n",
            " ositionalEmbedding)                                             \n",
            "                                                                 \n",
            " transformer_layer (Transfor  (None, 30, 9216)         339895300 \n",
            " merEncoder)                                                     \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Glo  (None, 9216)             0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 9216)              0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1)                 9217      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 340,088,837\n",
            "Trainable params: 340,088,837\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-cb6025f49509>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \"\"\"\n\u001b[1;32m     14\u001b[0m \u001b[0muse\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-cb6025f49509>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(X_train, y_train)\u001b[0m\n\u001b[1;32m      4\u001b[0m     model = _train(\n\u001b[1;32m      5\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0my_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     )\n\u001b[1;32m      8\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[Training] done.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-87202c5aee92>\u001b[0m in \u001b[0;36m_train\u001b[0;34m(X_train, y_train)\u001b[0m\n\u001b[1;32m    194\u001b[0m     )\n\u001b[1;32m    195\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'f1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#(\"loss\",\"accuracy\",\"precision\",\"recall\",\"f1\",\"fbeta\",\"specificity\",\"negative_predictive_value\",\"matthews_correlation_coefficient\",\"equal_error_rate\"):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"{} - LSTM, Chunk, Oversampling\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-77a7a040b5ee>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, y_train, epochs, validation_split, batch_size, model_path, monitor, mode)\u001b[0m\n\u001b[1;32m    162\u001b[0m                     \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                     \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m                 )\n\u001b[1;32m    166\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'model_3/transformer_layer/multi_head_attention_1/einsum/Einsum' defined at (most recent call last):\n    File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n      handler_func(fileobj, events)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 452, in _handle_events\n      self._handle_recv()\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 481, in _handle_recv\n      self._run_callback(callback, msg)\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 431, in _run_callback\n      callback(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n      return self.dispatch_shell(stream, msg)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n      handler(stream, idents, msg)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n      user_expressions, allow_stdin)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n      if self.run_code(code, result):\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-12-cb6025f49509>\", line 12, in <module>\n      model = training(X_train,y_train)\n    File \"<ipython-input-12-cb6025f49509>\", line 6, in training\n      y_train\n    File \"<ipython-input-6-87202c5aee92>\", line 196, in _train\n      model.train(X_train, y_train, 1000, 0.1, 1024)\n    File \"<ipython-input-11-77a7a040b5ee>\", line 164, in train\n      mode=mode\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 859, in train_step\n      y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\", line 452, in call\n      inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"<ipython-input-5-f267994e311c>\", line 40, in call\n      attention_output = self.attention(inputs, inputs, attention_mask=mask)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/layers/multi_head_attention.py\", line 511, in call\n      query, key, value, attention_mask, training)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/layers/multi_head_attention.py\", line 473, in _compute_attention\n      query)\nNode: 'model_3/transformer_layer/multi_head_attention_1/einsum/Einsum'\nOOM when allocating tensor with shape[1024,1,30,9216] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model_3/transformer_layer/multi_head_attention_1/einsum/Einsum}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_14715]"
          ]
        }
      ],
      "source": [
        "def training(X_train:np.array,y_train:np.array) -> None:\n",
        "    \n",
        "    logging.info(\"[Training] Start ...\")\n",
        "    model = _train(\n",
        "        X_train,\n",
        "        y_train\n",
        "    )\n",
        "    logging.info(\"[Training] done.\")\n",
        "    return model\n",
        "\n",
        "\n",
        "model = training(X_train,y_train)\n",
        "\"\"\"\n",
        "use gpu:\n",
        "https://www.tensorflow.org/guide/gpu\n",
        "\"\"\"\n",
        "# vimdiff ~/googlecv/train.py /home/henrychao/googlecv/train.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-82KUqTINlv"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "3oViaL6AiLPV",
        "outputId": "e216dc48-3a75-4d05-b6cd-8af4e8f448fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max f1: 0.7059, at thres = 0.5580\n",
            "[[135   2]\n",
            " [  8  12]]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nMax f1: 0.6857, at thres = 0.6580\\n[[134   3]\\n [  8  12]]\\n'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def testing(X_test,y_test):\n",
        "    # logging.info(\"[Testing] Start ...\")\n",
        "    _test(\"/content/drive/MyDrive/google_cv/flicker_detection/models/model1.h5\", X_test, y_test)\n",
        "    # logging.info(\"[Testing] done.\")\n",
        "\n",
        "testing(X_test,y_test)\n",
        "\"\"\"\n",
        "Max f1: 0.6857, at thres = 0.6580\n",
        "[[134   3]\n",
        " [  8  12]]\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "vu3vmJEbI8gq",
        "7z1c3JEjJATB",
        "cIEgQALXuUW5",
        "o1VIfZ1LHCfV",
        "8NYdNlmfHkWu",
        "IumzEnVcIGLa",
        "M0HjDjQdAfnT"
      ],
      "name": "model1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}