2022-05-15 23:33:05.502053: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-15 23:33:06.344792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10410 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:2a:00.0, compute capability: 6.1
2022-05-15 23:33:06.345394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 10386 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:41:00.0, compute capability: 6.1
WARNING:tensorflow:From /tmp2/test/testing-cv/flicker_detection/flicker_detection/preprocessing/embedding/backbone.py:24: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.

[05/15/22 23:33:07] INFO     root[720466] MainProcess(MainThread) logger:29  Initializing ok.                           logger.py:29
                    INFO     root[720466] MainProcess(MainThread) extract_embedding:284  [Embedding] Start  extract_embedding.py:284
                             ...                                                                                                    
2022-05-15 23:33:07.030183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10410 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:2a:00.0, compute capability: 6.1
2022-05-15 23:33:07.030378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 10386 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:41:00.0, compute capability: 6.1
WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.
                    WARNING  tensorflow[720466] MainProcess(MainThread) mobilenet:226  `input_shape` is undefined   mobilenet.py:226
                             or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224,                 
                             224) will be loaded as the default.                                                                    
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
                    INFO     tensorflow[720466] MainProcess(MainThread) cross_device_ops:616  Reduce to      cross_device_ops.py:616
                             /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to                                         
                             ('/job:localhost/replica:0/task:0/device:CPU:0',).                                                     
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
                    INFO     tensorflow[720466] MainProcess(MainThread) cross_device_ops:616  Reduce to      cross_device_ops.py:616
                             /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to                                         
                             ('/job:localhost/replica:0/task:0/device:CPU:0',).                                                     
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
                    INFO     tensorflow[720466] MainProcess(MainThread) cross_device_ops:616  Reduce to      cross_device_ops.py:616
                             /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to                                         
                             ('/job:localhost/replica:0/task:0/device:CPU:0',).                                                     
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
                    INFO     tensorflow[720466] MainProcess(MainThread) cross_device_ops:616  Reduce to      cross_device_ops.py:616
                             /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to                                         
                             ('/job:localhost/replica:0/task:0/device:CPU:0',).                                                     
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
                    INFO     tensorflow[720466] MainProcess(MainThread) cross_device_ops:616  Reduce to      cross_device_ops.py:616
                             /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to                                         
                             ('/job:localhost/replica:0/task:0/device:CPU:0',).                                                     
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
                    INFO     tensorflow[720466] MainProcess(MainThread) cross_device_ops:616  Reduce to      cross_device_ops.py:616
                             /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to                                         
                             ('/job:localhost/replica:0/task:0/device:CPU:0',).                                                     
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
                    INFO     tensorflow[720466] MainProcess(MainThread) cross_device_ops:616  Reduce to      cross_device_ops.py:616
                             /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to                                         
                             ('/job:localhost/replica:0/task:0/device:CPU:0',).                                                     
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
                    INFO     tensorflow[720466] MainProcess(MainThread) cross_device_ops:616  Reduce to      cross_device_ops.py:616
                             /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to                                         
                             ('/job:localhost/replica:0/task:0/device:CPU:0',).                                                     
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
                    INFO     tensorflow[720466] MainProcess(MainThread) cross_device_ops:616  Reduce to      cross_device_ops.py:616
                             /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to                                         
                             ('/job:localhost/replica:0/task:0/device:CPU:0',).                                                     
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
                    INFO     tensorflow[720466] MainProcess(MainThread) cross_device_ops:616  Reduce to      cross_device_ops.py:616
                             /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to                                         
                             ('/job:localhost/replica:0/task:0/device:CPU:0',).                                                     
                    DEBUG    h5py._conv[720466] MainProcess(MainThread) attrs:77  Creating converter from 3 to 5         attrs.py:77
  0%|          | 0/181 [00:00<?, ?it/s]100%|██████████| 181/181 [00:00<00:00, 306858.94it/s]
                    INFO     root[720466] MainProcess(MainThread) extract_embedding:289  [Embedding] done.  extract_embedding.py:289
                    INFO     root[720466] MainProcess(MainThread) extract_embedding:291  [Preprocessing]    extract_embedding.py:291
                             Start ...                                                                                              
                    INFO     root[720466] MainProcess(MainThread) extract_embedding:298  [Preprocessing]    extract_embedding.py:298
                             done.                                                                                                  
                    INFO     root[720466] MainProcess(MainThread) extract_embedding:300  [Training] Start   extract_embedding.py:300
                             ...                                                                                                    
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')
                    INFO     tensorflow[720466] MainProcess(MainThread) mirrored_strategy:374  Using        mirrored_strategy.py:374
                             MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',                         
                             '/job:localhost/replica:0/task:0/device:GPU:1')                                                        
                    INFO     root[720466] MainProcess(MainThread) custom_models:143  Epoch 0                    custom_models.py:143
[05/15/22 23:33:08] DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (200, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:33:08.883956: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\017TensorDataset:0"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 200
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 200
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

INFO:tensorflow:batch_all_reduce: 7 all-reduces with algorithm = nccl, num_packs = 1
[05/15/22 23:33:09] INFO     tensorflow[720466] MainProcess(MainThread) cross_device_ops:897                 cross_device_ops.py:897
                             batch_all_reduce: 7 all-reduces with algorithm = nccl, num_packs = 1                                   
INFO:tensorflow:batch_all_reduce: 7 all-reduces with algorithm = nccl, num_packs = 1
[05/15/22 23:33:11] INFO     tensorflow[720466] MainProcess(MainThread) cross_device_ops:897                 cross_device_ops.py:897
                             batch_all_reduce: 7 all-reduces with algorithm = nccl, num_packs = 1                                   
2022-05-15 23:33:13.408839: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100
2022-05-15 23:33:13.688941: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100
2022-05-15 23:33:14.634743: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_9880"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\021FlatMapDataset:20"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:33:16.784728: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_11066"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\021FlatMapDataset:48"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/7 [===>..........................] - ETA: 10s - loss: 0.6353 - f1: 0.0000e+005/7 [====================>.........] - ETA: 0s - loss: 0.6732 - f1: 0.0000e+00 7/7 [==============================] - 2s 14ms/step - loss: 0.6860 - f1: 0.0000e+00
[05/15/22 23:33:18] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.697, f1  custom_models.py:172
                             - 0.000, val_loss - 0.686, val_f1 - 0.000                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (138, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:33:19.219592: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\020TensorDataset:72"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 138
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 138
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

INFO:tensorflow:batch_all_reduce: 7 all-reduces with algorithm = nccl, num_packs = 1
[05/15/22 23:33:19] INFO     tensorflow[720466] MainProcess(MainThread) cross_device_ops:897                 cross_device_ops.py:897
                             batch_all_reduce: 7 all-reduces with algorithm = nccl, num_packs = 1                                   
2022-05-15 23:33:21.802554: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_14882"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\021FlatMapDataset:92"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:33:22.311540: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_15084"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022FlatMapDataset:120"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/5 [=====>........................] - ETA: 0s - loss: 0.6201 - f1: 0.0000e+005/5 [==============================] - 0s 12ms/step - loss: 0.6825 - f1: 0.0000e+00
[05/15/22 23:33:22] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.686, f1  custom_models.py:172
                             - 0.000, val_loss - 0.682, val_f1 - 0.000                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (206, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:33:23.320540: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\021TensorDataset:144"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 206
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 206
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

INFO:tensorflow:batch_all_reduce: 7 all-reduces with algorithm = nccl, num_packs = 1
[05/15/22 23:33:24] INFO     tensorflow[720466] MainProcess(MainThread) cross_device_ops:897                 cross_device_ops.py:897
                             batch_all_reduce: 7 all-reduces with algorithm = nccl, num_packs = 1                                   
2022-05-15 23:33:26.447369: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_18140"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022FlatMapDataset:164"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:33:27.119587: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_18350"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022FlatMapDataset:192"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/7 [===>..........................] - ETA: 1s - loss: 0.6728 - f1: 0.0000e+005/7 [====================>.........] - ETA: 0s - loss: 0.6858 - f1: 0.0000e+007/7 [==============================] - 0s 14ms/step - loss: 0.6918 - f1: 0.0000e+00
[05/15/22 23:33:27] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.698, f1  custom_models.py:172
                             - 0.000, val_loss - 0.692, val_f1 - 0.000                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (192, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:33:28.124306: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\021TensorDataset:216"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 192
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 192
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:33:28.707847: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_18759"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022FlatMapDataset:236"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 32
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:33:30.745370: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_19861"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022FlatMapDataset:262"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 32
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/6 [====>.........................] - ETA: 8s - loss: 0.7221 - f1: 0.06676/6 [==============================] - 2s 8ms/step - loss: 0.6938 - f1: 0.5248
[05/15/22 23:33:32] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.697, f1  custom_models.py:172
                             - 0.000, val_loss - 0.694, val_f1 - 0.525                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (154, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:33:33.121973: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\021TensorDataset:284"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 154
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 154
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:33:33.589199: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_21400"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022FlatMapDataset:304"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:33:34.118643: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_21602"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022FlatMapDataset:332"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/5 [=====>........................] - ETA: 0s - loss: 0.7138 - f1: 0.47115/5 [==============================] - ETA: 0s - loss: 0.6852 - f1: 0.56695/5 [==============================] - 0s 14ms/step - loss: 0.6852 - f1: 0.5669
[05/15/22 23:33:34] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.687, f1  custom_models.py:172
                             - 0.612, val_loss - 0.685, val_f1 - 0.567                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (120, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:33:34.812784: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\021TensorDataset:356"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 120
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 120
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:33:35.208300: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_22006"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022FlatMapDataset:376"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:33:35.646878: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_22204"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022FlatMapDataset:404"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/4 [======>.......................] - ETA: 0s - loss: 0.7139 - f1: 0.47114/4 [==============================] - 0s 13ms/step - loss: 0.6873 - f1: 0.5966
[05/15/22 23:33:35] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.690, f1  custom_models.py:172
                             - 0.617, val_loss - 0.687, val_f1 - 0.597                                                              
[05/15/22 23:33:36] DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (120, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:33:36.312693: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\021TensorDataset:428"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 120
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 120
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:33:36.712192: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_22603"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022FlatMapDataset:448"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:33:37.159995: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_22801"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022FlatMapDataset:476"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/4 [======>.......................] - ETA: 0s - loss: 0.7113 - f1: 0.50434/4 [==============================] - 0s 13ms/step - loss: 0.6836 - f1: 0.5911
[05/15/22 23:33:37] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.689, f1  custom_models.py:172
                             - 0.607, val_loss - 0.684, val_f1 - 0.591                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (232, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:33:38.231591: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\021TensorDataset:500"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 232
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 232
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:33:38.959005: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_23200"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022FlatMapDataset:520"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:33:39.702320: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_23414"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022FlatMapDataset:548"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/8 [==>...........................] - ETA: 1s - loss: 0.6870 - f1: 0.18185/8 [=================>............] - ETA: 0s - loss: 0.6866 - f1: 0.32958/8 [==============================] - 0s 14ms/step - loss: 0.6810 - f1: 0.5617
[05/15/22 23:33:40] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.684, f1  custom_models.py:172
                             - 0.527, val_loss - 0.681, val_f1 - 0.562                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (230, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:33:40.907712: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\021TensorDataset:572"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 230
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 230
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:33:41.559145: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_23833"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022FlatMapDataset:592"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:33:42.310810: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_24047"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022FlatMapDataset:620"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/8 [==>...........................] - ETA: 1s - loss: 0.6946 - f1: 0.10005/8 [=================>............] - ETA: 0s - loss: 0.6920 - f1: 0.07228/8 [==============================] - 0s 13ms/step - loss: 0.6991 - f1: 0.1365
[05/15/22 23:33:42] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.699, f1  custom_models.py:172
                             - 0.280, val_loss - 0.699, val_f1 - 0.137                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (136, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:33:43.157999: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\021TensorDataset:644"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 136
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 136
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:33:43.581298: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_24466"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022FlatMapDataset:664"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:33:44.072174: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_24668"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022FlatMapDataset:692"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/5 [=====>........................] - ETA: 0s - loss: 0.6617 - f1: 0.16675/5 [==============================] - 0s 12ms/step - loss: 0.6796 - f1: 0.2722
[05/15/22 23:33:44] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.682, f1  custom_models.py:172
                             - 0.444, val_loss - 0.680, val_f1 - 0.272                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (112, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:33:44.725713: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\021TensorDataset:716"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 112
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 112
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:33:45.087607: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_25072"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022FlatMapDataset:736"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:33:45.514609: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_25270"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022FlatMapDataset:764"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/4 [======>.......................] - ETA: 0s - loss: 0.6682 - f1: 0.16674/4 [==============================] - 0s 12ms/step - loss: 0.6725 - f1: 0.5661
[05/15/22 23:33:45] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.677, f1  custom_models.py:172
                             - 0.468, val_loss - 0.673, val_f1 - 0.566                                                              
[05/15/22 23:33:46] DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (180, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:33:46.377907: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\021TensorDataset:788"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 180
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 180
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:33:46.910680: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_25669"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022FlatMapDataset:808"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:33:47.517587: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_25875"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022FlatMapDataset:836"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/6 [====>.........................] - ETA: 1s - loss: 0.7066 - f1: 0.10004/6 [===================>..........] - ETA: 0s - loss: 0.6871 - f1: 0.38576/6 [==============================] - 0s 14ms/step - loss: 0.6818 - f1: 0.5905
[05/15/22 23:33:47] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.687, f1  custom_models.py:172
                             - 0.000, val_loss - 0.682, val_f1 - 0.590                                                              
[05/15/22 23:33:48] DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (146, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:33:48.362491: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\021TensorDataset:860"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 146
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 146
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:33:48.817453: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_26284"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022FlatMapDataset:880"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:33:49.327897: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_26486"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022FlatMapDataset:908"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/5 [=====>........................] - ETA: 0s - loss: 0.6898 - f1: 0.0000e+005/5 [==============================] - ETA: 0s - loss: 0.6649 - f1: 0.5898    5/5 [==============================] - 0s 15ms/step - loss: 0.6649 - f1: 0.5898
[05/15/22 23:33:49] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.668, f1  custom_models.py:172
                             - 0.606, val_loss - 0.665, val_f1 - 0.590                                                              
[05/15/22 23:33:50] DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (272, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:33:50.567655: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\021TensorDataset:932"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 272
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 272
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:33:51.322284: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_26890"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022FlatMapDataset:952"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:33:52.149063: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_27108"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022FlatMapDataset:980"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/9 [==>...........................] - ETA: 2s - loss: 0.7227 - f1: 0.11115/9 [===============>..............] - ETA: 0s - loss: 0.7101 - f1: 0.18569/9 [==============================] - ETA: 0s - loss: 0.6766 - f1: 0.54759/9 [==============================] - 0s 15ms/step - loss: 0.6766 - f1: 0.5475
[05/15/22 23:33:52] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.686, f1  custom_models.py:172
                             - 0.519, val_loss - 0.677, val_f1 - 0.548                                                              
[05/15/22 23:33:53] DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (252, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:33:53.469558: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:1004"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 252
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 252
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:33:54.184150: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_27532"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:1024"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:33:54.955648: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_27746"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:1052"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/8 [==>...........................] - ETA: 1s - loss: 0.6690 - f1: 0.0000e+005/8 [=================>............] - ETA: 0s - loss: 0.6720 - f1: 0.3036    8/8 [==============================] - 0s 14ms/step - loss: 0.6663 - f1: 0.5336
[05/15/22 23:33:55] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.672, f1  custom_models.py:172
                             - 0.644, val_loss - 0.666, val_f1 - 0.534                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (222, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:33:56.126370: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:1076"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 222
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 222
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:33:56.751855: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_28165"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:1096"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:33:57.465329: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_28375"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:1124"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/7 [===>..........................] - ETA: 1s - loss: 0.6437 - f1: 0.0000e+005/7 [====================>.........] - ETA: 0s - loss: 0.6564 - f1: 0.3647    7/7 [==============================] - 0s 15ms/step - loss: 0.6635 - f1: 0.4871
[05/15/22 23:33:57] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.670, f1  custom_models.py:172
                             - 0.558, val_loss - 0.664, val_f1 - 0.487                                                              
[05/15/22 23:33:58] DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (186, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:33:58.462281: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:1148"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 186
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 186
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:33:59.033845: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_28789"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:1168"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:33:59.653782: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_28995"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:1196"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/6 [====>.........................] - ETA: 1s - loss: 0.6001 - f1: 0.25005/6 [========================>.....] - ETA: 0s - loss: 0.6462 - f1: 0.41146/6 [==============================] - 0s 14ms/step - loss: 0.6532 - f1: 0.4475
[05/15/22 23:33:59] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.663, f1  custom_models.py:172
                             - 0.557, val_loss - 0.653, val_f1 - 0.447                                                              
[05/15/22 23:34:00] DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (190, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:00.619519: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:1220"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 190
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 190
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:01.187717: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_29404"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:1240"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:01.810430: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_29610"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:1268"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/6 [====>.........................] - ETA: 1s - loss: 0.5892 - f1: 0.0000e+005/6 [========================>.....] - ETA: 0s - loss: 0.6758 - f1: 0.0000e+006/6 [==============================] - 0s 14ms/step - loss: 0.6917 - f1: 0.0000e+00
[05/15/22 23:34:02] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.695, f1  custom_models.py:172
                             - 0.000, val_loss - 0.692, val_f1 - 0.000                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (116, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:02.538080: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:1292"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 116
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 116
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:02.940794: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_30019"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:1312"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:03.374573: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_30217"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:1340"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/4 [======>.......................] - ETA: 0s - loss: 0.5882 - f1: 0.25004/4 [==============================] - 0s 13ms/step - loss: 0.6394 - f1: 0.6394
[05/15/22 23:34:03] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.651, f1  custom_models.py:172
                             - 0.538, val_loss - 0.639, val_f1 - 0.639                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (104, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:03.983206: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:1364"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 104
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 104
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:04.345950: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_30616"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:1384"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:04.757702: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_30814"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:1412"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/4 [======>.......................] - ETA: 0s - loss: 0.7025 - f1: 0.31254/4 [==============================] - 0s 11ms/step - loss: 0.6803 - f1: 0.6217
[05/15/22 23:34:04] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.681, f1  custom_models.py:172
                             - 0.605, val_loss - 0.680, val_f1 - 0.622                                                              
[05/15/22 23:34:05] DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (190, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:05.661236: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:1436"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 190
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 190
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:06.222975: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_31213"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:1456"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:06.867449: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_31419"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:1484"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/6 [====>.........................] - ETA: 1s - loss: 0.7183 - f1: 0.09095/6 [========================>.....] - ETA: 0s - loss: 0.6503 - f1: 0.43486/6 [==============================] - 0s 14ms/step - loss: 0.6356 - f1: 0.5290
[05/15/22 23:34:07] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.653, f1  custom_models.py:172
                             - 0.530, val_loss - 0.636, val_f1 - 0.529                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (258, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:08.081869: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:1508"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 258
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 258
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:08.797422: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_31828"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:1528"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:09.618004: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_32046"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:1556"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/9 [==>...........................] - ETA: 2s - loss: 0.6493 - f1: 0.12505/9 [===============>..............] - ETA: 0s - loss: 0.6608 - f1: 0.23749/9 [==============================] - 0s 13ms/step - loss: 0.6290 - f1: 0.5763
[05/15/22 23:34:10] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.635, f1  custom_models.py:172
                             - 0.548, val_loss - 0.629, val_f1 - 0.576                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (146, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:10.546218: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:1580"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 146
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 146
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:10.994104: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_32470"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:1600"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:11.512184: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_32672"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:1628"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/5 [=====>........................] - ETA: 0s - loss: 0.6480 - f1: 0.52755/5 [==============================] - ETA: 0s - loss: 0.5968 - f1: 0.63325/5 [==============================] - 0s 14ms/step - loss: 0.5968 - f1: 0.6332
[05/15/22 23:34:11] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.630, f1  custom_models.py:172
                             - 0.659, val_loss - 0.597, val_f1 - 0.633                                                              
[05/15/22 23:34:12] DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (128, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:12.240606: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:1652"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 128
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 128
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:12.641610: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_33071"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:1672"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 32
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:13.089349: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_33266"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:1698"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 32
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/4 [======>.......................] - ETA: 0s - loss: 0.5750 - f1: 0.37504/4 [==============================] - 0s 8ms/step - loss: 0.6110 - f1: 0.6189
[05/15/22 23:34:13] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.622, f1  custom_models.py:172
                             - 0.772, val_loss - 0.611, val_f1 - 0.619                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (204, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:14.034282: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:1720"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 204
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 204
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:14.636697: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_33667"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:1740"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:15.298319: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_33877"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:1768"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/7 [===>..........................] - ETA: 1s - loss: 0.5929 - f1: 0.0000e+005/7 [====================>.........] - ETA: 0s - loss: 0.6369 - f1: 0.4439    7/7 [==============================] - 0s 14ms/step - loss: 0.6448 - f1: 0.4998
[05/15/22 23:34:15] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.648, f1  custom_models.py:172
                             - 0.591, val_loss - 0.645, val_f1 - 0.500                                                              
[05/15/22 23:34:16] DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (266, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:16.611089: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:1792"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 266
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 266
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:17.352297: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_34291"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:1812"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:18.173290: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_34509"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:1840"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/9 [==>...........................] - ETA: 2s - loss: 0.5642 - f1: 0.16675/9 [===============>..............] - ETA: 0s - loss: 0.5620 - f1: 0.28019/9 [==============================] - 0s 13ms/step - loss: 0.5785 - f1: 0.5796
[05/15/22 23:34:18] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.601, f1  custom_models.py:172
                             - 0.542, val_loss - 0.578, val_f1 - 0.580                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (218, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:19.352772: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:1864"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 218
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 218
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:19.981521: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_34933"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:1884"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:20.675365: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_35143"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:1912"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/7 [===>..........................] - ETA: 1s - loss: 0.7186 - f1: 0.17145/7 [====================>.........] - ETA: 0s - loss: 0.6490 - f1: 0.22807/7 [==============================] - 0s 14ms/step - loss: 0.6468 - f1: 0.3982
[05/15/22 23:34:21] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.649, f1  custom_models.py:172
                             - 0.410, val_loss - 0.647, val_f1 - 0.398                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (80, 30,     custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:21.321660: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:1936"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 80
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 80
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:21.616726: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_35557"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:1956"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:21.945569: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_35751"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:1984"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/3 [=========>....................] - ETA: 0s - loss: 0.6245 - f1: 0.15383/3 [==============================] - 0s 10ms/step - loss: 0.6199 - f1: 0.6351
[05/15/22 23:34:22] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.645, f1  custom_models.py:172
                             - 0.739, val_loss - 0.620, val_f1 - 0.635                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (112, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:22.538829: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:2008"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 112
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 112
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:22.914058: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_36145"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:2028"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:23.325396: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_36343"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:2056"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/4 [======>.......................] - ETA: 0s - loss: 0.6179 - f1: 0.66674/4 [==============================] - 0s 11ms/step - loss: 0.6210 - f1: 0.6885
[05/15/22 23:34:23] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.620, f1  custom_models.py:172
                             - 0.705, val_loss - 0.621, val_f1 - 0.688                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (76, 30,     custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:23.821589: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:2080"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 76
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 76
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:24.114010: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_36742"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:2100"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:24.429814: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_36936"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:2128"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/3 [=========>....................] - ETA: 0s - loss: 0.6074 - f1: 0.34293/3 [==============================] - 0s 12ms/step - loss: 0.5713 - f1: 0.7624
[05/15/22 23:34:24] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.587, f1  custom_models.py:172
                             - 0.779, val_loss - 0.571, val_f1 - 0.762                                                              
[05/15/22 23:34:25] DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (260, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:25.525656: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:2152"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 260
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 260
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:26.266911: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_37330"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:2172"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:27.081896: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_37548"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:2200"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/9 [==>...........................] - ETA: 2s - loss: 0.7851 - f1: 0.0000e+005/9 [===============>..............] - ETA: 0s - loss: 0.7047 - f1: 0.2255    9/9 [==============================] - ETA: 0s - loss: 0.6307 - f1: 0.56979/9 [==============================] - 0s 15ms/step - loss: 0.6307 - f1: 0.5697
[05/15/22 23:34:27] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.658, f1  custom_models.py:172
                             - 0.369, val_loss - 0.631, val_f1 - 0.570                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (258, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:28.375231: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:2224"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 258
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 258
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:29.102303: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_37972"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:2244"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:29.906967: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_38190"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:2272"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/9 [==>...........................] - ETA: 2s - loss: 0.8537 - f1: 0.07695/9 [===============>..............] - ETA: 0s - loss: 0.7604 - f1: 0.23129/9 [==============================] - 0s 14ms/step - loss: 0.6453 - f1: 0.5729
[05/15/22 23:34:30] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.653, f1  custom_models.py:172
                             - 0.526, val_loss - 0.645, val_f1 - 0.573                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (184, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:30.960301: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:2296"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 184
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 184
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:31.500477: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_38614"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:2316"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:32.125111: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_38820"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:2344"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/6 [====>.........................] - ETA: 1s - loss: 0.7455 - f1: 0.09095/6 [========================>.....] - ETA: 0s - loss: 0.6679 - f1: 0.47676/6 [==============================] - 0s 14ms/step - loss: 0.6547 - f1: 0.5640
[05/15/22 23:34:32] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.656, f1  custom_models.py:172
                             - 0.557, val_loss - 0.655, val_f1 - 0.564                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (92, 30,     custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:32.769564: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:2368"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 92
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 92
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:33.095684: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_39229"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:2388"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:33.476073: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_39423"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:2416"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/3 [=========>....................] - ETA: 0s - loss: 0.6456 - f1: 0.16673/3 [==============================] - 0s 14ms/step - loss: 0.6271 - f1: 0.6498
[05/15/22 23:34:33] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.642, f1  custom_models.py:172
                             - 0.614, val_loss - 0.627, val_f1 - 0.650                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (146, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:34.197070: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:2440"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 146
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 146
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:34.642959: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_39817"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:2460"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:35.168626: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_40019"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:2488"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/5 [=====>........................] - ETA: 0s - loss: 0.6452 - f1: 0.43945/5 [==============================] - ETA: 0s - loss: 0.6509 - f1: 0.57595/5 [==============================] - 0s 13ms/step - loss: 0.6509 - f1: 0.5759
[05/15/22 23:34:35] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.655, f1  custom_models.py:172
                             - 0.623, val_loss - 0.651, val_f1 - 0.576                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (146, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:35.951285: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:2512"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 146
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 146
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:36.391258: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_40423"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:2532"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:36.922319: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_40625"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:2560"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/5 [=====>........................] - ETA: 0s - loss: 0.7050 - f1: 0.07695/5 [==============================] - ETA: 0s - loss: 0.6883 - f1: 0.36105/5 [==============================] - 0s 13ms/step - loss: 0.6883 - f1: 0.3610
[05/15/22 23:34:37] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.697, f1  custom_models.py:172
                             - 0.566, val_loss - 0.688, val_f1 - 0.361                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (102, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:37.553047: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:2584"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 102
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 102
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:37.908417: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_41029"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:2604"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:38.291600: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_41227"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:2632"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/4 [======>.......................] - ETA: 0s - loss: 0.5580 - f1: 0.0000e+004/4 [==============================] - 0s 11ms/step - loss: 0.6280 - f1: 0.5261
[05/15/22 23:34:38] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.643, f1  custom_models.py:172
                             - 0.595, val_loss - 0.628, val_f1 - 0.526                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (214, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:39.279177: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:2656"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 214
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 214
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:39.885258: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_41626"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:2676"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:40.575461: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_41836"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:2704"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/7 [===>..........................] - ETA: 1s - loss: 0.4474 - f1: 0.0000e+005/7 [====================>.........] - ETA: 0s - loss: 0.5736 - f1: 0.0000e+007/7 [==============================] - 0s 14ms/step - loss: 0.6266 - f1: 0.0000e+00
[05/15/22 23:34:40] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.634, f1  custom_models.py:172
                             - 0.287, val_loss - 0.627, val_f1 - 0.000                                                              
[05/15/22 23:34:41] DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (142, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:41.441804: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:2728"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 142
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 142
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:41.874158: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_42250"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:2748"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:42.372868: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_42452"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:2776"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/5 [=====>........................] - ETA: 0s - loss: 0.5661 - f1: 0.0000e+005/5 [==============================] - 0s 12ms/step - loss: 0.6096 - f1: 0.4150
[05/15/22 23:34:42] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.634, f1  custom_models.py:172
                             - 0.000, val_loss - 0.610, val_f1 - 0.415                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (122, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:43.091777: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:2800"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 122
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 122
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:43.481020: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_42856"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:2820"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:43.916048: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_43054"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:2848"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/4 [======>.......................] - ETA: 0s - loss: 0.5823 - f1: 0.20004/4 [==============================] - 0s 14ms/step - loss: 0.6585 - f1: 0.2547
[05/15/22 23:34:44] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 0: loss - 0.670, f1  custom_models.py:172
                             - 0.192, val_loss - 0.659, val_f1 - 0.255                                                              
                    DEBUG    h5py._conv[720466] MainProcess(MainThread) attrs:199  Creating converter from 5 to 3       attrs.py:199
                    INFO     root[720466] MainProcess(MainThread) custom_models:143  Epoch 1                    custom_models.py:143
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (146, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:44.781418: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:2872"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 146
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 146
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:45.229678: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_43528"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:2892"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:45.741033: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_43730"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:2920"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/5 [=====>........................] - ETA: 0s - loss: 0.7802 - f1: 0.12135/5 [==============================] - ETA: 0s - loss: 0.6842 - f1: 0.57985/5 [==============================] - 0s 13ms/step - loss: 0.6842 - f1: 0.5798
[05/15/22 23:34:45] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.685, f1  custom_models.py:172
                             - 0.450, val_loss - 0.684, val_f1 - 0.580                                                              
[05/15/22 23:34:46] DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (252, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:46.880710: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:2944"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 252
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 252
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:47.932274: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_44134"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:2964"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:48.767789: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_44348"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:2992"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/8 [==>...........................] - ETA: 1s - loss: 0.5422 - f1: 0.25005/8 [=================>............] - ETA: 0s - loss: 0.5842 - f1: 0.45338/8 [==============================] - 0s 14ms/step - loss: 0.5669 - f1: 0.6583
[05/15/22 23:34:49] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.581, f1  custom_models.py:172
                             - 0.678, val_loss - 0.567, val_f1 - 0.658                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (260, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:50.061090: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:3016"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 260
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 260
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:50.829854: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_44767"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:3036"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:51.652483: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_44985"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:3064"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/9 [==>...........................] - ETA: 2s - loss: 0.6526 - f1: 0.0000e+004/9 [============>.................] - ETA: 0s - loss: 0.6365 - f1: 0.0903    8/9 [=========================>....] - ETA: 0s - loss: 0.6218 - f1: 0.53559/9 [==============================] - 0s 14ms/step - loss: 0.6208 - f1: 0.5871
[05/15/22 23:34:52] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.641, f1  custom_models.py:172
                             - 0.485, val_loss - 0.621, val_f1 - 0.587                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (186, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:52.741196: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:3088"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 186
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 186
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:53.325407: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_45409"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:3108"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:53.959911: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_45615"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:3136"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/6 [====>.........................] - ETA: 1s - loss: 0.5839 - f1: 0.25005/6 [========================>.....] - ETA: 0s - loss: 0.5832 - f1: 0.50876/6 [==============================] - 0s 14ms/step - loss: 0.5744 - f1: 0.5906
[05/15/22 23:34:54] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.585, f1  custom_models.py:172
                             - 0.670, val_loss - 0.574, val_f1 - 0.591                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (142, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:54.787787: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:3160"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 142
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 142
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:55.224653: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_46024"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:3180"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:55.736744: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_46226"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:3208"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/5 [=====>........................] - ETA: 0s - loss: 0.6106 - f1: 0.33335/5 [==============================] - ETA: 0s - loss: 0.6647 - f1: 0.06675/5 [==============================] - 0s 13ms/step - loss: 0.6647 - f1: 0.0667
[05/15/22 23:34:55] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.661, f1  custom_models.py:172
                             - 0.239, val_loss - 0.665, val_f1 - 0.067                                                              
[05/15/22 23:34:56] DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (116, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:56.426035: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:3232"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 116
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 116
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:56.804868: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_46630"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:3252"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:57.253390: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_46828"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:3280"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/4 [======>.......................] - ETA: 0s - loss: 0.5146 - f1: 0.50004/4 [==============================] - 0s 12ms/step - loss: 0.6087 - f1: 0.7019
[05/15/22 23:34:57] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.627, f1  custom_models.py:172
                             - 0.761, val_loss - 0.609, val_f1 - 0.702                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (76, 30,     custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:57.765540: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:3304"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 76
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 76
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:58.065332: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_47227"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:3324"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:34:58.405987: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_47421"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:3352"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/3 [=========>....................] - ETA: 0s - loss: 0.5814 - f1: 0.50003/3 [==============================] - 0s 9ms/step - loss: 0.6296 - f1: 0.8333
[05/15/22 23:34:58] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.640, f1  custom_models.py:172
                             - 0.455, val_loss - 0.630, val_f1 - 0.833                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (146, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:34:59.130197: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:3376"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 146
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 146
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:34:59.581837: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_47815"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:3396"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:00.101778: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_48017"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:3424"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/5 [=====>........................] - ETA: 0s - loss: 0.6022 - f1: 0.40005/5 [==============================] - ETA: 0s - loss: 0.6266 - f1: 0.55215/5 [==============================] - 0s 14ms/step - loss: 0.6266 - f1: 0.5521
[05/15/22 23:35:00] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.633, f1  custom_models.py:172
                             - 0.651, val_loss - 0.627, val_f1 - 0.552                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (258, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:35:01.298243: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:3448"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 258
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 258
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:02.037962: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_48421"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:3468"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:02.856680: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_48639"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:3496"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/9 [==>...........................] - ETA: 2s - loss: 0.4961 - f1: 0.0000e+005/9 [===============>..............] - ETA: 0s - loss: 0.5814 - f1: 0.0000e+009/9 [==============================] - 0s 14ms/step - loss: 0.6741 - f1: 0.0000e+00
[05/15/22 23:35:03] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.679, f1  custom_models.py:172
                             - 0.000, val_loss - 0.674, val_f1 - 0.000                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (222, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:35:04.053931: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:3520"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 222
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 222
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:04.689307: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_49063"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:3540"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:05.394456: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_49273"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:3568"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/7 [===>..........................] - ETA: 1s - loss: 0.5890 - f1: 0.0000e+004/7 [================>.............] - ETA: 0s - loss: 0.5903 - f1: 0.0000e+007/7 [==============================] - 0s 15ms/step - loss: 0.6653 - f1: 0.0000e+00
[05/15/22 23:35:05] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.675, f1  custom_models.py:172
                             - 0.000, val_loss - 0.665, val_f1 - 0.000                                                              
[05/15/22 23:35:06] DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (230, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:35:06.574831: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:3592"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 230
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 230
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:07.237911: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_49687"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:3612"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:07.978608: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_49901"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:3640"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/8 [==>...........................] - ETA: 1s - loss: 0.6438 - f1: 0.0000e+005/8 [=================>............] - ETA: 0s - loss: 0.6554 - f1: 0.0000e+008/8 [==============================] - 0s 14ms/step - loss: 0.6974 - f1: 0.0000e+00
[05/15/22 23:35:08] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.700, f1  custom_models.py:172
                             - 0.000, val_loss - 0.697, val_f1 - 0.000                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (204, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:35:09.075989: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:3664"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 204
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 204
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:09.662295: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_50320"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:3684"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:10.332411: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_50530"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:3712"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/7 [===>..........................] - ETA: 1s - loss: 0.6210 - f1: 0.0000e+004/7 [================>.............] - ETA: 0s - loss: 0.6327 - f1: 0.0167    7/7 [==============================] - 0s 15ms/step - loss: 0.6720 - f1: 0.0458
[05/15/22 23:35:10] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.676, f1  custom_models.py:172
                             - 0.121, val_loss - 0.672, val_f1 - 0.046                                                              
[05/15/22 23:35:11] DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (214, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:35:11.473009: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:3736"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 214
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 214
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:12.114128: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_50944"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:3756"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:12.816513: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_51154"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:3784"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/7 [===>..........................] - ETA: 1s - loss: 0.5696 - f1: 0.50005/7 [====================>.........] - ETA: 0s - loss: 0.6441 - f1: 0.10007/7 [==============================] - 0s 14ms/step - loss: 0.6650 - f1: 0.0882
[05/15/22 23:35:13] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.671, f1  custom_models.py:172
                             - 0.000, val_loss - 0.665, val_f1 - 0.088                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (200, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:35:13.866293: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:3808"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 200
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 200
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:14.482211: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_51568"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:3828"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:15.135247: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_51778"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:3856"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/7 [===>..........................] - ETA: 1s - loss: 0.6391 - f1: 0.14294/7 [================>.............] - ETA: 0s - loss: 0.6375 - f1: 0.29857/7 [==============================] - 0s 15ms/step - loss: 0.6221 - f1: 0.5991
[05/15/22 23:35:15] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.636, f1  custom_models.py:172
                             - 0.641, val_loss - 0.622, val_f1 - 0.599                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (192, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:35:16.160958: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:3880"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 192
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 192
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:16.746099: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_52187"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:3900"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 32
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:17.355443: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_52390"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:3926"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 32
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/6 [====>.........................] - ETA: 1s - loss: 0.6319 - f1: 0.09096/6 [==============================] - 0s 8ms/step - loss: 0.6751 - f1: 0.5347
[05/15/22 23:35:17] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.684, f1  custom_models.py:172
                             - 0.201, val_loss - 0.675, val_f1 - 0.535                                                              
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (128, 30,    custom_models.py:148
                             9216)                                                                                                  
2022-05-15 23:35:18.088074: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:3948"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 128
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 128
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:18.505929: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_52796"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:3968"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 32
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:18.955537: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_52991"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:3994"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 32
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/4 [======>.......................] - ETA: 0s - loss: 0.6881 - f1: 0.22224/4 [==============================] - 0s 8ms/step - loss: 0.6264 - f1: 0.5940
[05/15/22 23:35:19] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.642, f1  custom_models.py:172
                             - 0.600, val_loss - 0.626, val_f1 - 0.594                                                              
                    DEBUG    root[720466]                     custom_models.py:148
                             MainProcess(MainThread)                              
                             custom_models:148  X_train shape                     
                             (112, 30, 9216)                                      
2022-05-15 23:35:19.580780: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:4016"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 112
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 112
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:19.949337: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_53392"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:4036"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:20.388244: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_53590"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:4064"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/4 [======>.......................] - ETA: 0s - loss: 0.7348 - f1: 0.16674/4 [==============================] - 0s 14ms/step - loss: 0.6307 - f1: 0.6632
[05/15/22 23:35:20] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.636, f1 - 0.508, val_loss - 0.631,   custom_models.py:172
                             val_f1 - 0.663                                                                                                                     
[05/15/22 23:35:21] DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (232, 30, 9216)                          custom_models.py:148
2022-05-15 23:35:21.446535: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:4088"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 232
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 232
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:22.094768: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_53989"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:4108"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:22.857229: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_54203"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:4136"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/8 [==>...........................] - ETA: 1s - loss: 0.7373 - f1: 0.11115/8 [=================>............] - ETA: 0s - loss: 0.6920 - f1: 0.34738/8 [==============================] - 0s 13ms/step - loss: 0.6342 - f1: 0.5921
[05/15/22 23:35:23] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.628, f1 - 0.561, val_loss - 0.634,   custom_models.py:172
                             val_f1 - 0.592                                                                                                                     
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (218, 30, 9216)                          custom_models.py:148
2022-05-15 23:35:23.996019: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:4160"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 218
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 218
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:24.612293: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_54622"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:4180"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:25.339166: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_54832"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:4208"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/7 [===>..........................] - ETA: 1s - loss: 0.8989 - f1: 0.12134/7 [================>.............] - ETA: 0s - loss: 0.8407 - f1: 0.19617/7 [==============================] - 0s 17ms/step - loss: 0.7157 - f1: 0.5406
[05/15/22 23:35:25] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.716, f1 - 0.526, val_loss - 0.716,   custom_models.py:172
                             val_f1 - 0.541                                                                                                                     
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (154, 30, 9216)                          custom_models.py:148
2022-05-15 23:35:26.255339: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:4232"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 154
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 154
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:26.733173: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_55246"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:4252"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:27.286121: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_55448"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:4280"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/5 [=====>........................] - ETA: 0s - loss: 0.7131 - f1: 0.49335/5 [==============================] - ETA: 0s - loss: 0.6871 - f1: 0.57485/5 [==============================] - 0s 14ms/step - loss: 0.6871 - f1: 0.5748
[05/15/22 23:35:27] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.688, f1 - 0.604, val_loss - 0.687,   custom_models.py:172
                             val_f1 - 0.575                                                                                                                     
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (112, 30, 9216)                          custom_models.py:148
2022-05-15 23:35:27.979558: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:4304"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 112
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 112
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:28.349643: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_55852"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:4324"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:28.767783: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_56050"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:4352"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/4 [======>.......................] - ETA: 0s - loss: 0.7065 - f1: 0.50004/4 [==============================] - 0s 13ms/step - loss: 0.6445 - f1: 0.6625
[05/15/22 23:35:29] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.651, f1 - 0.636, val_loss - 0.644,   custom_models.py:172
                             val_f1 - 0.662                                                                                                                     
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (120, 30, 9216)                          custom_models.py:148
2022-05-15 23:35:29.446623: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:4376"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 120
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 120
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:29.849429: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_56449"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:4396"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:30.293129: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_56647"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:4424"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/4 [======>.......................] - ETA: 0s - loss: 0.6902 - f1: 0.54184/4 [==============================] - 0s 13ms/step - loss: 0.6742 - f1: 0.6054
[05/15/22 23:35:30] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.683, f1 - 0.635, val_loss - 0.674,   custom_models.py:172
                             val_f1 - 0.605                                                                                                                     
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (120, 30, 9216)                          custom_models.py:148
2022-05-15 23:35:30.975966: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:4448"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 120
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 120
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:31.370998: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_57046"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:4468"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:31.812088: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_57244"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:4496"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/4 [======>.......................] - ETA: 0s - loss: 0.7121 - f1: 0.51494/4 [==============================] - 0s 12ms/step - loss: 0.6806 - f1: 0.5705
[05/15/22 23:35:32] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.681, f1 - 0.601, val_loss - 0.681,   custom_models.py:172
                             val_f1 - 0.570                                                                                                                     
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (92, 30, 9216)                           custom_models.py:148
2022-05-15 23:35:32.395433: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:4520"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 92
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 92
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:32.718483: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_57643"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:4540"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:33.078921: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_57837"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:4568"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/3 [=========>....................] - ETA: 0s - loss: 0.5982 - f1: 0.20003/3 [==============================] - 0s 12ms/step - loss: 0.6233 - f1: 0.6222
[05/15/22 23:35:33] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.636, f1 - 0.625, val_loss - 0.623,   custom_models.py:172
                             val_f1 - 0.622                                                                                                                     
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (138, 30, 9216)                          custom_models.py:148
2022-05-15 23:35:33.768019: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:4592"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 138
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 138
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:34.220174: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_58231"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:4612"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:34.710615: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_58433"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:4640"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/5 [=====>........................] - ETA: 0s - loss: 0.6641 - f1: 0.0000e+005/5 [==============================] - 0s 12ms/step - loss: 0.6579 - f1: 0.4862
[05/15/22 23:35:34] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.673, f1 - 0.332, val_loss - 0.658,   custom_models.py:172
                             val_f1 - 0.486                                                                                                                     
[05/15/22 23:35:35] DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (102, 30, 9216)                          custom_models.py:148
2022-05-15 23:35:35.334959: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:4664"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 102
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 102
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:35.703471: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_58837"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:4684"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:36.092289: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_59035"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:4712"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/4 [======>.......................] - ETA: 0s - loss: 0.6672 - f1: 0.12504/4 [==============================] - 0s 14ms/step - loss: 0.6596 - f1: 0.6099
[05/15/22 23:35:36] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.666, f1 - 0.622, val_loss - 0.660,   custom_models.py:172
                             val_f1 - 0.610                                                                                                                     
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (136, 30, 9216)                          custom_models.py:148
2022-05-15 23:35:36.795943: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:4736"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 136
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 136
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:37.252617: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_59434"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:4756"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:37.753402: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_59636"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:4784"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/5 [=====>........................] - ETA: 0s - loss: 0.6069 - f1: 0.12505/5 [==============================] - 0s 12ms/step - loss: 0.6470 - f1: 0.3615
[05/15/22 23:35:37] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.639, f1 - 0.696, val_loss - 0.647,   custom_models.py:172
                             val_f1 - 0.362                                                                                                                     
[05/15/22 23:35:38] DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (184, 30, 9216)                          custom_models.py:148
2022-05-15 23:35:38.658729: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:4808"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 184
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 184
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:39.204909: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_60040"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:4828"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:39.814672: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_60246"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:4856"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/6 [====>.........................] - ETA: 1s - loss: 0.6561 - f1: 0.14295/6 [========================>.....] - ETA: 0s - loss: 0.6774 - f1: 0.24216/6 [==============================] - 0s 14ms/step - loss: 0.6801 - f1: 0.2479
[05/15/22 23:35:40] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.679, f1 - 0.549, val_loss - 0.680,   custom_models.py:172
                             val_f1 - 0.248                                                                                                                     
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (206, 30, 9216)                          custom_models.py:148
2022-05-15 23:35:40.856697: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:4880"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 206
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 206
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:41.451637: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_60655"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:4900"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:42.140024: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_60865"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:4928"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/7 [===>..........................] - ETA: 1s - loss: 0.6639 - f1: 0.20005/7 [====================>.........] - ETA: 0s - loss: 0.6847 - f1: 0.06357/7 [==============================] - 0s 13ms/step - loss: 0.6956 - f1: 0.0454
[05/15/22 23:35:42] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.696, f1 - 0.083, val_loss - 0.696,   custom_models.py:172
                             val_f1 - 0.045                                                                                                                     
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (122, 30, 9216)                          custom_models.py:148
2022-05-15 23:35:42.908494: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:4952"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 122
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 122
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:43.309561: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_61279"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:4972"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:43.781378: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_61477"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:5000"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/4 [======>.......................] - ETA: 0s - loss: 0.6422 - f1: 0.20004/4 [==============================] - 0s 13ms/step - loss: 0.6955 - f1: 0.0500
[05/15/22 23:35:44] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.696, f1 - 0.000, val_loss - 0.695,   custom_models.py:172
                             val_f1 - 0.050                                                                                                                     
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (80, 30, 9216)                           custom_models.py:148
2022-05-15 23:35:44.324710: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:5024"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 80
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 80
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:44.614281: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_61876"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:5044"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:44.969671: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_62070"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:5072"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/3 [=========>....................] - ETA: 0s - loss: 0.6670 - f1: 0.66673/3 [==============================] - 0s 11ms/step - loss: 0.6755 - f1: 0.7120
[05/15/22 23:35:45] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.676, f1 - 0.691, val_loss - 0.675,   custom_models.py:172
                             val_f1 - 0.712                                                                                                                     
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (190, 30, 9216)                          custom_models.py:148
2022-05-15 23:35:45.835996: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:5096"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 190
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 190
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:46.388124: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_62464"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:5116"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:47.027771: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_62670"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:5144"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/6 [====>.........................] - ETA: 1s - loss: 0.6544 - f1: 0.20005/6 [========================>.....] - ETA: 0s - loss: 0.6554 - f1: 0.45676/6 [==============================] - 0s 15ms/step - loss: 0.6542 - f1: 0.5472
[05/15/22 23:35:47] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.657, f1 - 0.608, val_loss - 0.654,   custom_models.py:172
                             val_f1 - 0.547                                                                                                                     
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (104, 30, 9216)                          custom_models.py:148
2022-05-15 23:35:47.728557: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:5168"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 104
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 104
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:48.075677: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_63079"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:5188"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:48.472097: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_63277"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:5216"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/4 [======>.......................] - ETA: 0s - loss: 0.7123 - f1: 0.25104/4 [==============================] - 0s 11ms/step - loss: 0.6868 - f1: 0.6544
[05/15/22 23:35:48] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.687, f1 - 0.600, val_loss - 0.687,   custom_models.py:172
                             val_f1 - 0.654                                                                                                                     
[05/15/22 23:35:49] DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (272, 30, 9216)                          custom_models.py:148
2022-05-15 23:35:49.649001: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:5240"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 272
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 272
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:50.410319: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_63676"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:5260"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:51.246367: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_63894"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:5288"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/9 [==>...........................] - ETA: 2s - loss: 0.6778 - f1: 0.12505/9 [===============>..............] - ETA: 0s - loss: 0.6999 - f1: 0.19509/9 [==============================] - ETA: 0s - loss: 0.6861 - f1: 0.55289/9 [==============================] - 0s 15ms/step - loss: 0.6861 - f1: 0.5528
[05/15/22 23:35:51] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.687, f1 - 0.527, val_loss - 0.686,   custom_models.py:172
                             val_f1 - 0.553                                                                                                                     
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (146, 30, 9216)                          custom_models.py:148
2022-05-15 23:35:52.192192: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:5312"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 146
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 146
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:52.642269: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_64318"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:5332"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:53.145266: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_64520"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:5360"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/5 [=====>........................] - ETA: 0s - loss: 0.6660 - f1: 0.38465/5 [==============================] - ETA: 0s - loss: 0.6605 - f1: 0.59085/5 [==============================] - 0s 13ms/step - loss: 0.6605 - f1: 0.5908
[05/15/22 23:35:53] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.662, f1 - 0.780, val_loss - 0.660,   custom_models.py:172
                             val_f1 - 0.591                                                                                                                     
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (190, 30, 9216)                          custom_models.py:148
2022-05-15 23:35:54.099728: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:5384"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 190
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 190
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:54.665211: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_64924"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:5404"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:55.289597: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_65130"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:5432"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/6 [====>.........................] - ETA: 1s - loss: 0.6588 - f1: 0.33334/6 [===================>..........] - ETA: 0s - loss: 0.6654 - f1: 0.11376/6 [==============================] - 0s 15ms/step - loss: 0.6767 - f1: 0.1467
[05/15/22 23:35:55] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.679, f1 - 0.000, val_loss - 0.677,   custom_models.py:172
                             val_f1 - 0.147                                                                                                                     
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (180, 30, 9216)                          custom_models.py:148
2022-05-15 23:35:56.243225: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:5456"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 180
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 180
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:56.799524: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_65539"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:5476"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:35:57.393829: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_65745"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:5504"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/6 [====>.........................] - ETA: 1s - loss: 0.6479 - f1: 0.0000e+005/6 [========================>.....] - ETA: 0s - loss: 0.6587 - f1: 0.4151    6/6 [==============================] - 0s 13ms/step - loss: 0.6624 - f1: 0.4825
[05/15/22 23:35:57] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.674, f1 - 0.318, val_loss - 0.662,   custom_models.py:172
                             val_f1 - 0.483                                                                                                                     
[05/15/22 23:35:58] DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (258, 30, 9216)                          custom_models.py:148
2022-05-15 23:35:58.581873: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:5528"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 258
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 258
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:35:59.307637: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_66154"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:5548"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:00.102328: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_66372"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:5576"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/9 [==>...........................] - ETA: 2s - loss: 0.6293 - f1: 0.20005/9 [===============>..............] - ETA: 0s - loss: 0.6581 - f1: 0.25959/9 [==============================] - 0s 13ms/step - loss: 0.6532 - f1: 0.5886
[05/15/22 23:36:00] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.663, f1 - 0.542, val_loss - 0.653,   custom_models.py:172
                             val_f1 - 0.589                                                                                                                     
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (266, 30, 9216)                          custom_models.py:148
2022-05-15 23:36:01.417821: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:5600"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 266
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 266
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:36:02.178387: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_66796"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:5620"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:03.006273: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_67014"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:5648"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/9 [==>...........................] - ETA: 2s - loss: 0.6582 - f1: 0.14294/9 [============>.................] - ETA: 0s - loss: 0.6075 - f1: 0.08578/9 [=========================>....] - ETA: 0s - loss: 0.6347 - f1: 0.43249/9 [==============================] - 0s 15ms/step - loss: 0.6349 - f1: 0.4831
[05/15/22 23:36:03] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.636, f1 - 0.483, val_loss - 0.635,   custom_models.py:172
                             val_f1 - 0.483                                                                                                                     
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (146, 30, 9216)                          custom_models.py:148
2022-05-15 23:36:03.929033: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:5672"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 146
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 146
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:36:04.393598: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_67438"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:5692"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:04.915058: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_67640"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:5720"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/5 [=====>........................] - ETA: 0s - loss: 0.4816 - f1: 0.0000e+004/5 [=======================>......] - ETA: 0s - loss: 0.6048 - f1: 0.4531    5/5 [==============================] - 0s 15ms/step - loss: 0.6088 - f1: 0.5507
[05/15/22 23:36:05] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 1: loss - 0.613, f1 - 0.631, val_loss - 0.609,   custom_models.py:172
                             val_f1 - 0.551                                                                                                                     
                    INFO     root[720466] MainProcess(MainThread) custom_models:143  Epoch 2                                                custom_models.py:143
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (272, 30, 9216)                          custom_models.py:148
2022-05-15 23:36:06.128951: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:5744"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 272
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 272
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:36:06.891124: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_68044"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:5764"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:07.737138: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_68262"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:5792"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/9 [==>...........................] - ETA: 2s - loss: 0.6265 - f1: 0.0000e+005/9 [===============>..............] - ETA: 0s - loss: 0.6732 - f1: 0.1286    9/9 [==============================] - ETA: 0s - loss: 0.6840 - f1: 0.33509/9 [==============================] - 0s 14ms/step - loss: 0.6840 - f1: 0.3350
[05/15/22 23:36:08] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 2: loss - 0.687, f1 - 0.319, val_loss - 0.684,   custom_models.py:172
                             val_f1 - 0.335                                                                                                                     
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (222, 30, 9216)                          custom_models.py:148
2022-05-15 23:36:08.926575: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:5816"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 222
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 222
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:36:09.552175: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_68686"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:5836"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:10.279753: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_68896"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:5864"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/7 [===>..........................] - ETA: 1s - loss: 0.5833 - f1: 0.0000e+005/7 [====================>.........] - ETA: 0s - loss: 0.5646 - f1: 0.3533    7/7 [==============================] - 0s 14ms/step - loss: 0.6007 - f1: 0.4799
[05/15/22 23:36:10] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 2: loss - 0.610, f1 - 0.543, val_loss - 0.601,   custom_models.py:172
                             val_f1 - 0.480                                                                                                                     
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (122, 30, 9216)                          custom_models.py:148
2022-05-15 23:36:11.061105: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:5888"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 122
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 122
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:36:11.455881: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_69310"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:5908"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:11.927629: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_69508"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:5936"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/4 [======>.......................] - ETA: 0s - loss: 0.5667 - f1: 0.16674/4 [==============================] - 0s 14ms/step - loss: 0.6368 - f1: 0.2688
[05/15/22 23:36:12] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 2: loss - 0.658, f1 - 0.319, val_loss - 0.637,   custom_models.py:172
                             val_f1 - 0.269                                                                                                                     
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (260, 30, 9216)                          custom_models.py:148
2022-05-15 23:36:13.089893: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:5960"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 260
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 260
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:36:13.820961: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_69907"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:5980"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:14.662586: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_70125"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:6008"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/9 [==>...........................] - ETA: 2s - loss: 0.7311 - f1: 0.0000e+004/9 [============>.................] - ETA: 0s - loss: 0.7059 - f1: 0.0333    8/9 [=========================>....] - ETA: 0s - loss: 0.7011 - f1: 0.34929/9 [==============================] - 0s 15ms/step - loss: 0.7008 - f1: 0.3845
[05/15/22 23:36:15] INFO     root[720466] MainProcess(MainThread) custom_models:172  EPOCH 2: loss - 0.759, f1 - 0.000, val_loss - 0.701,   custom_models.py:172
                             val_f1 - 0.385                                                                                                                     
                    DEBUG    root[720466] MainProcess(MainThread) custom_models:148  X_train shape (190, 30, 9216)                          custom_models.py:148
2022-05-15 23:36:15.758956: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:6032"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 190
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 190
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:36:16.325720: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_70549"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:6052"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:16.958796: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_70755"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:6080"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/6 [====>.........................] - ETA: 1s - loss: 0.7474 - f1: 0.07145/6 [========================>.....] - ETA: 0s - loss: 0.6219 - f1: 0.43106/6 [==============================] - 0s 14ms/step - loss: 0.6025 - f1: 0.5258
[05/1… INFO   r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              7        
              2        
                       
                       
              E        
              P        
              O        
              C        
              H        
                       
              2        
              :        
                       
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              6        
              1        
              8        
              ,        
                       
              f        
              1        
                       
              -        
              0        
              .        
              5        
              3        
              7        
              ,        
                       
              v        
              a        
              l        
              _        
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              6        
              0        
              2        
              ,        
                       
              v        
              a        
              l        
              _        
              f        
              1        
                       
              -        
              0        
              .        
              5        
              2        
              6        
       DEBUG  r custom…
              o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              4        
              8        
                       
                       
              X        
              _        
              t        
              r        
              a        
              i        
              n        
                       
              s        
              h        
              a        
              p        
              e        
                       
              (        
              2        
              0        
              4        
              ,        
                       
              3        
              0        
              ,        
                       
              9        
              2        
              1        
              6        
              )        
2022-05-15 23:36:18.018650: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:6104"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 204
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 204
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:36:18.618813: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_71164"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:6124"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:19.291488: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_71374"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:6152"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/7 [===>..........................] - ETA: 1s - loss: 0.8099 - f1: 0.05884/7 [================>.............] - ETA: 0s - loss: 0.7472 - f1: 0.32157/7 [==============================] - 0s 16ms/step - loss: 0.6673 - f1: 0.6123
[05/1… INFO   r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              7        
              2        
                       
                       
              E        
              P        
              O        
              C        
              H        
                       
              2        
              :        
                       
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              6        
              6        
              6        
              ,        
                       
              f        
              1        
                       
              -        
              0        
              .        
              5        
              5        
              2        
              ,        
                       
              v        
              a        
              l        
              _        
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              6        
              6        
              7        
              ,        
                       
              v        
              a        
              l        
              _        
              f        
              1        
                       
              -        
              0        
              .        
              6        
              1        
              2        
[05/1… DEBUG  r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              4        
              8        
                       
                       
              X        
              _        
              t        
              r        
              a        
              i        
              n        
                       
              s        
              h        
              a        
              p        
              e        
                       
              (        
              2        
              5        
              2        
              ,        
                       
              3        
              0        
              ,        
                       
              9        
              2        
              1        
              6        
              )        
2022-05-15 23:36:20.541803: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:6176"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 252
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 252
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:36:21.234448: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_71788"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:6196"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:22.010501: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_72002"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:6224"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/8 [==>...........................] - ETA: 1s - loss: 0.5828 - f1: 0.0000e+005/8 [=================>............] - ETA: 0s - loss: 0.6044 - f1: 0.3033    8/8 [==============================] - 0s 15ms/step - loss: 0.5995 - f1: 0.5334
[05/1… INFO   r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              7        
              2        
                       
                       
              E        
              P        
              O        
              C        
              H        
                       
              2        
              :        
                       
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              6        
              2        
              2        
              ,        
                       
              f        
              1        
                       
              -        
              0        
              .        
              6        
              2        
              4        
              ,        
                       
              v        
              a        
              l        
              _        
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              5        
              9        
              9        
              ,        
                       
              v        
              a        
              l        
              _        
              f        
              1        
                       
              -        
              0        
              .        
              5        
              3        
              3        
       DEBUG  r custom…
              o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              4        
              8        
                       
                       
              X        
              _        
              t        
              r        
              a        
              i        
              n        
                       
              s        
              h        
              a        
              p        
              e        
                       
              (        
              1        
              9        
              2        
              ,        
                       
              3        
              0        
              ,        
                       
              9        
              2        
              1        
              6        
              )        
2022-05-15 23:36:23.099311: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:6248"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 192
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 192
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:36:23.659336: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_72416"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:6268"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 32
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:24.261321: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_72619"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:6294"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 32
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/6 [====>.........................] - ETA: 1s - loss: 0.5313 - f1: 0.0000e+006/6 [==============================] - 0s 8ms/step - loss: 0.6728 - f1: 0.3491
[05/1… INFO   r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              7        
              2        
                       
                       
              E        
              P        
              O        
              C        
              H        
                       
              2        
              :        
                       
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              6        
              6        
              5        
              ,        
                       
              f        
              1        
                       
              -        
              0        
              .        
              5        
              4        
              5        
              ,        
                       
              v        
              a        
              l        
              _        
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              6        
              7        
              3        
              ,        
                       
              v        
              a        
              l        
              _        
              f        
              1        
                       
              -        
              0        
              .        
              3        
              4        
              9        
       DEBUG  r custom…
              o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              4        
              8        
                       
                       
              X        
              _        
              t        
              r        
              a        
              i        
              n        
                       
              s        
              h        
              a        
              p        
              e        
                       
              (        
              1        
              8        
              6        
              ,        
                       
              3        
              0        
              ,        
                       
              9        
              2        
              1        
              6        
              )        
2022-05-15 23:36:25.221134: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:6316"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 186
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 186
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:36:25.759596: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_73030"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:6336"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:26.370889: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_73236"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:6364"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/6 [====>.........................] - ETA: 1s - loss: 0.5680 - f1: 0.25005/6 [========================>.....] - ETA: 0s - loss: 0.5928 - f1: 0.48496/6 [==============================] - 0s 15ms/step - loss: 0.5967 - f1: 0.5530
[05/1… INFO   r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              7        
              2        
                       
                       
              E        
              P        
              O        
              C        
              H        
                       
              2        
              :        
                       
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              5        
              9        
              4        
              ,        
                       
              f        
              1        
                       
              -        
              0        
              .        
              6        
              6        
              9        
              ,        
                       
              v        
              a        
              l        
              _        
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              5        
              9        
              7        
              ,        
                       
              v        
              a        
              l        
              _        
              f        
              1        
                       
              -        
              0        
              .        
              5        
              5        
              3        
       DEBUG  r custom…
              o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              4        
              8        
                       
                       
              X        
              _        
              t        
              r        
              a        
              i        
              n        
                       
              s        
              h        
              a        
              p        
              e        
                       
              (        
              8        
              0        
              ,        
                       
              3        
              0        
              ,        
                       
              9        
              2        
              1        
              6        
              )        
2022-05-15 23:36:26.982529: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:6388"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 80
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 80
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:36:27.279898: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_73645"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:6408"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:27.608958: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_73839"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:6436"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/3 [=========>....................] - ETA: 0s - loss: 0.6777 - f1: 0.32143/3 [==============================] - 0s 10ms/step - loss: 0.6192 - f1: 0.7681
[05/1… INFO   r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              7        
              2        
                       
                       
              E        
              P        
              O        
              C        
              H        
                       
              2        
              :        
                       
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              6        
              2        
              1        
              ,        
                       
              f        
              1        
                       
              -        
              0        
              .        
              6        
              8        
              7        
              ,        
                       
              v        
              a        
              l        
              _        
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              6        
              1        
              9        
              ,        
                       
              v        
              a        
              l        
              _        
              f        
              1        
                       
              -        
              0        
              .        
              7        
              6        
              8        
[05/1… DEBUG  r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              4        
              8        
                       
                       
              X        
              _        
              t        
              r        
              a        
              i        
              n        
                       
              s        
              h        
              a        
              p        
              e        
                       
              (        
              2        
              3        
              2        
              ,        
                       
              3        
              0        
              ,        
                       
              9        
              2        
              1        
              6        
              )        
2022-05-15 23:36:28.640856: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:6460"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 232
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 232
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:36:29.307972: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_74233"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:6480"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:30.036054: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_74447"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:6508"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/8 [==>...........................] - ETA: 1s - loss: 0.6023 - f1: 0.16675/8 [=================>............] - ETA: 0s - loss: 0.6021 - f1: 0.35338/8 [==============================] - 0s 14ms/step - loss: 0.5849 - f1: 0.5918
[05/1… INFO   r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              7        
              2        
                       
                       
              E        
              P        
              O        
              C        
              H        
                       
              2        
              :        
                       
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              6        
              0        
              4        
              ,        
                       
              f        
              1        
                       
              -        
              0        
              .        
              4        
              6        
              6        
              ,        
                       
              v        
              a        
              l        
              _        
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              5        
              8        
              5        
              ,        
                       
              v        
              a        
              l        
              _        
              f        
              1        
                       
              -        
              0        
              .        
              5        
              9        
              2        
       DEBUG  r custom…
              o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              4        
              8        
                       
                       
              X        
              _        
              t        
              r        
              a        
              i        
              n        
                       
              s        
              h        
              a        
              p        
              e        
                       
              (        
              1        
              3        
              6        
              ,        
                       
              3        
              0        
              ,        
                       
              9        
              2        
              1        
              6        
              )        
2022-05-15 23:36:30.912527: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:6532"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 136
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 136
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:36:31.346347: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_74866"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:6552"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:31.839390: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_75068"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:6580"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/5 [=====>........................] - ETA: 0s - loss: 0.5742 - f1: 0.18185/5 [==============================] - 0s 12ms/step - loss: 0.5580 - f1: 0.6644
[05/1… INFO   r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              7        
              2        
                       
                       
              E        
              P        
              O        
              C        
              H        
                       
              2        
              :        
                       
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              5        
              6        
              1        
              ,        
                       
              f        
              1        
                       
              -        
              0        
              .        
              6        
              3        
              9        
              ,        
                       
              v        
              a        
              l        
              _        
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              5        
              5        
              8        
              ,        
                       
              v        
              a        
              l        
              _        
              f        
              1        
                       
              -        
              0        
              .        
              6        
              6        
              4        
       DEBUG  r custom…
              o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              4        
              8        
                       
                       
              X        
              _        
              t        
              r        
              a        
              i        
              n        
                       
              s        
              h        
              a        
              p        
              e        
                       
              (        
              1        
              4        
              2        
              ,        
                       
              3        
              0        
              ,        
                       
              9        
              2        
              1        
              6        
              )        
2022-05-15 23:36:32.618742: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:6604"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 142
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 142
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:36:33.058596: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_75472"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:6624"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:33.564135: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_75674"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:6652"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/5 [=====>........................] - ETA: 0s - loss: 0.6337 - f1: 0.20005/5 [==============================] - ETA: 0s - loss: 0.5936 - f1: 0.66575/5 [==============================] - 0s 13ms/step - loss: 0.5936 - f1: 0.6657
[05/1… INFO   r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              7        
              2        
                       
                       
              E        
              P        
              O        
              C        
              H        
                       
              2        
              :        
                       
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              6        
              0        
              6        
              ,        
                       
              f        
              1        
                       
              -        
              0        
              .        
              5        
              5        
              9        
              ,        
                       
              v        
              a        
              l        
              _        
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              5        
              9        
              4        
              ,        
                       
              v        
              a        
              l        
              _        
              f        
              1        
                       
              -        
              0        
              .        
              6        
              6        
              6        
[05/1… DEBUG  r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              4        
              8        
                       
                       
              X        
              _        
              t        
              r        
              a        
              i        
              n        
                       
              s        
              h        
              a        
              p        
              e        
                       
              (        
              1        
              2        
              0        
              ,        
                       
              3        
              0        
              ,        
                       
              9        
              2        
              1        
              6        
              )        
2022-05-15 23:36:34.281622: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:6676"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 120
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 120
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:36:34.670708: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_76078"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:6696"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:35.117591: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_76276"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:6724"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/4 [======>.......................] - ETA: 0s - loss: 0.6762 - f1: 0.58084/4 [==============================] - 0s 14ms/step - loss: 0.6120 - f1: 0.5516
[05/1… INFO   r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              7        
              2        
                       
                       
              E        
              P        
              O        
              C        
              H        
                       
              2        
              :        
                       
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              6        
              3        
              0        
              ,        
                       
              f        
              1        
                       
              -        
              0        
              .        
              7        
              1        
              5        
              ,        
                       
              v        
              a        
              l        
              _        
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              6        
              1        
              2        
              ,        
                       
              v        
              a        
              l        
              _        
              f        
              1        
                       
              -        
              0        
              .        
              5        
              5        
              2        
       DEBUG  r custom…
              o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              4        
              8        
                       
                       
              X        
              _        
              t        
              r        
              a        
              i        
              n        
                       
              s        
              h        
              a        
              p        
              e        
                       
              (        
              1        
              8        
              0        
              ,        
                       
              3        
              0        
              ,        
                       
              9        
              2        
              1        
              6        
              )        
2022-05-15 23:36:36.005722: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:6748"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 180
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 180
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:36:36.568445: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_76675"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:6768"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:37.169115: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_76881"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:6796"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/6 [====>.........................] - ETA: 1s - loss: 0.6453 - f1: 0.0000e+004/6 [===================>..........] - ETA: 0s - loss: 0.6235 - f1: 0.0000e+006/6 [==============================] - 0s 15ms/step - loss: 0.6814 - f1: 0.0000e+00
[05/1… INFO   r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              7        
              2        
                       
                       
              E        
              P        
              O        
              C        
              H        
                       
              2        
              :        
                       
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              6        
              8        
              4        
              ,        
                       
              f        
              1        
                       
              -        
              0        
              .        
              0        
              0        
              0        
              ,        
                       
              v        
              a        
              l        
              _        
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              6        
              8        
              1        
              ,        
                       
              v        
              a        
              l        
              _        
              f        
              1        
                       
              -        
              0        
              .        
              0        
              0        
              0        
       DEBUG  r custom…
              o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              4        
              8        
                       
                       
              X        
              _        
              t        
              r        
              a        
              i        
              n        
                       
              s        
              h        
              a        
              p        
              e        
                       
              (        
              7        
              6        
              ,        
                       
              3        
              0        
              ,        
                       
              9        
              2        
              1        
              6        
              )        
2022-05-15 23:36:37.778391: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:6820"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 76
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 76
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:36:38.064041: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_77290"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:6840"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:38.383875: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_77484"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:6868"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/3 [=========>....................] - ETA: 0s - loss: 0.5906 - f1: 0.42423/3 [==============================] - 0s 10ms/step - loss: 0.5944 - f1: 0.7896
[05/1… INFO   r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              7        
              2        
                       
                       
              E        
              P        
              O        
              C        
              H        
                       
              2        
              :        
                       
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              6        
              1        
              0        
              ,        
                       
              f        
              1        
                       
              -        
              0        
              .        
              5        
              3        
              8        
              ,        
                       
              v        
              a        
              l        
              _        
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              5        
              9        
              4        
              ,        
                       
              v        
              a        
              l        
              _        
              f        
              1        
                       
              -        
              0        
              .        
              7        
              9        
              0        
       DEBUG  r custom…
              o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              4        
              8        
                       
                       
              X        
              _        
              t        
              r        
              a        
              i        
              n        
                       
              s        
              h        
              a        
              p        
              e        
                       
              (        
              1        
              0        
              4        
              ,        
                       
              3        
              0        
              ,        
                       
              9        
              2        
              1        
              6        
              )        
2022-05-15 23:36:38.972153: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:6892"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 104
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 104
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:36:39.327355: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_77878"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:6912"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:39.721662: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_78076"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:6940"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/4 [======>.......................] - ETA: 0s - loss: 0.7927 - f1: 0.29864/4 [==============================] - 0s 11ms/step - loss: 0.6672 - f1: 0.6850
[05/1… INFO   r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              7        
              2        
                       
                       
              E        
              P        
              O        
              C        
              H        
                       
              2        
              :        
                       
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              6        
              6        
              4        
              ,        
                       
              f        
              1        
                       
              -        
              0        
              .        
              6        
              3        
              9        
              ,        
                       
              v        
              a        
              l        
              _        
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              6        
              6        
              7        
              ,        
                       
              v        
              a        
              l        
              _        
              f        
              1        
                       
              -        
              0        
              .        
              6        
              8        
              5        
[05/1… DEBUG  r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              4        
              8        
                       
                       
              X        
              _        
              t        
              r        
              a        
              i        
              n        
                       
              s        
              h        
              a        
              p        
              e        
                       
              (        
              2        
              5        
              8        
              ,        
                       
              3        
              0        
              ,        
                       
              9        
              2        
              1        
              6        
              )        
2022-05-15 23:36:40.883248: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:6964"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 258
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 258
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:36:41.614319: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_78475"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:6984"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:42.430428: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_78693"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:7012"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/9 [==>...........................] - ETA: 2s - loss: 0.6806 - f1: 0.11115/9 [===============>..............] - ETA: 0s - loss: 0.6504 - f1: 0.24189/9 [==============================] - 0s 13ms/step - loss: 0.5864 - f1: 0.5788
[05/1… INFO   r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              7        
              2        
                       
                       
              E        
              P        
              O        
              C        
              H        
                       
              2        
              :        
                       
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              5        
              9        
              8        
              ,        
                       
              f        
              1        
                       
              -        
              0        
              .        
              5        
              5        
              7        
              ,        
                       
              v        
              a        
              l        
              _        
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              5        
              8        
              6        
              ,        
                       
              v        
              a        
              l        
              _        
              f        
              1        
                       
              -        
              0        
              .        
              5        
              7        
              9        
[05/1… DEBUG  r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              4        
              8        
                       
                       
              X        
              _        
              t        
              r        
              a        
              i        
              n        
                       
              s        
              h        
              a        
              p        
              e        
                       
              (        
              1        
              2        
              0        
              ,        
                       
              3        
              0        
              ,        
                       
              9        
              2        
              1        
              6        
              )        
2022-05-15 23:36:43.269575: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:7036"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 120
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 120
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:36:43.655890: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_79117"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:7056"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:44.093797: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_79315"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:7084"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/4 [======>.......................] - ETA: 0s - loss: 0.6624 - f1: 0.58224/4 [==============================] - 0s 13ms/step - loss: 0.6561 - f1: 0.6139
[05/1… INFO   r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              7        
              2        
                       
                       
              E        
              P        
              O        
              C        
              H        
                       
              2        
              :        
                       
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              6        
              6        
              0        
              ,        
                       
              f        
              1        
                       
              -        
              0        
              .        
              6        
              5        
              8        
              ,        
                       
              v        
              a        
              l        
              _        
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              6        
              5        
              6        
              ,        
                       
              v        
              a        
              l        
              _        
              f        
              1        
                       
              -        
              0        
              .        
              6        
              1        
              4        
       DEBUG  r custom…
              o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              4        
              8        
                       
                       
              X        
              _        
              t        
              r        
              a        
              i        
              n        
                       
              s        
              h        
              a        
              p        
              e        
                       
              (        
              1        
              4        
              6        
              ,        
                       
              3        
              0        
              ,        
                       
              9        
              2        
              1        
              6        
              )        
2022-05-15 23:36:44.868220: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:7108"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 146
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 146
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:36:45.321662: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_79714"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:7128"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:45.826348: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_79916"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:7156"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/5 [=====>........................] - ETA: 0s - loss: 0.6369 - f1: 0.58465/5 [==============================] - ETA: 0s - loss: 0.5845 - f1: 0.64375/5 [==============================] - 0s 13ms/step - loss: 0.5845 - f1: 0.6437
[05/1… INFO   r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              7        
              2        
                       
                       
              E        
              P        
              O        
              C        
              H        
                       
              2        
              :        
                       
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              5        
              8        
              5        
              ,        
                       
              f        
              1        
                       
              -        
              0        
              .        
              7        
              6        
              0        
              ,        
                       
              v        
              a        
              l        
              _        
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              5        
              8        
              5        
              ,        
                       
              v        
              a        
              l        
              _        
              f        
              1        
                       
              -        
              0        
              .        
              6        
              4        
              4        
       DEBUG  r custom…
              o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              4        
              8        
                       
                       
              X        
              _        
              t        
              r        
              a        
              i        
              n        
                       
              s        
              h        
              a        
              p        
              e        
                       
              (        
              1        
              1        
              2        
              ,        
                       
              3        
              0        
              ,        
                       
              9        
              2        
              1        
              6        
              )        
2022-05-15 23:36:46.514872: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:7180"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 112
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 112
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:36:46.889638: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_80320"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:7200"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:47.304319: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_80518"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:7228"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/4 [======>.......................] - ETA: 0s - loss: 0.6187 - f1: 0.16674/4 [==============================] - 0s 14ms/step - loss: 0.5900 - f1: 0.6000
[05/1… INFO   r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              7        
              2        
                       
                       
              E        
              P        
              O        
              C        
              H        
                       
              2        
              :        
                       
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              5        
              9        
              9        
              ,        
                       
              f        
              1        
                       
              -        
              0        
              .        
              4        
              9        
              0        
              ,        
                       
              v        
              a        
              l        
              _        
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              5        
              9        
              0        
              ,        
                       
              v        
              a        
              l        
              _        
              f        
              1        
                       
              -        
              0        
              .        
              6        
              0        
              0        
       DEBUG  r custom…
              o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              4        
              8        
                       
                       
              X        
              _        
              t        
              r        
              a        
              i        
              n        
                       
              s        
              h        
              a        
              p        
              e        
                       
              (        
              1        
              4        
              6        
              ,        
                       
              3        
              0        
              ,        
                       
              9        
              2        
              1        
              6        
              )        
2022-05-15 23:36:48.067764: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:7252"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 146
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 146
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:36:48.513947: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_80917"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:7272"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:49.014435: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_81119"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:7300"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/5 [=====>........................] - ETA: 0s - loss: 0.8182 - f1: 0.16785/5 [==============================] - 0s 12ms/step - loss: 0.6930 - f1: 0.5904
[05/1… INFO   r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              7        
              2        
                       
                       
              E        
              P        
              O        
              C        
              H        
                       
              2        
              :        
                       
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              7        
              1        
              1        
              ,        
                       
              f        
              1        
                       
              -        
              0        
              .        
              5        
              2        
              6        
              ,        
                       
              v        
              a        
              l        
              _        
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              6        
              9        
              3        
              ,        
                       
              v        
              a        
              l        
              _        
              f        
              1        
                       
              -        
              0        
              .        
              5        
              9        
              0        
       DEBUG  r custom…
              o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              4        
              8        
                       
                       
              X        
              _        
              t        
              r        
              a        
              i        
              n        
                       
              s        
              h        
              a        
              p        
              e        
                       
              (        
              1        
              0        
              2        
              ,        
                       
              3        
              0        
              ,        
                       
              9        
              2        
              1        
              6        
              )        
2022-05-15 23:36:49.670402: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:7324"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 102
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 102
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:36:50.011284: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_81523"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:7344"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:50.401329: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_81721"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:7372"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/4 [======>.......................] - ETA: 0s - loss: 0.6985 - f1: 0.07144/4 [==============================] - 0s 11ms/step - loss: 0.6204 - f1: 0.6782
[05/1… INFO   r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              7        
              2        
                       
                       
              E        
              P        
              O        
              C        
              H        
                       
              2        
              :        
                       
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              6        
              3        
              0        
              ,        
                       
              f        
              1        
                       
              -        
              0        
              .        
              6        
              2        
              4        
              ,        
                       
              v        
              a        
              l        
              _        
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              6        
              2        
              0        
              ,        
                       
              v        
              a        
              l        
              _        
              f        
              1        
                       
              -        
              0        
              .        
              6        
              7        
              8        
       DEBUG  r custom…
              o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              4        
              8        
                       
                       
              X        
              _        
              t        
              r        
              a        
              i        
              n        
                       
              s        
              h        
              a        
              p        
              e        
                       
              (        
              1        
              1        
              6        
              ,        
                       
              3        
              0        
              ,        
                       
              9        
              2        
              1        
              6        
              )        
2022-05-15 23:36:51.050916: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:7396"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 116
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 116
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:36:51.428596: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_82120"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:7416"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:51.859670: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_82318"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:7444"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/4 [======>.......................] - ETA: 0s - loss: 0.4943 - f1: 0.11114/4 [==============================] - 0s 12ms/step - loss: 0.5733 - f1: 0.6047
[05/1… INFO   r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              7        
              2        
                       
                       
              E        
              P        
              O        
              C        
              H        
                       
              2        
              :        
                       
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              5        
              7        
              9        
              ,        
                       
              f        
              1        
                       
              -        
              0        
              .        
              6        
              8        
              4        
              ,        
                       
              v        
              a        
              l        
              _        
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              5        
              7        
              3        
              ,        
                       
              v        
              a        
              l        
              _        
              f        
              1        
                       
              -        
              0        
              .        
              6        
              0        
              5        
       DEBUG  r custom…
              o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              4        
              8        
                       
                       
              X        
              _        
              t        
              r        
              a        
              i        
              n        
                       
              s        
              h        
              a        
              p        
              e        
                       
              (        
              1        
              3        
              8        
              ,        
                       
              3        
              0        
              ,        
                       
              9        
              2        
              1        
              6        
              )        
2022-05-15 23:36:52.591482: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:7468"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 138
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 138
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:36:53.025398: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_82717"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:7488"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:53.500268: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_82919"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:7516"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/5 [=====>........................] - ETA: 0s - loss: 0.4938 - f1: 0.0000e+005/5 [==============================] - 0s 12ms/step - loss: 0.6186 - f1: 0.3702
[05/1… INFO   r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              7        
              2        
                       
                       
              E        
              P        
              O        
              C        
              H        
                       
              2        
              :        
                       
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              6        
              1        
              0        
              ,        
                       
              f        
              1        
                       
              -        
              0        
              .        
              4        
              9        
              6        
              ,        
                       
              v        
              a        
              l        
              _        
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              6        
              1        
              9        
              ,        
                       
              v        
              a        
              l        
              _        
              f        
              1        
                       
              -        
              0        
              .        
              3        
              7        
              0        
[05/1… DEBUG  r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              4        
              8        
                       
                       
              X        
              _        
              t        
              r        
              a        
              i        
              n        
                       
              s        
              h        
              a        
              p        
              e        
                       
              (        
              2        
              6        
              6        
              ,        
                       
              3        
              0        
              ,        
                       
              9        
              2        
              1        
              6        
              )        
2022-05-15 23:36:54.711191: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:7540"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 266
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 266
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:36:55.458719: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_83323"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:7560"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
2022-05-15 23:36:56.292687: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_9"
op: "FlatMapDataset"
input: "PrefetchDataset/_8"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_slice_batch_indices_83541"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\023FlatMapDataset:7588"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_INT64
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
1/9 [==>...........................] - ETA: 2s - loss: 0.5699 - f1: 0.14295/9 [===============>..............] - ETA: 0s - loss: 0.5200 - f1: 0.28499/9 [==============================] - 0s 14ms/step - loss: 0.5729 - f1: 0.5386
[05/1… INFO   r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              7        
              2        
                       
                       
              E        
              P        
              O        
              C        
              H        
                       
              2        
              :        
                       
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              5        
              7        
              3        
              ,        
                       
              f        
              1        
                       
              -        
              0        
              .        
              5        
              4        
              8        
              ,        
                       
              v        
              a        
              l        
              _        
              l        
              o        
              s        
              s        
                       
              -        
              0        
              .        
              5        
              7        
              3        
              ,        
                       
              v        
              a        
              l        
              _        
              f        
              1        
                       
              -        
              0        
              .        
              5        
              3        
              9        
[05/1… DEBUG  r custom…
23:36…        o        
              o        
              t        
              [        
              7        
              2        
              0        
              4        
              6        
              6        
              ]        
                       
              M        
              a        
              i        
              n        
              P        
              r        
              o        
              c        
              e        
              s        
              s        
              (        
              M        
              a        
              i        
              n        
              T        
              h        
              r        
              e        
              a        
              d        
              )        
                       
              c        
              u        
              s        
              t        
              o        
              m        
              _        
              m        
              o        
              d        
              e        
              l        
              s        
              :        
              1        
              4        
              8        
                       
                       
              X        
              _        
              t        
              r        
              a        
              i        
              n        
                       
              s        
              h        
              a        
              p        
              e        
                       
              (        
              2        
              1        
              8        
              ,        
                       
              3        
              0        
              ,        
                       
              9        
              2        
              1        
              6        
              )        
2022-05-15 23:36:57.531968: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: "TensorDataset/_2"
op: "TensorDataset"
input: "Placeholder/_0"
input: "Placeholder/_1"
attr {
  key: "Toutput_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT64
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: 1
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\022TensorDataset:7612"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: 218
        }
        dim {
          size: 30
        }
        dim {
          size: 9216
        }
      }
      shape {
        dim {
          size: 218
        }
      }
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT64
        }
      }
    }
  }
}

2022-05-15 23:37:07.785789: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 114.96MiB (rounded to 120545280)requested by op transpose_0
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. 
Current allocation summary follows.
Current allocation summary follows.
2022-05-15 23:37:07.785845: I tensorflow/core/common_runtime/bfc_allocator.cc:1010] BFCAllocator dump for GPU_0_bfc
2022-05-15 23:37:07.785861: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (256): 	Total Chunks: 135, Chunks in use: 134. 33.8KiB allocated for chunks. 33.5KiB in use in bin. 2.2KiB client-requested in use in bin.
2022-05-15 23:37:07.785873: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (512): 	Total Chunks: 7, Chunks in use: 6. 4.0KiB allocated for chunks. 3.5KiB in use in bin. 2.8KiB client-requested in use in bin.
2022-05-15 23:37:07.785885: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1024): 	Total Chunks: 27, Chunks in use: 27. 35.0KiB allocated for chunks. 35.0KiB in use in bin. 31.2KiB client-requested in use in bin.
2022-05-15 23:37:07.785910: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2048): 	Total Chunks: 17, Chunks in use: 17. 38.8KiB allocated for chunks. 38.8KiB in use in bin. 33.8KiB client-requested in use in bin.
2022-05-15 23:37:07.785921: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4096): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-05-15 23:37:07.785933: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8192): 	Total Chunks: 3, Chunks in use: 2. 32.2KiB allocated for chunks. 20.0KiB in use in bin. 16.0KiB client-requested in use in bin.
2022-05-15 23:37:07.785943: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16384): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-05-15 23:37:07.785954: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (32768): 	Total Chunks: 7, Chunks in use: 6. 318.5KiB allocated for chunks. 264.5KiB in use in bin. 250.5KiB client-requested in use in bin.
2022-05-15 23:37:07.785966: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (65536): 	Total Chunks: 5, Chunks in use: 4. 418.5KiB allocated for chunks. 327.0KiB in use in bin. 256.0KiB client-requested in use in bin.
2022-05-15 23:37:07.785976: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (131072): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-05-15 23:37:07.785986: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (262144): 	Total Chunks: 4, Chunks in use: 3. 1.39MiB allocated for chunks. 1.14MiB in use in bin. 768.0KiB client-requested in use in bin.
2022-05-15 23:37:07.785996: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (524288): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-05-15 23:37:07.786006: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1048576): 	Total Chunks: 1, Chunks in use: 0. 1.46MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-05-15 23:37:07.786017: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2097152): 	Total Chunks: 1, Chunks in use: 1. 2.29MiB allocated for chunks. 2.29MiB in use in bin. 1.46MiB client-requested in use in bin.
2022-05-15 23:37:07.786028: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4194304): 	Total Chunks: 3, Chunks in use: 3. 16.00MiB allocated for chunks. 16.00MiB in use in bin. 13.50MiB client-requested in use in bin.
2022-05-15 23:37:07.786039: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8388608): 	Total Chunks: 1, Chunks in use: 1. 8.00MiB allocated for chunks. 8.00MiB in use in bin. 4.50MiB client-requested in use in bin.
2022-05-15 23:37:07.786050: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16777216): 	Total Chunks: 3, Chunks in use: 3. 64.00MiB allocated for chunks. 64.00MiB in use in bin. 54.00MiB client-requested in use in bin.
2022-05-15 23:37:07.786061: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (33554432): 	Total Chunks: 1, Chunks in use: 0. 32.00MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-05-15 23:37:07.786072: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (67108864): 	Total Chunks: 9, Chunks in use: 8. 999.84MiB allocated for chunks. 890.16MiB in use in bin. 890.16MiB client-requested in use in bin.
2022-05-15 23:37:07.786083: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (134217728): 	Total Chunks: 34, Chunks in use: 34. 6.01GiB allocated for chunks. 6.01GiB in use in bin. 5.41GiB client-requested in use in bin.
2022-05-15 23:37:07.786094: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (268435456): 	Total Chunks: 11, Chunks in use: 11. 3.06GiB allocated for chunks. 3.06GiB in use in bin. 2.70GiB client-requested in use in bin.
2022-05-15 23:37:07.786111: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] Bin for 114.96MiB was 64.00MiB, Chunk State: 
2022-05-15 23:37:07.786131: I tensorflow/core/common_runtime/bfc_allocator.cc:1039]   Size: 109.69MiB | Requested Size: 18.25MiB | in_use: 0 | bin_num: 18, prev:   Size: 84.38MiB | Requested Size: 84.38MiB | in_use: 1 | bin_num: -1, next:   Size: 265.58MiB | Requested Size: 217.27MiB | in_use: 1 | bin_num: -1
2022-05-15 23:37:07.786140: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 2194407424
2022-05-15 23:37:07.786153: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1d40000000 of size 199065600 next 245
2022-05-15 23:37:07.786162: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1d4bdd8000 of size 132710400 next 260
2022-05-15 23:37:07.786170: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1d53c68000 of size 152616960 next 261
2022-05-15 23:37:07.786179: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1d5cdf4000 of size 285327360 next 262
2022-05-15 23:37:07.786187: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1d6de10000 of size 161464320 next 250
2022-05-15 23:37:07.786195: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1d7780c000 of size 161464320 next 83
2022-05-15 23:37:07.786203: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1d81208000 of size 161464320 next 255
2022-05-15 23:37:07.786210: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1d8ac04000 of size 128286720 next 259
2022-05-15 23:37:07.786218: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1d9265c000 of size 152616960 next 99
2022-05-15 23:37:07.786226: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1d9b7e8000 of size 294174720 next 243
2022-05-15 23:37:07.786235: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1dad074000 of size 365215744 next 18446744073709551615
2022-05-15 23:37:07.786243: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 4294967296
2022-05-15 23:37:07.786251: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1e96000000 of size 210124800 next 56
2022-05-15 23:37:07.786259: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1ea2864000 of size 210124800 next 185
2022-05-15 23:37:07.786267: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1eaf0c8000 of size 161464320 next 238
2022-05-15 23:37:07.786275: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1eb8ac4000 of size 139345920 next 237
2022-05-15 23:37:07.786283: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1ec0fa8000 of size 300810240 next 79
2022-05-15 23:37:07.786291: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1ed2e88000 of size 161464320 next 221
2022-05-15 23:37:07.786298: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1edc884000 of size 300810240 next 233
2022-05-15 23:37:07.786306: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1eee764000 of size 150405120 next 147
2022-05-15 23:37:07.786314: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1ef76d4000 of size 150405120 next 75
2022-05-15 23:37:07.786322: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1f00644000 of size 245514240 next 218
2022-05-15 23:37:07.786330: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1f0f068000 of size 287539200 next 229
2022-05-15 23:37:07.786338: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1f202a0000 of size 210124800 next 40
2022-05-15 23:37:07.786346: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1f2cb04000 of size 225607680 next 240
2022-05-15 23:37:07.786355: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1f3a22c000 of size 278691840 next 232
2022-05-15 23:37:07.786369: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1f4abf4000 of size 212336640 next 182
2022-05-15 23:37:07.786378: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1f57674000 of size 205701120 next 25
2022-05-15 23:37:07.786386: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1f63aa0000 of size 256573440 next 247
2022-05-15 23:37:07.786394: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1f72f50000 of size 157040640 next 184
2022-05-15 23:37:07.786402: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1f7c514000 of size 157040640 next 32
2022-05-15 23:37:07.786409: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1f85ad8000 of size 115015680 next 234
2022-05-15 23:37:07.786417: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f1f8c888000 of size 158826496 next 18446744073709551615
2022-05-15 23:37:07.786426: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 2147483648
2022-05-15 23:37:07.786434: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f201a000000 of size 254361600 next 219
2022-05-15 23:37:07.786442: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2029294000 of size 256573440 next 30
2022-05-15 23:37:07.786450: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2038744000 of size 256573440 next 222
2022-05-15 23:37:07.786458: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2047bf4000 of size 241090560 next 224
2022-05-15 23:37:07.786466: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f20561e0000 of size 170311680 next 55
2022-05-15 23:37:07.786474: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f206044c000 of size 132710400 next 112
2022-05-15 23:37:07.786481: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f20682dc000 of size 150405120 next 54
2022-05-15 23:37:07.786489: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f207124c000 of size 203489280 next 26
2022-05-15 23:37:07.786497: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f207d45c000 of size 88473600 next 169
2022-05-15 23:37:07.786505: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f20828bc000 of size 115015680 next 227
2022-05-15 23:37:07.786513: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f208966c000 of size 278478848 next 18446744073709551615
2022-05-15 23:37:07.786521: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 1073741824
2022-05-15 23:37:07.786529: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f217c000000 of size 161464320 next 33
2022-05-15 23:37:07.786537: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f21859fc000 of size 123863040 next 181
2022-05-15 23:37:07.786545: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f218d01c000 of size 161464320 next 87
2022-05-15 23:37:07.786553: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2196a18000 of size 285327360 next 24
2022-05-15 23:37:07.786561: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f21a7a34000 of size 341622784 next 18446744073709551615
2022-05-15 23:37:07.786569: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 536870912
2022-05-15 23:37:07.786577: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f227c000000 of size 128286720 next 39
2022-05-15 23:37:07.786585: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2283a58000 of size 159252480 next 15
2022-05-15 23:37:07.786593: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f228d238000 of size 249331712 next 18446744073709551615
2022-05-15 23:37:07.786600: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 268435456
2022-05-15 23:37:07.786608: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f22e4000000 of size 84049920 next 180
2022-05-15 23:37:07.786628: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f22e9028000 of size 184385536 next 18446744073709551615
2022-05-15 23:37:07.786637: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 268435456
2022-05-15 23:37:07.786645: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f237c000000 of size 268435456 next 18446744073709551615
2022-05-15 23:37:07.786653: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 67108864
2022-05-15 23:37:07.786661: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f23aa000000 of size 18874368 next 189
2022-05-15 23:37:07.786669: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f23ab200000 of size 18874368 next 187
2022-05-15 23:37:07.786677: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f23ac400000 of size 29360128 next 18446744073709551615
2022-05-15 23:37:07.786684: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 33554432
2022-05-15 23:37:07.786692: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f23ae000000 of size 33554432 next 18446744073709551615
2022-05-15 23:37:07.786700: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 16777216
2022-05-15 23:37:07.786708: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f242e000000 of size 4718592 next 282
2022-05-15 23:37:07.786715: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f242e480000 of size 4718592 next 10
2022-05-15 23:37:07.786723: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f242e900000 of size 7340032 next 18446744073709551615
2022-05-15 23:37:07.786731: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 2097152
2022-05-15 23:37:07.786739: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c00000 of size 256 next 1
2022-05-15 23:37:07.786747: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c00100 of size 1280 next 2
2022-05-15 23:37:07.786754: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c00600 of size 256 next 3
2022-05-15 23:37:07.786762: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c00700 of size 256 next 4
2022-05-15 23:37:07.786770: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c00800 of size 256 next 5
2022-05-15 23:37:07.786777: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c00900 of size 256 next 6
2022-05-15 23:37:07.786785: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c00a00 of size 256 next 9
2022-05-15 23:37:07.786792: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c00b00 of size 256 next 137
2022-05-15 23:37:07.786801: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c00c00 of size 512 next 12
2022-05-15 23:37:07.786808: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c00e00 of size 256 next 13
2022-05-15 23:37:07.786816: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c00f00 of size 256 next 14
2022-05-15 23:37:07.786824: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c01000 of size 256 next 21
2022-05-15 23:37:07.786832: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c01100 of size 512 next 136
2022-05-15 23:37:07.786839: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c01300 of size 256 next 135
2022-05-15 23:37:07.786847: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c01400 of size 256 next 27
2022-05-15 23:37:07.786854: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c01500 of size 256 next 28
2022-05-15 23:37:07.786862: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c01600 of size 256 next 29
2022-05-15 23:37:07.786870: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c01700 of size 256 next 139
2022-05-15 23:37:07.786883: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c01800 of size 512 next 34
2022-05-15 23:37:07.786904: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c01a00 of size 256 next 17
2022-05-15 23:37:07.786912: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c01b00 of size 256 next 151
2022-05-15 23:37:07.786920: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c01c00 of size 256 next 150
2022-05-15 23:37:07.786927: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c01d00 of size 256 next 160
2022-05-15 23:37:07.786935: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c01e00 of size 256 next 132
2022-05-15 23:37:07.786943: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c01f00 of size 256 next 149
2022-05-15 23:37:07.786950: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c02000 of size 256 next 174
2022-05-15 23:37:07.786958: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c02100 of size 256 next 154
2022-05-15 23:37:07.786966: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c02200 of size 256 next 153
2022-05-15 23:37:07.786973: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c02300 of size 256 next 20
2022-05-15 23:37:07.786981: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c02400 of size 256 next 7
2022-05-15 23:37:07.786989: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c02500 of size 3584 next 8
2022-05-15 23:37:07.786997: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c03300 of size 256 next 35
2022-05-15 23:37:07.787005: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c03400 of size 256 next 36
2022-05-15 23:37:07.787012: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c03500 of size 256 next 52
2022-05-15 23:37:07.787020: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c03600 of size 256 next 53
2022-05-15 23:37:07.787028: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c03700 of size 256 next 144
2022-05-15 23:37:07.787035: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c03800 of size 256 next 156
2022-05-15 23:37:07.787043: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c03900 of size 256 next 141
2022-05-15 23:37:07.787051: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c03a00 of size 256 next 138
2022-05-15 23:37:07.787058: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c03b00 of size 256 next 143
2022-05-15 23:37:07.787066: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c03c00 of size 256 next 42
2022-05-15 23:37:07.787074: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c03d00 of size 256 next 43
2022-05-15 23:37:07.787081: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c03e00 of size 256 next 44
2022-05-15 23:37:07.787089: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c03f00 of size 256 next 119
2022-05-15 23:37:07.787102: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c04000 of size 2048 next 161
2022-05-15 23:37:07.787110: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c04800 of size 256 next 103
2022-05-15 23:37:07.787117: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c04900 of size 256 next 121
2022-05-15 23:37:07.787125: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c04a00 of size 256 next 84
2022-05-15 23:37:07.787133: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c04b00 of size 256 next 109
2022-05-15 23:37:07.787140: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c04c00 of size 256 next 113
2022-05-15 23:37:07.787160: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c04d00 of size 256 next 225
2022-05-15 23:37:07.787168: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c04e00 of size 256 next 226
2022-05-15 23:37:07.787176: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c04f00 of size 256 next 48
2022-05-15 23:37:07.787184: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c05000 of size 256 next 49
2022-05-15 23:37:07.787192: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c05100 of size 256 next 50
2022-05-15 23:37:07.787199: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c05200 of size 256 next 57
2022-05-15 23:37:07.787207: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c05300 of size 256 next 74
2022-05-15 23:37:07.787215: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c05400 of size 256 next 101
2022-05-15 23:37:07.787223: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c05500 of size 256 next 106
2022-05-15 23:37:07.787230: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c05600 of size 256 next 168
2022-05-15 23:37:07.787238: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c05700 of size 256 next 58
2022-05-15 23:37:07.787245: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c05800 of size 256 next 146
2022-05-15 23:37:07.787253: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c05900 of size 256 next 120
2022-05-15 23:37:07.787261: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c05a00 of size 256 next 165
2022-05-15 23:37:07.787268: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c05b00 of size 256 next 173
2022-05-15 23:37:07.787276: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c05c00 of size 256 next 128
2022-05-15 23:37:07.787284: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c05d00 of size 256 next 115
2022-05-15 23:37:07.787291: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c05e00 of size 256 next 111
2022-05-15 23:37:07.787299: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c05f00 of size 256 next 100
2022-05-15 23:37:07.787307: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c06000 of size 256 next 114
2022-05-15 23:37:07.787314: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c06100 of size 256 next 69
2022-05-15 23:37:07.787322: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c06200 of size 256 next 70
2022-05-15 23:37:07.787330: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c06300 of size 256 next 71
2022-05-15 23:37:07.787337: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c06400 of size 256 next 19
2022-05-15 23:37:07.787345: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c06500 of size 256 next 131
2022-05-15 23:37:07.787352: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c06600 of size 256 next 94
2022-05-15 23:37:07.787361: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c06700 of size 1792 next 16
2022-05-15 23:37:07.787368: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c06e00 of size 1280 next 122
2022-05-15 23:37:07.787377: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c07300 of size 1024 next 47
2022-05-15 23:37:07.787384: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c07700 of size 256 next 67
2022-05-15 23:37:07.787392: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c07800 of size 1024 next 51
2022-05-15 23:37:07.787400: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c07c00 of size 256 next 22
2022-05-15 23:37:07.787407: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c07d00 of size 256 next 65
2022-05-15 23:37:07.787420: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c07e00 of size 256 next 220
2022-05-15 23:37:07.787428: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c07f00 of size 256 next 108
2022-05-15 23:37:07.787436: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c08000 of size 256 next 93
2022-05-15 23:37:07.787444: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c08100 of size 256 next 45
2022-05-15 23:37:07.787452: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c08200 of size 256 next 107
2022-05-15 23:37:07.787459: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c08300 of size 256 next 95
2022-05-15 23:37:07.787467: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c08400 of size 1536 next 41
2022-05-15 23:37:07.787475: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c08a00 of size 1024 next 183
2022-05-15 23:37:07.787483: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c08e00 of size 2304 next 117
2022-05-15 23:37:07.787491: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c09700 of size 1280 next 123
2022-05-15 23:37:07.787499: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c09c00 of size 256 next 228
2022-05-15 23:37:07.787506: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c09d00 of size 1792 next 236
2022-05-15 23:37:07.787514: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c0a400 of size 2304 next 62
2022-05-15 23:37:07.787522: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c0ad00 of size 256 next 63
2022-05-15 23:37:07.787529: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c0ae00 of size 256 next 64
2022-05-15 23:37:07.787537: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c0af00 of size 2048 next 125
2022-05-15 23:37:07.787544: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c0b700 of size 2304 next 18
2022-05-15 23:37:07.787552: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c0c000 of size 1792 next 37
2022-05-15 23:37:07.787560: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c0c700 of size 256 next 116
2022-05-15 23:37:07.787568: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c0c800 of size 768 next 246
2022-05-15 23:37:07.787576: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c0cb00 of size 256 next 231
2022-05-15 23:37:07.787583: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c0cc00 of size 256 next 60
2022-05-15 23:37:07.787591: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c0cd00 of size 256 next 76
2022-05-15 23:37:07.787598: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c0ce00 of size 256 next 77
2022-05-15 23:37:07.787606: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c0cf00 of size 256 next 78
2022-05-15 23:37:07.787614: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c0d000 of size 2304 next 176
2022-05-15 23:37:07.787621: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c0d900 of size 2048 next 148
2022-05-15 23:37:07.787629: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c0e100 of size 2048 next 230
2022-05-15 23:37:07.787637: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c0e900 of size 256 next 11
2022-05-15 23:37:07.787644: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c0ea00 of size 256 next 223
2022-05-15 23:37:07.787652: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c0eb00 of size 1280 next 96
2022-05-15 23:37:07.787660: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c0f000 of size 256 next 97
2022-05-15 23:37:07.787672: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c0f100 of size 256 next 98
2022-05-15 23:37:07.787681: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c0f200 of size 1792 next 81
2022-05-15 23:37:07.787688: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c0f900 of size 256 next 281
2022-05-15 23:37:07.787696: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c0fa00 of size 256 next 239
2022-05-15 23:37:07.787704: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c0fb00 of size 2048 next 241
2022-05-15 23:37:07.787711: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f2528c10300 of size 256 next 73
2022-05-15 23:37:07.787719: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c10400 of size 1536 next 217
2022-05-15 23:37:07.787727: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c10a00 of size 1536 next 72
2022-05-15 23:37:07.787735: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c11000 of size 256 next 130
2022-05-15 23:37:07.787742: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c11100 of size 256 next 129
2022-05-15 23:37:07.787750: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c11200 of size 2048 next 127
2022-05-15 23:37:07.787757: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c11a00 of size 256 next 235
2022-05-15 23:37:07.787765: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c11b00 of size 1280 next 252
2022-05-15 23:37:07.787773: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c12000 of size 1280 next 85
2022-05-15 23:37:07.787780: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c12500 of size 1024 next 46
2022-05-15 23:37:07.787788: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c12900 of size 1536 next 253
2022-05-15 23:37:07.787796: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c12f00 of size 768 next 258
2022-05-15 23:37:07.787803: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c13200 of size 1024 next 254
2022-05-15 23:37:07.787811: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c13600 of size 3072 next 88
2022-05-15 23:37:07.787819: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c14200 of size 256 next 66
2022-05-15 23:37:07.787827: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c14300 of size 256 next 190
2022-05-15 23:37:07.787834: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c14400 of size 256 next 191
2022-05-15 23:37:07.787842: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c14500 of size 256 next 89
2022-05-15 23:37:07.787850: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c14600 of size 256 next 90
2022-05-15 23:37:07.787857: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c14700 of size 256 next 91
2022-05-15 23:37:07.787865: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c14800 of size 51968 next 38
2022-05-15 23:37:07.787873: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c21300 of size 256 next 104
2022-05-15 23:37:07.787880: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c21400 of size 256 next 105
2022-05-15 23:37:07.787889: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c21500 of size 8192 next 110
2022-05-15 23:37:07.787896: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c23500 of size 256 next 170
2022-05-15 23:37:07.787904: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c23600 of size 256 next 171
2022-05-15 23:37:07.787912: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c23700 of size 12288 next 177
2022-05-15 23:37:07.787925: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c26700 of size 2048 next 194
2022-05-15 23:37:07.787933: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c26f00 of size 256 next 192
2022-05-15 23:37:07.787941: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c27000 of size 256 next 195
2022-05-15 23:37:07.787948: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c27100 of size 256 next 196
2022-05-15 23:37:07.787956: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c27200 of size 256 next 197
2022-05-15 23:37:07.787964: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c27300 of size 256 next 198
2022-05-15 23:37:07.787971: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c27400 of size 256 next 199
2022-05-15 23:37:07.787979: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c27500 of size 256 next 200
2022-05-15 23:37:07.787986: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c27600 of size 256 next 178
2022-05-15 23:37:07.787994: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c27700 of size 256 next 179
2022-05-15 23:37:07.788002: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c27800 of size 119808 next 145
2022-05-15 23:37:07.788010: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c44c00 of size 32768 next 193
2022-05-15 23:37:07.788019: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c4cc00 of size 36608 next 133
2022-05-15 23:37:07.788026: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f2528c55b00 of size 55296 next 142
2022-05-15 23:37:07.788034: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c63300 of size 256 next 157
2022-05-15 23:37:07.788041: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c63400 of size 256 next 158
2022-05-15 23:37:07.788049: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c63500 of size 1024 next 251
2022-05-15 23:37:07.788057: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c63900 of size 1280 next 186
2022-05-15 23:37:07.788065: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c63e00 of size 1024 next 257
2022-05-15 23:37:07.788072: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c64200 of size 1280 next 264
2022-05-15 23:37:07.788080: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c64700 of size 1024 next 263
2022-05-15 23:37:07.788088: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c64b00 of size 1024 next 242
2022-05-15 23:37:07.788095: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c64f00 of size 1280 next 277
2022-05-15 23:37:07.788103: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c65400 of size 2304 next 267
2022-05-15 23:37:07.788111: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c65d00 of size 1792 next 244
2022-05-15 23:37:07.788118: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f2528c66400 of size 512 next 82
2022-05-15 23:37:07.788126: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c66600 of size 256 next 271
2022-05-15 23:37:07.788134: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c66700 of size 256 next 279
2022-05-15 23:37:07.788141: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c66800 of size 512 next 126
2022-05-15 23:37:07.788149: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f2528c66a00 of size 12544 next 175
2022-05-15 23:37:07.788157: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c69b00 of size 256 next 202
2022-05-15 23:37:07.788164: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c69c00 of size 256 next 203
2022-05-15 23:37:07.788177: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c69d00 of size 256 next 204
2022-05-15 23:37:07.788186: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c69e00 of size 256 next 205
2022-05-15 23:37:07.788193: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c69f00 of size 2048 next 206
2022-05-15 23:37:07.788201: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c6a700 of size 256 next 207
2022-05-15 23:37:07.788209: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c6a800 of size 256 next 208
2022-05-15 23:37:07.788216: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c6a900 of size 256 next 209
2022-05-15 23:37:07.788224: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c6aa00 of size 2048 next 211
2022-05-15 23:37:07.788231: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c6b200 of size 256 next 212
2022-05-15 23:37:07.788239: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c6b300 of size 256 next 213
2022-05-15 23:37:07.788247: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c6b400 of size 256 next 214
2022-05-15 23:37:07.788254: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c6b500 of size 256 next 215
2022-05-15 23:37:07.788262: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c6b600 of size 256 next 216
2022-05-15 23:37:07.788269: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c6b700 of size 3072 next 152
2022-05-15 23:37:07.788277: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c6c300 of size 256 next 163
2022-05-15 23:37:07.788285: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c6c400 of size 256 next 164
2022-05-15 23:37:07.788293: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c6c500 of size 65536 next 86
2022-05-15 23:37:07.788301: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528c7c500 of size 65536 next 31
2022-05-15 23:37:07.788308: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f2528c8c500 of size 93696 next 167
2022-05-15 23:37:07.788316: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528ca3300 of size 36864 next 166
2022-05-15 23:37:07.788324: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528cac300 of size 83968 next 155
2022-05-15 23:37:07.788332: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528cc0b00 of size 51200 next 134
2022-05-15 23:37:07.788341: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528ccd300 of size 61440 next 61
2022-05-15 23:37:07.788349: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528cdc300 of size 479232 next 80
2022-05-15 23:37:07.788356: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f2528d51300 of size 262144 next 59
2022-05-15 23:37:07.788365: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2528d91300 of size 453888 next 18446744073709551615
2022-05-15 23:37:07.788373: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 4194304
2022-05-15 23:37:07.788381: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2529000000 of size 262144 next 210
2022-05-15 23:37:07.788389: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f2529040000 of size 1536000 next 159
2022-05-15 23:37:07.788397: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f25291b7000 of size 2396160 next 18446744073709551615
2022-05-15 23:37:07.788405: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 8388608
2022-05-15 23:37:07.788413: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f2529800000 of size 8388608 next 18446744073709551615
2022-05-15 23:37:07.788420: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size: 
2022-05-15 23:37:07.788437: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 134 Chunks of size 256 totalling 33.5KiB
2022-05-15 23:37:07.788447: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 4 Chunks of size 512 totalling 2.0KiB
2022-05-15 23:37:07.788456: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 768 totalling 1.5KiB
2022-05-15 23:37:07.788465: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 9 Chunks of size 1024 totalling 9.0KiB
2022-05-15 23:37:07.788475: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 9 Chunks of size 1280 totalling 11.2KiB
2022-05-15 23:37:07.788484: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 4 Chunks of size 1536 totalling 6.0KiB
2022-05-15 23:37:07.788493: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 5 Chunks of size 1792 totalling 8.8KiB
2022-05-15 23:37:07.788502: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 9 Chunks of size 2048 totalling 18.0KiB
2022-05-15 23:37:07.788511: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 5 Chunks of size 2304 totalling 11.2KiB
2022-05-15 23:37:07.788520: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 3072 totalling 6.0KiB
2022-05-15 23:37:07.788529: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 3584 totalling 3.5KiB
2022-05-15 23:37:07.788538: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 8192 totalling 8.0KiB
2022-05-15 23:37:07.788547: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 12288 totalling 12.0KiB
2022-05-15 23:37:07.788556: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 32768 totalling 32.0KiB
2022-05-15 23:37:07.788565: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 36608 totalling 35.8KiB
2022-05-15 23:37:07.788575: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 36864 totalling 36.0KiB
2022-05-15 23:37:07.788584: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 51200 totalling 50.0KiB
2022-05-15 23:37:07.788593: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 51968 totalling 50.8KiB
2022-05-15 23:37:07.788602: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 61440 totalling 60.0KiB
2022-05-15 23:37:07.788611: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 65536 totalling 128.0KiB
2022-05-15 23:37:07.788620: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 83968 totalling 82.0KiB
2022-05-15 23:37:07.788629: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 119808 totalling 117.0KiB
2022-05-15 23:37:07.788639: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 262144 totalling 256.0KiB
2022-05-15 23:37:07.788648: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 453888 totalling 443.2KiB
2022-05-15 23:37:07.788657: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 479232 totalling 468.0KiB
2022-05-15 23:37:07.788666: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 2396160 totalling 2.29MiB
2022-05-15 23:37:07.788675: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 4718592 totalling 9.00MiB
2022-05-15 23:37:07.788684: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 7340032 totalling 7.00MiB
2022-05-15 23:37:07.788693: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 8388608 totalling 8.00MiB
2022-05-15 23:37:07.788702: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 18874368 totalling 36.00MiB
2022-05-15 23:37:07.788711: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 29360128 totalling 28.00MiB
2022-05-15 23:37:07.788720: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 84049920 totalling 80.16MiB
2022-05-15 23:37:07.788729: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 88473600 totalling 84.38MiB
2022-05-15 23:37:07.788744: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 115015680 totalling 109.69MiB
2022-05-15 23:37:07.788754: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 123863040 totalling 118.12MiB
2022-05-15 23:37:07.788763: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 128286720 totalling 244.69MiB
2022-05-15 23:37:07.788772: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 132710400 totalling 253.12MiB
2022-05-15 23:37:07.788782: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 139345920 totalling 132.89MiB
2022-05-15 23:37:07.788791: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 150405120 totalling 430.31MiB
2022-05-15 23:37:07.788800: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 152616960 totalling 291.09MiB
2022-05-15 23:37:07.788809: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 157040640 totalling 299.53MiB
2022-05-15 23:37:07.788818: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 158826496 totalling 151.47MiB
2022-05-15 23:37:07.788827: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 159252480 totalling 151.88MiB
2022-05-15 23:37:07.788836: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 7 Chunks of size 161464320 totalling 1.05GiB
2022-05-15 23:37:07.788845: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 170311680 totalling 162.42MiB
2022-05-15 23:37:07.788854: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 184385536 totalling 175.84MiB
2022-05-15 23:37:07.788863: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 199065600 totalling 189.84MiB
2022-05-15 23:37:07.788872: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 203489280 totalling 194.06MiB
2022-05-15 23:37:07.788882: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 205701120 totalling 196.17MiB
2022-05-15 23:37:07.788891: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 210124800 totalling 601.17MiB
2022-05-15 23:37:07.788900: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 212336640 totalling 202.50MiB
2022-05-15 23:37:07.788909: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 225607680 totalling 215.16MiB
2022-05-15 23:37:07.788918: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 241090560 totalling 229.92MiB
2022-05-15 23:37:07.788927: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 245514240 totalling 234.14MiB
2022-05-15 23:37:07.788936: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 249331712 totalling 237.78MiB
2022-05-15 23:37:07.788945: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 254361600 totalling 242.58MiB
2022-05-15 23:37:07.788955: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 256573440 totalling 734.06MiB
2022-05-15 23:37:07.788964: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 268435456 totalling 256.00MiB
2022-05-15 23:37:07.788973: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 278478848 totalling 265.58MiB
2022-05-15 23:37:07.788982: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 278691840 totalling 265.78MiB
2022-05-15 23:37:07.788991: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 285327360 totalling 544.22MiB
2022-05-15 23:37:07.789000: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 287539200 totalling 274.22MiB
2022-05-15 23:37:07.789009: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 294174720 totalling 280.55MiB
2022-05-15 23:37:07.789018: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 300810240 totalling 573.75MiB
2022-05-15 23:37:07.789033: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 341622784 totalling 325.80MiB
2022-05-15 23:37:07.789043: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 365215744 totalling 348.30MiB
2022-05-15 23:37:07.789052: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 10.03GiB
2022-05-15 23:37:07.789060: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 10916462592 memory_limit_: 10916462592 available bytes: 0 curr_region_allocation_bytes_: 17179869184
2022-05-15 23:37:07.789073: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats: 
Limit:                     10916462592
InUse:                     10765932032
MaxInUse:                  10812628736
NumAllocs:                       81764
MaxAllocSize:                365215744
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-05-15 23:37:07.789095: W tensorflow/core/common_runtime/bfc_allocator.cc:474] ****************************************************************************************************
2022-05-15 23:37:07.789141: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at transpose_op.cc:183 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[30,109,9216] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
